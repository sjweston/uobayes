[
  {
    "objectID": "resource/bayes.html",
    "href": "resource/bayes.html",
    "title": "Bayesian statistics resources",
    "section": "",
    "text": "In class session 2 (see this from the FAQ slides) we talked briefly about the difference between frequentist statistics, where you test for the probability of your data given a null hypothesis, or \\(P(\\text{data} \\mid H_0)\\), and Bayesian statistics, where you test for the probability of your hypothesis given your data, or \\(P(H \\mid \\text{data})\\).\nThis difference is important. In the world of frequentism and null hypothesis significance testing (NHST), which is what pretty much all statistics classes use (including this one!), you have to compare your findings to a hypothetical null world and you have to talk about rejecting null hypotheses. In the Bayes world, though, you get to talk about the probability that your hypothesis is correct rather than the probability of seeing a value in a null world. So much more convenient and easy to interpret!\nBayesian statistics, though, requires a lot of computational power and a different way of thinking about statistics and numbers in general. And very few classes teach it. Including this one! I use Bayesian stats all the time in my own research (see this or this, for instance), but don’t teach it (yet!) because nobody else really teaches it and frequentist statistics still rule the policy world, so you need to know it."
  },
  {
    "objectID": "resource/bayes.html#resources",
    "href": "resource/bayes.html#resources",
    "title": "Bayesian statistics resources",
    "section": "Resources",
    "text": "Resources\nBut you can learn it on your own. Because very few stats classes actually teach Bayesian statistics, tons of people who use it are self-taught (like me!), in part because there are a ton of resources online for learning this stuff. Here are some of the best I’ve found:\n\nThis new Bayes Rules book is designed to be an introductory textbook for a stats class teaching Bayesian stuff. It’s really accessible and good (and free!). If I ever get to teach an intro stats class with Bayesian stats, I’ll use this.\nThis post from 2016 is a great short introduction and is what made me start using Bayesian methods. The brms package makes it incredibly easy to do Bayesian stuff, and the syntax is basically the same as lm()\nThis post shows how to do one simple task (a difference-in-means test) with regular old frequentist methods, bootstrapping, and with Bayesian stats both with brms and raw Stan code\nThis short post gives a helpful overview of the intuition behind Bayesianism\nThe super canonical everyone-has-this-book book is Statistical Rethinking by Richard McElreath. At that page he also has an entire set of accompanying lectures on YouTube. He doesn’t use brms or ggplot, but someone has translated all his models to tidyverse-based brms code here\nThe Theory That Would Not Die is a fun little general introduction to the history of Bayesianism and why it kind of disappeared in the 20th century and was replaced by frequentism and p-values and null hypothesis testing"
  },
  {
    "objectID": "resource/bayes.html#super-short-example",
    "href": "resource/bayes.html#super-short-example",
    "title": "Bayesian statistics resources",
    "section": "Super short example",
    "text": "Super short example\nIn practice, the R code for Bayesian models should be very familiar. For instance, here’s a regular old frequentist OLS model:\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\n\nmodel_ols &lt;- lm(hwy ~ displ + drv, data = mpg)\ntidy(model_ols, conf.int = TRUE)\n## # A tibble: 4 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    30.8      0.924     33.4  4.21e-90    29.0      32.6 \n## 2 displ          -2.91     0.218    -13.4  1.73e-30    -3.34     -2.48\n## 3 drvf            4.79     0.530      9.05 6.40e-17     3.75      5.83\n## 4 drvr            5.26     0.734      7.17 1.03e-11     3.81      6.70\n\n\nHere’s that same model using the brms package, with default priors. Note how the code is basically the same:\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)         # For Bayesian regression with brm()\nlibrary(broom.mixed)  # For tidy() and glance() with brms-based models\nlibrary(tidybayes)    # For extracting posterior draws\n\n\n\n\nCode\n# This will take a few seconds to run\nmodel_bayes &lt;- brm(hwy ~ displ + drv, data = mpg)\n\n\n\n\nCode\ntidy(model_bayes)\n## # A tibble: 5 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)        30.8      0.911    29.1      32.6 \n## 2 fixed    disp      &lt;NA&gt;     displ              -2.92     0.214    -3.34     -2.50\n## 3 fixed    cond      &lt;NA&gt;     drvf                4.79     0.532     3.73      5.81\n## 4 fixed    cond      &lt;NA&gt;     drvr                5.27     0.731     3.84      6.69\n## 5 ran_pars cond      Residual sd__Observation     3.09     0.146     2.82      3.39\n\n\nIn Bayes land, you get a distribution of plausible values given the data (or what is called the “posterior distribution”), and you can visualize this posterior distribution:\n\n\nCode\n# Make a long dataset of the draws for these three coefficients\nposterior_draws &lt;- model_bayes %&gt;% \n  gather_draws(c(b_displ, b_drv, b_drvf, b_drvr))\n\n# Plot this thing\nggplot(posterior_draws, aes(x = .value, y = fct_rev(.variable), fill = .variable)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(.width = c(0.8, 0.95), alpha = 0.8, point_interval = \"median_hdi\") +\n  guides(fill = \"none\") +\n  labs(x = \"Coefficient\", y = \"Variable\",\n       caption = \"80% and 95% credible intervals shown in black\")\n\n\n\n\n\n\n\n\n\nThose are all the plausible values for these coefficients, given the data that we’ve fed the model, and the black bars at the bottom show the 80% and 95% credible intervals (or the range of values that 80/95% of the posterior covers). With this, there’s a 95% chance that the coefficient for displacement is between −3.35 and −2.48. Neat!"
  },
  {
    "objectID": "resource/bayes.html#confidence-intervals-vs.-credible-intervals",
    "href": "resource/bayes.html#confidence-intervals-vs.-credible-intervals",
    "title": "Bayesian statistics resources",
    "section": "Confidence intervals vs. credible intervals",
    "text": "Confidence intervals vs. credible intervals\nIn session 6, we talked about frequentist confidence intervals and Bayesian credible (or posterior) intervals, since I had you read Guido Imbens’s essay on p-values, where his conclusion is that:\n\nIt would be preferable if reporting standards emphasized confidence intervals or standard errors, and, even better, Bayesian posterior intervals.\n\nImbens wants us to use Bayesian posterior intervals (or credible intervals), but how do we do that?\n\nFrequentist confidence intervals\nIn frequentist statistics (i.e. all the statistics you’ve been exposed to in this class and all previous classes), your whole goal is to estimate and infer something about a population using a sample. This “something” is a true (but unknown) thing called a population parameter. It is a single fixed value that exists out in the world, and it’s the main thing you’re interested in discovering. Here are a bunch of different population parameters:\n\nAverage treatment effect of a program\nProportion of left-handed students at GSU\nMedian rent of apartments in NYC\nProportion of red M&Ms produced in a factory\n\nIn frequentist statistics, we take a sample from the population, calculate the parameter (i.e. mean, median, proportion, whatever) in the sample, and then check to see how good of a guess it might be for the whole population. To do that, we can look at a confidence interval. Think of a confidence interval as a net—it’s a range of possible values for the population parameters, and we can be X% confident (typically 95%) that the net is picking up the population parameter. Another way to think about it is to imagine taking more samples. If you take 100 samples, at least 95 of them would have the true population parameter in their 95% confidence intervals. Frequentist statistics assumes that the unknown population parameter is fixed and singular, but that the data can vary—you can repeat an experiment over and over again, or take repeated samples from a population in order to be more certain about the estimate of the parameter (and shrink the net of the confidence interval).\nImportantly, when talking about confidence intervals, you cannot really say anything about the estimate of the parameter itself. Confidence intervals are all about the net, or the range itself. You can legally say this:\n\nWe are 95% confident that this confidence interval captures the true population parameter.\n\nYou cannot say this:\n\nThere’s a 95% chance that the population parameter is X. or There’s a 95% chance that the true value falls in this range.\n\nConfidence intervals tell you about the range, or the net. That’s all.\nHere’s an example with some data from The Effect on restaurant inspections in Alaska. We want to know if weekend inspections are more lenient that ones conducted during the work week.\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(gghalves)\n\ninspections &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/causaldata/restaurant_inspections.csv\")\n\n\nFirst we should look at the data to see if there are any obvious patterns. Let’s look at scores separated by weekend status. We’ll use the neat gghalves package to plot both the raw points and a density plot. The orange points show the average value:\n\n\nCode\nggplot(inspections, aes(x = Weekend, y = inspection_score)) +\n  geom_half_point(side = \"l\", alpha = 0.2, size = 0.5,\n                  transformation = position_jitter(height = 0)) +\n  geom_half_violin(side = \"r\") +\n  stat_summary(fun.data = \"mean_se\", fun.args = list(mult = 1.96), color = \"orange\")\n\n\n\n\n\n\n\n\n\nIt looks like weekend inspections are far more rare than weekday ones, and no weekend inspections every give scores lower than 80. It also looks like the average weekend score is slightly higher than the average weekday score. Let’s figure out how much of a difference there is.\nBut first, we’ll use the language of inference and sampling. Our population parameter (we’ll call it the Greek letter theta, or \\(\\theta\\)) is some single true fixed number that exists out in the world—weekend restaurant inspections in Alaska have a \\(\\theta\\) higher average score than weekday inspections. We want to find out what that \\(\\theta\\) is, so we’ll look at some confidence intervals.\nWe can look at a basic difference in means based on weekend status:\n\n\nCode\nmodel_naive &lt;- lm(inspection_score ~ Weekend, \n                  data = inspections)\ntidy(model_naive, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic    p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    93.6     0.0381   2458.   0             93.6      93.7 \n## 2 WeekendTRUE     2.10    0.433       4.84 0.00000131     1.25      2.95\n\n\nHere, weekend scores are 2.1 points higher than weekday scores, on average (that’s our estimate, or \\(\\hat{\\theta}\\). We have a confidence interval of 1.2–2.9. We cannot say that we’re 95% confident that the weekend score boost (or \\(\\theta\\)) is between 1.2 and 2.9. What we can say is that we’re 95% confident that the range 1.2–2.9 captures the true population parameter \\(\\theta\\). If we took a bunch of different samples of inspection scores and calculated the average weekend vs. weekday score in each of those samples, 95% of those confidence intervals should capture the true \\(\\theta\\). Importantly, we still have no idea what the actual \\(\\theta\\) is, but we’re pretty sure that our confidence interval net has captured it.\nThis estimate is probably wrong, since there are other factors that confound the weekend → score relationship. Maybe the health department only conducts weekend inspections in places with lots of branches, or maybe they did more weekend inspections in certain years. We can adjust/control for these in the model:\n\n\nCode\nmodel_adjusted &lt;- lm(inspection_score ~ Weekend + NumberofLocations + Year, \n                     data = inspections)\ntidy(model_adjusted, conf.int = TRUE)\n## # A tibble: 4 × 7\n##   term              estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)       225.     12.4          18.1  7.88e-73 200.      249.    \n## 2 WeekendTRUE         1.43    0.419         3.42 6.25e- 4   0.611     2.25  \n## 3 NumberofLocations  -0.0191  0.000436    -43.9  0         -0.0200   -0.0183\n## 4 Year               -0.0646  0.00617     -10.5  1.45e-25  -0.0767   -0.0525\n\n\nOur weekend estimate shrunk a little and is now 1.43, with a confidence interval of 0.6–2.3. Again, think of this as a net—we’re 95% sure that the true \\(\\theta\\) is in that net somewhere. \\(\\theta\\) could be 0.7, it could be 1.4, it could be 2.2—who knows. All we know is that our net most likely picked it up.\nFor fun, let’s plot both these weekend estimates and their confidence intervals:\n\n\nCode\n# Save just the weekend coefficient from both models\nfreq_results_naive &lt;- tidy(model_naive, conf.int = TRUE) %&gt;% \n  mutate(model = \"Naive model\") %&gt;% \n  filter(term == \"WeekendTRUE\")\n\nfreq_results_full &lt;- tidy(model_adjusted, conf.int = TRUE) %&gt;% \n  mutate(model = \"Full model\") %&gt;% \n  filter(term == \"WeekendTRUE\")\n\n# Put these coefficients in a single dataset and plot them\nfreq_results &lt;- bind_rows(freq_results_naive, freq_results_full) %&gt;% \n  # Make sure the model name follows the order it appears in the data instead of\n  # alphabetical order\n  mutate(model = fct_inorder(model))\n\nggplot(freq_results, aes(x = estimate, y = model, color = model)) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nBayesian credible intervals\nRemember, with frequentist statistics, \\(\\theta\\) is fixed and singular and we’re hoping to pick it up with our confidence interval nets. The data we collect is variable—we can hypothetically take more and more samples and calculate a bunch of confidence intervals and become more certain about where \\(\\theta\\) might be. We can only interpret confidence intervals as ranges: “There’s a 95% probability that the range contains the true value \\(\\theta\\)”. We can’t say anything about the estimate of \\(\\theta\\) itself. We’ve calculated the probability of the range, not the probability of the actual value.\nBayesian analysis, however, does let us talk about the probability of the actual value. Under Bayesianism, the data you’re working with is fixed (i.e. you collected it once and it’s all you have—you can’t go out and collect infinite additional samples), and the population parameter \\(\\theta\\) varies and has uncertainty about it (i.e. instead of imagining some single number uncapturable that’s the average difference in weekend vs. weekday scores, \\(\\theta\\) has some range around it).\nThis difference is apparent in the formulas for testing hypotheses under each of these approaches:\n\\[\\underbrace{P(\\text{Data} \\mid \\theta)}_{\\substack{\\textbf{Frequentism} \\\\ \\text{Probability of seeing the} \\\\ \\text{data given that } \\theta \\text{ exists}}} \\qquad \\underbrace{P(\\theta \\mid \\text{Data})}_{\\substack{\\textbf{Bayesianism} \\\\ \\text{Probability of } \\theta \\\\ \\text{given the current data}}}\\]\nBayes’ theorem has a nice formula (with neat intuition, like in this video):\n\\[\n\\underbrace{P(\\theta \\mid \\text{Data})}_{\\text{Posterior}} = \\frac{\\overbrace{P(\\theta)}^{\\text{Prior}} \\times \\overbrace{P(\\text{Data} \\mid \\theta)}^{\\text{Likelihood}}}{P(\\text{Data})}\n\\]\nPut (hopefully!) simply, combine the observed likelihood of the data \\(P(\\text{Data} \\mid \\theta)\\) (that’s basically frequentism!) with prior knowledge about the distribution of \\(\\theta\\) and you’ll get a posterior estimate of \\(\\theta\\).\nActually calculating this with real data, though, can be tricky and computationally intensive—often there’s no formal mathematical way to figure out the actual equation. So instead, we can use computers to simulate thousands of guesses and then look at the distribution of those guesses (just like we did with the Zilch simulation in class). One modern method for doing this is called Monte Carlo Markov Chain (MCMC) simulation, which is what most R-based tools for Bayesian stats use nowadays.\nLet’s look at restaurant inspection scores on the weekend Bayesianly. Here, we’re still interested in our population parameter \\(\\theta\\), or the average weekend score boost. Only now, we’re not assuming that \\(\\theta\\) is some single fixed value out in the world that we’re trying to capture with confidence intervals—we’ll use the data that we have to estimate the variation in \\(\\theta\\). The easiest way to do Bayesian analysis with R is with the brms package, which uses the familiar formula syntax you’ve been using with lm(). The syntax is super similar, just with a few additional arguments:\nMCMC things\nArguments like chains, iter, and cores deal with the simulation. chains defines how many parallel simulations should happen, iter controls how many iterations should happen with in each chain, and cores spreads those chains across the CPUs in your computer (i.e. if you have a 4-core computer, you can run 4 chains all at the same time; run parallel::detectCores() in your R console to see how many CPU cores you have). seed makes it so that the random simulation results are reproducible (see here for more on seeds).\nPriors\nThese define your prior beliefs about the parameters (i.e. \\(\\theta\\)) in the model. If you think that the restaurant weekend inspection boost is probably positive, but could possibly be negative, or maybe zero, you can feed that belief into the model. For instance, if you’re fairly confident (based on experiences in other states maybe) that weekend scores really are higher, you can provide an informative prior that says that \\(\\theta\\) is most likely 1.5 points ± a little variation, following a normal distribution. Or, if you have no idea what it could be—maybe it’s super high like 10, maybe it’s negative like −5, or maybe it’s 0 and there’s no weekend boost—you can provide a vague prior that says that \\(\\theta\\) is 0 points ± a ton of variation.\n\n\nCode\nlibrary(patchwork)  # For combining ggplot plots\n\nplot_informative &lt;- ggplot() +\n  stat_function(fun = dnorm, args = list(mean = 1.5, sd = 0.5),\n                geom = \"area\", fill = \"grey80\", color = \"black\") +\n  xlim(-1, 4) +\n  labs(title = \"Informative prior\", subtitle = \"Normal(mean = 1.5, sd = 0.5)\",\n       caption = \"We're pretty sure θ is around 1.5\")\n\nplot_vague &lt;- ggplot() +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 10),\n                geom = \"area\", fill = \"grey80\", color = \"black\") +\n  xlim(-35, 35) +\n  labs(title = \"Vague prior\", subtitle = \"Normal(mean = 0, sd = 10)\",\n       caption = \"Surely θ is in there *somewhere*\")\n\nplot_informative | plot_vague\n\n\n\n\n\n\n\n\n\nFor the sake of this example, we’ll use a vague prior.\nHere’s how to officially do Bayesian analysis with brms and incorporate prior information about \\(\\theta\\). Again, the syntax is super similar to lm(), just with some extra bits about the prior and the MCMC settings:\n\n\nCode\nlibrary(brms)         # For Bayesian regression with brm()\nlibrary(broom.mixed)  # For tidy() and glance() with brms-based models\nlibrary(tidybayes)    # For extracting posterior draws\nlibrary(ggdist)       # For making pretty posterior plots\n\n\n\n\nCode\n# bf() stands for \"bayes formula\"; you technically don't need to use it, but it \n# makes life easier for more complex models, so it's good practice even when \n# using a simple formula like the one here\n#\n# This will take a little bit of time to run. Here's what it's actually doing:\n#\n# 1. Translate this R code to Stan (a specific language for doing Bayesian stuff with MCMC)\n# 2. Compile the Stan code to faster-running C++ code\n# 3. Actually do the MCMC sampling\n\n# Set the prior for the weekend coefficient\n# Use get_priors() to see all the other default priors\npriors &lt;- c(\n  prior(normal(0, 10), class = \"b\", coef = \"WeekendTRUE\")\n)\n\n# Run the model!\nmodel_bayes &lt;- brm(bf(inspection_score ~ Weekend + NumberofLocations + Year),\n                   data = inspections,\n                   prior = priors,\n                   chains = 4, iter = 2000, cores = 4, seed = 1234)\n## Compiling Stan program...\n## Start sampling\n\n\nPhew. That took a while to run, but it ran! Now we can check the results:\n\n\nCode\ntidy(model_bayes, conf.int = TRUE)\n## # A tibble: 5 × 8\n##   effect   component group    term              estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       225.     12.7      201.      251.    \n## 2 fixed    cond      &lt;NA&gt;     WeekendTRUE         1.42    0.421      0.614     2.26  \n## 3 fixed    cond      &lt;NA&gt;     NumberofLocations  -0.0191  0.000431  -0.0200   -0.0183\n## 4 fixed    cond      &lt;NA&gt;     Year               -0.0648  0.00633   -0.0775   -0.0528\n## 5 ran_pars cond      Residual sd__Observation     6.04    0.0264     5.99      6.09\n\n\nOur estimate for the weekend boost, or \\(\\hat{\\theta}\\), is 1.42, which is basically the same as the frequentist estimate we found before. We have an interval too, but it’s not a confidence interval—it’s a credible interval. Instead of telling us about the range of the confidence interval net, this credible interval tells us the probability that \\(\\hat{\\theta}\\) falls in that range. It’s essentially the probability of the actual value, not the probability of the range. Based on this, there’s a 95% chance that—given the data we have—the weekend score boost (\\(\\hat{\\theta}\\)) is between 0.61 and 2.26.\nWe can visualize this posterior distribution to see more information than we could with our frequentist estimate. Remember, our simulation estimated thousands of possible coefficients for WeekendTRUE, and each of them are equally likely. The value that we see in tidy() is the median of all these simulated coefficients, or draws. We can see a few of them here:\n\n\nCode\nmodel_bayes %&gt;%\n  spread_draws(b_WeekendTRUE) %&gt;%\n  head(10)\n## # A tibble: 10 × 4\n##    .chain .iteration .draw b_WeekendTRUE\n##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;         &lt;dbl&gt;\n##  1      1          1     1         0.661\n##  2      1          2     2         0.673\n##  3      1          3     3         0.724\n##  4      1          4     4         1.25 \n##  5      1          5     5         1.69 \n##  6      1          6     6         1.32 \n##  7      1          7     7         1.39 \n##  8      1          8     8         1.63 \n##  9      1          9     9         1.74 \n## 10      1         10    10         1.73\n\n\nSometimes the weekend boost is 1.2, sometimes 1.7, sometimes 1.3, etc. There’s a lot of variation in there. We can plot all these simulated coefficients to see where they mostly cluster:\n\n\nCode\nweekend_draws &lt;- model_bayes %&gt;%\n  spread_draws(b_WeekendTRUE)\n\nggplot(weekend_draws, aes(x = b_WeekendTRUE)) +\n  stat_halfeye() +\n  labs(caption = \"Point shows median value;\\nthick black bar shows 66% credible interval;\\nthin black bar shows 95% credible interval\")\n\n\n\n\n\n\n\n\n\nThe weekend point boost \\(\\hat{\\theta}\\) is mostly clustered around 1–2, and 95% of those draws are between 0.61 and 2.26. We’re thus 95% sure that the actual weekend point boost is between 0.61 and 2.26 with a median of 1.42.\nWe can also look at this distribution a slightly different way by collapsing all those posterior draws into 100 possible values. Each of these dots is equally likely, and the true value of \\(\\theta\\) could be any of them, but again, most are clustered around 1.42:\n\n\nCode\nggplot(weekend_draws, aes(x = b_WeekendTRUE)) +\n  stat_dotsinterval(quantiles = 100) +\n  labs(caption = \"Point shows median value;\\nthick black bar shows 66% credible interval;\\nthin black bar shows 95% credible interval\")"
  },
  {
    "objectID": "resource/bayes.html#general-summary-of-intervals",
    "href": "resource/bayes.html#general-summary-of-intervals",
    "title": "Bayesian statistics resources",
    "section": "General summary of intervals",
    "text": "General summary of intervals\nSo, we’ve seen two different philosophies for quantifying uncertainty with confidence intervals and credible intervals. Here’s a general overview of the two approaches and how you can interpret them:\n\n\n\n\n\n\n\n\n\n\n\nFrequentism\nBayesianism\n\n\n\n\nApproach\n\\(P(\\text{Data} \\mid \\theta)\\)\n\\(P(\\theta \\mid \\text{Data})\\)\n\n\n\n\\(\\theta\\) is a fixed single value; data is variable and can be repeatedly sampled\n\\(\\theta\\) is variable and has uncertainty; data is fixed (you only have one sample)\n\n\nHow to do it in R\nlm(...)\nlibrary(brms)\nbrm(...)\n\n\nName\nConfidence interval\nCredible interval (or posterior interval)\n\n\nIntuition\nProbability of the range\nProbability of the actual value\n\n\nInterpretation template\nThere's a 95% probability that this range contains the true value of \\(\\theta\\)\nThere's a 95% probability that the true value of \\(\\theta\\) falls in this range.\n\n\n\nFew people naturally think like this\nPeople do naturally think like this"
  },
  {
    "objectID": "resource/bayes.html#two-ways-of-making-decisions-with-posterior-distributions",
    "href": "resource/bayes.html#two-ways-of-making-decisions-with-posterior-distributions",
    "title": "Bayesian statistics resources",
    "section": "Two ways of making decisions with posterior distributions",
    "text": "Two ways of making decisions with posterior distributions\nIn the world of frequentism, we’re interested in whether coefficients are statistically different from 0 in a null world where there’s no effect. We rely on p-values to see the probability of seeing an estimate at least as large as what we’ve calculated in a hypothetical world where that estimate is actually 0. This is a really non-intuitive way of thinking about the world (imaginary null worlds?!), so everyone always misinterprets p-values.\nRemember what you read in Imbens’s article though—in real life, very few people care about whether a coefficient is significantly different from a hypothetical null. Instead, people want to know how certain you are of the estimate and what it means practically. Is it for sure a positive effect, or could it maybe be zero or maybe be negative? Significance stars can’t tell us much about those questions, but posterior Bayesian intervals can.\n\nProbability of direction\nOne question we can answer with Bayesian results is “How certain are we that this estimate is positive (or negative)?” Are we sure the weekend scores are higher on average, or could they sometimes be negative? Are we sure that the average treatment effect of your program decreases poverty, or could it maybe have a positive effect instead?\nTo figure this out, we can calculate something called the “probability of direction,” or the proportion of posterior draws that are above (or below) some arbitrary number. For instance, what’s the probability that the weekend boost is positive (or greater than 0)?\n\n\nCode\n# Find the proportion of posterior draws that are bigger than 0\nweekend_draws %&gt;% \n  summarize(prop_greater_0 = sum(b_WeekendTRUE &gt; 0) / n())\n## # A tibble: 1 × 1\n##   prop_greater_0\n##            &lt;dbl&gt;\n## 1              1\n\n\nWhoa. 99.9% of the posterior draws for the weekend boost are greater than 0, meaning that there’s a 99.9% chance that the coefficient is positive, given the data we have.\nThe neat thing about the probability of direction is that we can choose whatever value we want as the threshold. Let’s say the state health director wants to know if weekend scores are higher than weekday scores, but she’s fine with just a little boost (weekends are nice! inspectors are happier!). Pretend that she thinks an average difference of 1 or lower isn’t a big concern, but seeing a difference greater than 1 is a signal that weekend inspectors are maybe being too lenient. We can use 1 as our threshold instead:\n\n\nCode\n# Find the proportion of posterior draws that are bigger than 1\nweekend_draws %&gt;% \n  summarize(prop_greater_0 = sum(b_WeekendTRUE &gt; 1) / n())\n## # A tibble: 1 × 1\n##   prop_greater_0\n##            &lt;dbl&gt;\n## 1          0.845\n\n\nBased on this, 84% of the draws are higher than 1, so there’s an 84% chance that the actual \\(\\theta\\) is greater than 1. Notice how there’s no discussion of significance here—no alpha thresholds, no stars, no null worlds. We just have a probability that \\(\\hat{\\theta}\\) is above 1. We can even visualize it. Everything to the right of that vertical line at 1 is “significant” (but not significant with null worlds and stars).\n\n\nCode\nggplot(weekend_draws, aes(x = b_WeekendTRUE)) +\n  stat_halfeye(aes(fill_ramp = stat(x &gt; 1)), fill = \"red\") +\n  scale_fill_ramp_discrete(from = \"darkred\", guide = \"none\") +\n  geom_vline(xintercept = 1) +\n  labs(caption = \"Point shows median value;\\nthick black bar shows 66% credible interval;\\nthin black bar shows 95% credible interval\")\n## Warning: `stat(x &gt; 1)` was deprecated in ggplot2 3.4.0.\n## ℹ Please use `after_stat(x &gt; 1)` instead.\n\n\n\n\n\n\n\n\n\nShould the state health director be concerned? Probably. There’s an 84% chance that weekend inspection scores are at least 1 point higher than weekday scores, on average, given the data we have.\n\n\nRegion of practical equivalence (ROPE)\nLooking at the probability of direction is helpful if you are concerned whether an effect is positive or negative (i.e. greater or less than 0), but it’s also a little weird to think about because we’re testing if something is greater or less than some specific single number. In our example of the health director, we pretended that she cared whether the average weekend score was 1 point higher, but that’s arbitrary.\nAnother approach is that we can think of a range of \\(\\theta\\) where there’s practically no effect. Think of this as a “dead zone” of sorts. If \\(\\hat{\\theta}\\) is 0, we know there’s no effect. If \\(\\hat{\\theta}\\) is something tiny like 0.2 or -0.3, we probably don’t actually care—that’s a tiny amount and could just be because of measurement error. It’s not anything really actionable. If \\(\\hat{\\theta}\\) is big like 1.3 or -2.4 or whatever, then we have cause to worry, but if the estimate is in the “dead zone” (however we want to define it), then we shouldn’t really care or worry.\nThe official Bayesian term for this “dead zone” is the region of practical equivalence (ROPE). There are lots of ways to determine this dead zone—you can base it on experience with the phenomenon (e.g., if you’re the health director and know a lot about inspection scores, you know what kind of score ranges matter), or you can base it on the data you have (e.g., -0.1 * sd(outcome) to 0.1 * sd(outcome)).\nFor this example, let’s pretend that the health director tells you that any effect between −0.5 and 0.5 doesn’t matter—for her, those kind of values would be the same as 0. Now that we have a dead zone or ROPE, we can calculate the proportion of coefficient draws that fall outside of that ROPE:\n\n\nCode\n# Find the proportion of posterior draws that are bigger than 0.5 or less than -0.5\nweekend_draws %&gt;% \n  summarize(prop_outside_rope = 1 - sum(b_WeekendTRUE &gt;= -0.5 & b_WeekendTRUE &lt;= 0.5) / n())\n## # A tibble: 1 × 1\n##   prop_outside_rope\n##               &lt;dbl&gt;\n## 1             0.987\n\n\n\n\nCode\nggplot(weekend_draws, aes(x = b_WeekendTRUE)) +\n  stat_halfeye(aes(fill_ramp = stat(x &gt;= 0.5 | x &lt;= -0.5)), fill = \"red\") +\n  scale_fill_ramp_discrete(from = \"darkred\", guide = \"none\") +\n  annotate(geom = \"rect\", xmin = -0.5, xmax = 0.5, ymin = -Inf, ymax = Inf, fill = \"purple\", alpha = 0.3) +\n  annotate(geom = \"label\", x = 0, y = 0.75, label = \"ROPE\\n(dead zone)\") +\n  labs(caption = \"Point shows median value;\\nthick black bar shows 66% credible interval;\\nthin black bar shows 95% credible interval\")\n\n\n\n\n\n\n\n\n\nGiven this data, 98% of the posterior distribution of the weekend boost is outside of the ROPE, or dead zone, so we can consider this to be “significant” (again, this is a tricky word because it has nothing to do with null worlds and stars!).\nThere are some debates over what you should check with the ROPE. Some people say that you should look at how much of the 95% credible interval is inside the dead zone; other say you should look at how much of the entire distribution is inside the dead zone. We just did the latter, with the whole distribution. If we want to see how much of the area within the credible interval is inside the dead zone, we can change the code a little to filter those observations out:\n\n\nCode\n# Extract the 95% confidence interval range\nweekend_cred_int &lt;- weekend_draws %&gt;% \n  median_hdi()\nweekend_cred_int$.lower\n## [1] 0.61\n\n# Find the proportion of posterior draws that are bigger than 0.5 or less than \n# -0.5, but only look inside the 95% credible interval\nweekend_draws %&gt;% \n  # Only look inside the credible interval\n  filter(b_WeekendTRUE &gt;= weekend_cred_int$.lower & b_WeekendTRUE &lt;= weekend_cred_int$.upper) %&gt;% \n  summarize(prop_outside_rope = 1 - sum(b_WeekendTRUE &gt;= -0.5 & b_WeekendTRUE &lt;= 0.5) / n())\n## # A tibble: 1 × 1\n##   prop_outside_rope\n##               &lt;dbl&gt;\n## 1                 1\n\n\nIf we look only at the 95% credible interval of the posterior, there’s a 0% chance that any of those estimated coefficients are in the dead zone / ROPE. There’s a 100% chance that the credible interval doesn’t touch the ROPE. You can see this visually too—look at the figure above with the purple ROPE. The thin black bar that shows the 95% credible interval doesn’t show up in the purple area.\nWhich approach is better—using full distribution or just using the credible interval? Who knows. That’s up to you.\nFinally, here we decided on the ROPE kind of arbitrarily as −0.5 to 0.5, but there are more systematic ways of doing it. One common and standard suggestion is to use −0.1 and 0.1 times the standard deviation of the outcome variable:\n\n\nCode\nc(-0.1, 0.1) * sd(inspections$inspection_score)\n## [1] -0.63  0.63\n\n\nBased on this approach, our ROPE/dead zone should be −0.63 to 0.63. Let’s see how that looks:\n\n\nCode\n# Find the proportion of posterior draws that are bigger than 0.5 or less than \n# -0.5, but only look inside the 95% credible interval\nweekend_draws %&gt;% \n  # Only look inside the credible interval\n  filter(b_WeekendTRUE &gt;= weekend_cred_int$.lower & b_WeekendTRUE &lt;= weekend_cred_int$.upper) %&gt;% \n  summarize(prop_outside_rope = 1 - sum(b_WeekendTRUE &gt;= -0.63 & b_WeekendTRUE &lt;= 0.63) / n())\n## # A tibble: 1 × 1\n##   prop_outside_rope\n##               &lt;dbl&gt;\n## 1             0.996\n\n\n\n\nCode\nggplot(weekend_draws, aes(x = b_WeekendTRUE)) +\n  stat_halfeye(aes(fill_ramp = stat(x &gt;= 0.63 | x &lt;= -0.63)), fill = \"red\") +\n  scale_fill_ramp_discrete(from = \"darkred\", guide = \"none\") +\n  annotate(geom = \"rect\", xmin = -0.63, xmax = 0.63, ymin = -Inf, ymax = Inf, fill = \"purple\", alpha = 0.3) +\n  annotate(geom = \"label\", x = 0, y = 0.75, label = \"ROPE\\n(dead zone)\") +\n  labs(caption = \"Point shows median value;\\nthick black bar shows 66% credible interval;\\nthin black bar shows 95% credible interval\")\n\n\n\n\n\n\n\n\n\nThis changes our results just a tiny bit. 97% of the full posterior distribution and 99.7% of the credible interval falls outside this ROPE. Neat. We can thus safely say that the weekend effect, or our estimate of \\(\\theta\\) is definitely practical and substantial (or “significant” if we want to play with that language)."
  },
  {
    "objectID": "resource/zilch.html#zilch",
    "href": "resource/zilch.html#zilch",
    "title": "Zilch and simulations",
    "section": "Zilch!",
    "text": "Zilch!\n\nRules\nRoll six dice. You must keep at least one scoring die every roll. You must get at least 500 points before stopping and keeping the total. If you do not get 500 points, you get zilch (0) that round and the next person rolls.\nAfter reaching 500 points, you can either stop, write down, and keep your score or continue to roll. As long as you can keep at least one scoring die, you can continue to roll and accumulate points. If you do not get a scoring combination in a roll, you lose all those points.\nIf you score with all six dice, you can pick them all up and continue rolling with them.\nWhoever reaches 10,000 points first wins.\n\n\nPoint system\n\n\n\n\n\n\n\n\n\nRoll\nPoints\n\n\n\n\nSingle dice\n\n\n\n100\n\n\n\n50\n\n\nThree of a kind (number × 100)\n\n\n  \n200\n\n\n  \n300\n\n\n  \n400\n\n\n  \n500\n\n\n  \n600\n\n\nSpecial rolls\n\n\n  \n1,000\n\n\n   \n2,000\n\n\nAny three pairs\ne.g.,      \n1,000\n\n\n5-die straight\n     or\n    \n1,750"
  },
  {
    "objectID": "resource/zilch.html#probability-with-math",
    "href": "resource/zilch.html#probability-with-math",
    "title": "Zilch and simulations",
    "section": "Probability with math",
    "text": "Probability with math\nYou can use probability math to calculate the chance of rolling at least a 1 or a 5 (so that you score something and can roll again). If you’re only rolling 1 die, the chance of rolling a 1 or a 5 is \\(\\frac{2}{6}\\), or \\(\\frac{1}{3}\\). When you start thinking about 2 dice, though, the math gets a little trickier because you care about rolling at least a 1 or 5, and there’s a chance you could roll both. So instead, we can calculate the probability of not rolling a 1 or a 5 and then subtract that from 1.\nWith 2 dice, the probability of not rolling a 1 or a 5 is \\(\\frac{4}{6}\\) or \\(\\frac{2}{3}\\) for each die. To calculate the joint probability, we can multiply each die’s probability:\n\\[\n\\begin{aligned}\n1 - (\\frac{2}{3} \\times \\frac{2}{3}) &= \\\\\n1 - \\frac{4}{9} &= 0.5\\overline{55}\n\\end{aligned}\n\\]\nThat means there’s a 55% chance of rolling at least a 1 or a 5 when rolling 2 dice. This approach scales up to any number of dice—multiply that \\(\\frac{2}{3}\\) for each die included. For instance, here’s 4 dice:\n\\[\n\\begin{aligned}\n1 - (\\frac{2}{3} \\times \\frac{2}{3} \\times \\frac{2}{3} \\times \\frac{2}{3}) &= \\\\\n1 - \\frac{16}{81} &= 0.80\n\\end{aligned}\n\\]\nWe can generalize this by using exponents, where \\(n\\) is the number of dice you’re rolling:\n\\[\n1 - \\frac{2^n}{3^n}\\quad \\text{ or }\\quad 1 - \\left(\\frac{2}{3}\\right)^n\n\\]"
  },
  {
    "objectID": "resource/zilch.html#probabilty-with-math-and-computers",
    "href": "resource/zilch.html#probabilty-with-math-and-computers",
    "title": "Zilch and simulations",
    "section": "Probabilty with math and computers",
    "text": "Probabilty with math and computers\nDoing that math by hand gets tedious, so we can put it in an R function and have R do the math for us.\n\nprob_1_or_5 &lt;- function(n) {\n  1 - (2/3)^n\n}\n\n# 1 die\nprob_1_or_5(1)\n## [1] 0.3333\n\n# 5 dice\nprob_1_or_5(5)\n## [1] 0.8683\n\n# 6 dice\nprob_1_or_5(6)\n## [1] 0.9122"
  },
  {
    "objectID": "resource/zilch.html#probability-with-computers-only",
    "href": "resource/zilch.html#probability-with-computers-only",
    "title": "Zilch and simulations",
    "section": "Probability with computers only",
    "text": "Probability with computers only\nInstead of figuring out the math behind the probability of getting at least a 1 or a 5, we can simulate a bunch of dice rolls and brute force our way to the answer. Here’s the general process:\n\nRoll 6 dice 100,000 times (or whatever number you want)\nCount how many times a 1 or a 5 appears in a roll\nDivide that count by 100,000. That’s the probability.\n\nProbably the most intiuitive (though not necessarily computationally efficient) way to do with with R is to use a for loop. In R, a for loop will will repeat some chunk of repeatedly until some condition is met, and that condition is generally tracked with an index variable that is specific to the inside of the loop. For instance, this loop uses an index variable named i. It will set i to 1 the first time it runs the loop and then do whatever’s inside (print(...) in this case). When it finishes the inside code, it’ll bump i up to the next number (2 here) and run the inside code again, and again, and again until it reaches the end of the index range (4 here):\n\nfor (i in 1:4) {\n  print(paste(\"Loop number\", i))\n}\n## [1] \"Loop number 1\"\n## [1] \"Loop number 2\"\n## [1] \"Loop number 3\"\n## [1] \"Loop number 4\"\n\nTo make this work with our dice simulation, we need to add one more component. We want to store the results of each loop run in a variable that we can use later. The most efficient way to do this is to create an empty variable first that has enough slots in it to contain the output, then add to that variable while going through the loop:\n\n# This is \"logical\" because it's only going to hold TRUE and FALSE values. If we\n# wanted to put numbers in it, we'd need to use \"double\"; if we wanted to put\n# text in it, we'd need to use \"character\"\noutput &lt;- vector(\"logical\", 4)\n\nfor (i in 1:4) {\n  # Check if i is 3 and store the result in the ith slot\n  output[[i]] &lt;- i == 3\n}\n\noutput\n## [1] FALSE FALSE  TRUE FALSE\n\nLet’s build a loop now that rolls 6 dice, checks for a 1 or a 5, and stores the result. We’ll use 100,000 times for fun.\n\n# Create empty variable with enough slots\nhas_1_or_5 &lt;- vector(\"logical\", 100000)\n\n# Roll a bunch of dice a bunch of times\nfor (i in 1:100000) {\n  # Roll some dice\n  rolled_dice &lt;- sample(1:6, 6, replace = TRUE)\n  \n  # Check if there's a 1 or a 5 in there\n  did_it_happen &lt;- 1 %in% rolled_dice | 5 %in% rolled_dice\n  \n  # Store the result  \n  has_1_or_5[i] &lt;- did_it_happen\n}\n\n# Find the proportion of TRUEs\nsum(has_1_or_5) / 100000\n## [1] 0.9122\n\nOut of the 100,000 rolls, 91,220 of them had a 1 or a 5 in them, meaning that there’s a 91.22% chance of scoring something on an initial roll in Zilch.\nHow does that compare to the official math?\n\\[\n1 - \\left( \\frac{2}{3} \\right)^6\n\\]\n\nprob_1_or_5(6)\n## [1] 0.9122\n\nIt’s basically the same! But we found the answer without doing any actual probability math, which is neat.\nWe can generalize this simulation a little more by not hardcoding some of the parameters. We can make it so both the number of dice to roll and the number of simulations are adjustable by sticking this in a function. For example, here’s 4 dice 50,000 times:\n\nsimulate_zilch &lt;- function(n_dice, n_sims) {\n  # Create empty variable with enough slots\n  has_1_or_5 &lt;- vector(\"logical\", n_sims)\n  \n  # Roll a bunch of dice a bunch of times\n  for (i in 1:n_sims) {\n    # Roll some dice\n    rolled_dice &lt;- sample(1:6, n_dice, replace = TRUE)\n    \n    # Check if there's a 1 or a 5 in there\n    did_it_happen &lt;- 1 %in% rolled_dice | 5 %in% rolled_dice\n    \n    # Store the result  \n    has_1_or_5[i] &lt;- did_it_happen\n  }\n  \n  # Find the proportion of TRUEs\n  sum(has_1_or_5) / n_sims\n}\n\nsimulate_zilch(n_dice = 4, n_sims = 50000)\n## [1] 0.8027"
  },
  {
    "objectID": "resource/zilch.html#why-even-do-this",
    "href": "resource/zilch.html#why-even-do-this",
    "title": "Zilch and simulations",
    "section": "Why even do this?",
    "text": "Why even do this?\nBut we know the probability equation for getting at least a 1 or a 5, so why go through the hassle of making the computer roll millions of dice? Because we’re not actually calculating the true probability of scoring every valid scoring combination!\nIn Zilch, you can score with at least a 1 or a 5, but you can also score with three-of-a-kind or three pairs or a 5-die straight. We could calculate the probability of rolling those and combine them with the probability of a 1 or a 5, but the formal equation will get really hairy and complicated. So instead, we can simulate.\n\nsimulate_zilch_full &lt;- function(n_dice, n_sims) {\n  # Create empty variable with enough slots\n  did_something_score &lt;- vector(\"logical\", n_sims)\n  \n  # Roll a bunch of dice a bunch of times\n  for (i in 1:n_sims) {\n    # Roll some dice\n    rolled_dice &lt;- sample(1:6, n_dice, replace = TRUE)\n\n    # Check for 3 pairs separately since it's a complicated process\n    # If there are three different numbers...\n    if (length(table(rolled_dice)) == 3) {\n      # ...check if there are 3 pairs\n      three_pairs &lt;- all(table(rolled_dice) == c(2, 2, 2))\n    } else {\n      three_pairs &lt;- FALSE\n    }\n    \n    # Check if there's a scoring combination in there\n    did_it_happen &lt;- 1 %in% rolled_dice |  # A 1\n      5 %in% rolled_dice |  # A 5\n      all(1:5 %in% rolled_dice) |  # A 1-5 straight\n      all(2:6 %in% rolled_dice) |  # A 2-6 straight\n      max(table(rolled_dice)) &gt;= 3 |  # At least 3 of one number\n      three_pairs  # 3 pairs\n\n    # Store the result\n    did_something_score[i] &lt;- did_it_happen\n  }\n  \n  # Find the proportion of TRUEs\n  sum(did_something_score) / n_sims\n}\n\nsix_dice &lt;- simulate_zilch_full(6, 50000)\n\nPhew! Now that we account for every possible scoring combination, there’s a 97.69% chance of scoring something when rolling 6 dice. That’s really high!\nFor fun, let’s look at how that probability changes as the number of dice you roll decreases:\n\nlibrary(tidyverse)\n\n# hoooo boy this is slow\nnumber_of_dice &lt;- tibble(n_dice = 6:1) %&gt;% \n  mutate(prob = map_dbl(n_dice, ~simulate_zilch_full(., 50000)))\n\n\nggplot(number_of_dice, aes(x = fct_rev(factor(n_dice)), y = prob)) +\n  geom_col() +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(x = \"Number of dice rolled\", y = \"Probability of scoring something\",\n       caption = \"Results over 50,000 simulations per count of dice\") +\n  theme_minimal()"
  },
  {
    "objectID": "resource/rmarkdown.html",
    "href": "resource/rmarkdown.html",
    "title": "Using R Markdown",
    "section": "",
    "text": "R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:"
  },
  {
    "objectID": "resource/rmarkdown.html#key-terms",
    "href": "resource/rmarkdown.html#key-terms",
    "title": "Using R Markdown",
    "section": "Key terms",
    "text": "Key terms\n\nDocument: A Markdown file where you type stuff\nChunk: A piece of R code that is included in your document. It looks like this:\n\n```{r}\n# Code goes here\n```\nThere must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\nKnit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows."
  },
  {
    "objectID": "resource/rmarkdown.html#add-chunks",
    "href": "resource/rmarkdown.html#add-chunks",
    "title": "Using R Markdown",
    "section": "Add chunks",
    "text": "Add chunks\nThere are three ways to insert chunks:\n\nPress ⌘⌥I on macOS or control + alt + I on Windows\nClick on the “Insert” button at the top of the editor window\n\n\n\n\n\n\n\n\n\n\n\nManually type all the backticks and curly braces (don’t do this)"
  },
  {
    "objectID": "resource/rmarkdown.html#chunk-names",
    "href": "resource/rmarkdown.html#chunk-names",
    "title": "Using R Markdown",
    "section": "Chunk names",
    "text": "Chunk names\nYou can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\n\n\n\n\n\n\n\n\n\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk}\n# Code goes here\n```"
  },
  {
    "objectID": "resource/rmarkdown.html#chunk-options",
    "href": "resource/rmarkdown.html#chunk-options",
    "title": "Using R Markdown",
    "section": "Chunk options",
    "text": "Chunk options\nThere are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE}\n# Code goes here\n```\nThe most common chunk options are these:\n\nfig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures\necho=FALSE: The code is not shown in the final document, but the results are\nmessage=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted\nwarning=FALSE: Any warnings that R generates are omitted\ninclude=FALSE: The chunk still runs, but the code and results are not included in the final document\n\nYou can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:"
  },
  {
    "objectID": "resource/rmarkdown.html#inline-chunks",
    "href": "resource/rmarkdown.html#inline-chunks",
    "title": "Using R Markdown",
    "section": "Inline chunks",
    "text": "Inline chunks\nYou can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE}\navg_mpg &lt;- mean(mtcars$mpg)\n```\n\nThe average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon.\n… would knit into this:\n\nThe average fuel efficiency for cars from 1974 was 20.1 miles per gallon."
  },
  {
    "objectID": "resource/rmarkdown.html#output-formats",
    "href": "resource/rmarkdown.html#output-formats",
    "title": "Using R Markdown",
    "section": "Output formats",
    "text": "Output formats\nYou can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \"My document\"\noutput:\n  html_document: default\n  pdf_document: default\n  word_document: default\nYou can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\n\n\n\n\n\n\n\n\n\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n---\ntitle: \"My document\"\nauthor: \"My name\"\ndate: \"January 13, 2020\"\noutput: \n  html_document: \n    toc: yes\n    fig_caption: yes\n    fig_height: 8\n    fig_width: 10\n  pdf_document: \n    latex_engine: xelatex  # More modern PDF typesetting engine\n    toc: yes\n  word_document: \n    toc: yes\n    fig_caption: yes\n    fig_height: 4\n    fig_width: 5\n---"
  },
  {
    "objectID": "resource/unzipping.html",
    "href": "resource/unzipping.html",
    "title": "Unzipping files",
    "section": "",
    "text": "Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows."
  },
  {
    "objectID": "resource/unzipping.html#unzipping-files-on-macos",
    "href": "resource/unzipping.html#unzipping-files-on-macos",
    "title": "Unzipping files",
    "section": "Unzipping files on macOS",
    "text": "Unzipping files on macOS\nDouble click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started."
  },
  {
    "objectID": "resource/unzipping.html#unzipping-files-on-windows",
    "href": "resource/unzipping.html#unzipping-files-on-windows",
    "title": "Unzipping files",
    "section": "Unzipping files on Windows",
    "text": "Unzipping files on Windows\ntl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\n\n\n\n\n\n\n\n\n\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\n\n\n\n\n\n\n\n\n\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\n\n\n\n\n\n\n\n\n\nThen choose where you want to unzip all the files and click on “Extract”\n\n\n\n\n\n\n\n\n\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work."
  },
  {
    "objectID": "content/02-content.html#before-class",
    "href": "content/02-content.html#before-class",
    "title": "Bayes as coutning",
    "section": "Before class",
    "text": "Before class\n\n The Golum of Prague (51 min)\n The Garden of Forking Data (1 h 37 min)",
    "crumbs": [
      "Content",
      "Course content",
      "2: Bayes as counting"
    ]
  },
  {
    "objectID": "content/02-content.html#during-class",
    "href": "content/02-content.html#during-class",
    "title": "Bayes as coutning",
    "section": "During class",
    "text": "During class\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.",
    "crumbs": [
      "Content",
      "Course content",
      "2: Bayes as counting"
    ]
  },
  {
    "objectID": "content/01-content.html#before-class",
    "href": "content/01-content.html#before-class",
    "title": "Introduction to Bayes",
    "section": "Before class",
    "text": "Before class\nNothing. But if you want to get familiar with the site or ee what I’m using as the basis for the first lecture, here you go:\n\nThe syllabus, content, examples, and assignments pages for this class\n Introduction to Bayesian Inference for Psychology (Etz & Vandekerckhove, 2018)",
    "crumbs": [
      "Content",
      "Course content",
      "1: Introduction to Bayesian Analysis"
    ]
  },
  {
    "objectID": "content/01-content.html#during-class",
    "href": "content/01-content.html#during-class",
    "title": "Introduction to Bayes",
    "section": "During class",
    "text": "During class\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.",
    "crumbs": [
      "Content",
      "Course content",
      "1: Introduction to Bayesian Analysis"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "PSY 607 Bayesian Analysis",
    "section": "",
    "text": "Instructor: Dr. Sara Weston\nEmail: sweston2@uoregon.edu\nOffice: Straub 325\nOffice Hours: Friday 10:00 AM - 12:00 PM, and by appointment (schedule here)\nClass Meetings: Tuesday/Thursday 2:00-3:20 PM\nLocation: 119 FEN\nCourse Website: uobayes.netlify.app"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "PSY 607 Bayesian Analysis",
    "section": "Course Description",
    "text": "Course Description\nThis advanced statistics course introduces students to Bayesian methods with a focus on practical application using modern computational tools. Through a combination of theoretical foundations and hands-on programming, students learn to build, interpret, and critique Bayesian models using the R programming language, with particular emphasis on the brms package."
  },
  {
    "objectID": "syllabus.html#course-structure",
    "href": "syllabus.html#course-structure",
    "title": "PSY 607 Bayesian Analysis",
    "section": "Course Structure",
    "text": "Course Structure\nThis course is a mix of asynchronous lectures, in-class activities, and homework assignments:\n\nBefore each class, you will watch a video lecture. These will be posted on the course website alongside each lecture.\nAt the beginning of each class, there will be a short quiz to check your understanding of the lecture and help me know what to review.\nClass time will be used for in-class activities, including group work and discussion.\nEach week will have a homework assignment. These will be posted on Canvas.\nYou can find all the materials for each week on the course website."
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "PSY 607 Bayesian Analysis",
    "section": "Materials",
    "text": "Materials\nRequired:\n\nR and RStudio (free)\nRequired R packages:\n\nrethinking\nbrms\ntidybase\ntidyverse\n\n\nRecommended:\n\nStatistical Rethinking by Richard McElreath (2nd ed.)\n\nAdditional course materials, including lecture notes and code, will be provided on the course website."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "PSY 607 Bayesian Analysis",
    "section": "Grading",
    "text": "Grading\n\nHomework (95%)\n\n10 assignments\nGraded on completion (not accuracy)\nPosted on Friday, due the following Monday at 11:59pm\nOne week grace period for each assignment\n\nQuizzes (5%)\n\n19 quizzes\nCover material from the day’s recorded lecture\nCompleted in class\nNo makeup opportunities for missed quizzes"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "PSY 607 Bayesian Analysis",
    "section": "Course Policies",
    "text": "Course Policies\n\nCommunication\nIf you have questions about course policies, have trouble submitting an assignment, or want to schedule a meeting, please email. I will make an effort to respond to emails within one business day. Note that I neither plan nor commit to checking email outside of normal business hours (9am-5pm, Mon-Fri).\nIf you are having trouble understanding a concept covered in class, please come to office hours, schedule a meeting with me, ask for clarification during class periods, or use the class Slack to get help from the group. I will not explain course concepts over email.\nOccasionally, I will send out announcements to the entire class via Canvas announcements. These will typically appear when you open Canvas, but you can update your Canvas settings to receive these announcements as emails. It is strongly recommended that you do so.\n\n\nClassroom Expectations\nAll members of the class (students and instructor) can expect to:\nParticipate and Contribute: All students are expected to participate by sharing ideas and contributing to the learning environment. This entails preparing, following instructions, and engaging respectfully and thoughtfully with others.\nWhile all students should participate, participation is not just talking, and a range of participation activities support learning. Participation might look like speaking aloud in the full class and in small groups and collaborating on homework assignments.\nExpect and Respect Diversity: All classes at the University of Oregon welcome and respect diverse experiences, perspectives, and approaches. What is not welcome are behaviors or contributions that undermine, demean, or marginalize others based on race, ethnicity, gender, sex, age, sexual orientation, religion, ability, or socioeconomic status. We will value differences and communicate disagreements with respect.\nHelp Everyone Learn: Part of how we learn together is by learning from one another. To do this effectively, we need to be patient with each other, identify ways we can assist others, and be open-minded to receiving help and feedback from others. Don’t hesitate to contact me to ask for assistance or offer suggestions that might help us learn better.\n\n\nWorkload\nThis is a 3-credit hour course, so you should expect to complete 120 hours of work for the course—an average of about 12 hours each week (this includes time in-class).\n\n\nGenerative AI Policy\nWhile generative AI tools (such as ChatGPT, Claude, and Cursor) can be useful for assisting with certain tasks, it’s important to use these technologies responsibly and ethically.\nGood uses of AI:\n\nExploring concepts, generating code ideas, and providing explanations for topics covered in the course\nCreating preliminary drafts of code, data analysis plans, or written reports (with review and testing)\nBrainstorming research questions, identifying key terms, or helping summarize complex literature\n\nBad uses of AI:\n\nGenerating entire assignments, projects, or reports without manual oversight\nUsing AI in place of collaborating with peers\nSubmitting AI-generated content without proper attribution\n\nRequired Disclosure: Students who use AI tools must include a brief statement at the end of their submission describing how and where the tool was used. Failure to disclose AI use may result in grade reduction or disciplinary action.\n\n\nAcademic Integrity\nFrankly, you’re in graduate school, and the purpose of work is to create opportunities to learn and improve. Even if cheating helps you in the short term, you’ll quickly find yourself ill-prepared for the career you have chosen. If you find yourself tempted to cheat, please come speak to Dr. Weston about an extension and developing tools to improve your success.\n\n\nAccessibility\nThe University of Oregon and I are dedicated to fostering inclusive learning environments for all students and welcomes students with disabilities into all of the University’s educational programs. The Accessible Education Center (AEC) assists students with disabilities in reducing campus-wide and classroom-related barriers. If you have or think you have a disability (https://aec.uoregon.edu/content/what-disability) and experience academic barriers, please contact the AEC to discuss appropriate accommodations or support. Visit 360 Oregon Hall or aec.uoregon.edu for more information. You can contact AEC at 541-346-1155 or via email at uoaec@uoregon.edu.\n\n\nBasic Needs\nBeing able to meet your basic needs is foundational to your success as a student at the University of Oregon. If you are having difficulty affording food, don’t have a stable, safe place to live, or are struggling to meet another need, visit the UO Basic Needs Resource page for information on how to get support. They have information food, housing, healthcare, childcare, transportation, technology, finances (including emergency funds), and legal support.\nIf your need is urgent, please contact the Care and Advocacy Program by calling 541-346-3216, filling out the Community Care and Support form, or by scheduling an appointment with an advocate.\n\n\nReporting Obligations\nI am a designated reporter. For information about my reporting obligations as an employee, please see Employee Reporting Obligations on the Office of Investigations and Civil Rights Compliance (OICRC) website. Students experiencing sex- or gender-based discrimination, harassment or violence should call the 24-7 hotline 541-346-SAFE [7244] or visit safe.uoregon.edu for help. Students experiencing all forms of prohibited discrimination or harassment may contact the Dean of Students Office at 541-346-3216 or the non-confidential Title IX Coordinator/OICRC at 541-346-3123 to request information and resources. Students are not required to participate in an investigation to receive support, including requesting academic supportive measures. Additional resources are available at investigations.uoregon.edu/how-get-support.\nI am also a mandatory reporter of child abuse. Please find more information at Mandatory Reporting of Child Abuse and Neglect.\n\n\nPregnancy Accommodations\nPregnant and parenting students are eligible for academic and work modifications related to pregnancy, childbirth, loss of pregnancy, termination of pregnancy, lactation, and related medical conditions. To request pregnancy-related modifications, students should complete the Request for Pregnancy Modifications form on the OICRC website. OICRC coordinates academic and other modifications for pregnant and parenting students to ensure students can continue to access their education and university programs and activities.\n\n\nCampus Emergencies\nIn the event of a campus emergency that disrupts academic activities, course requirements, deadlines, and grading percentages are subject to change. Information about changes in this course will be communicated as soon as possible by email, and on Canvas. If we are not able to meet face-to-face, students should immediately log onto Canvas and read any announcements and/or access alternative assignments. Students are also expected to continue coursework as outlined in this syllabus or other instructions on Canvas."
  },
  {
    "objectID": "example/rstudio-tidyverse.html",
    "href": "example/rstudio-tidyverse.html",
    "title": "R, RStudio, and the tidyverse",
    "section": "",
    "text": "This course assumes that you’re already familiar with R, RStudio, Markdown, and the tidyverse. If you’re not, or if you’re looking to brush up, here are some resources for helping you get up to speed."
  },
  {
    "objectID": "example/rstudio-tidyverse.html#part-1-the-basics-of-r-and-dplyr",
    "href": "example/rstudio-tidyverse.html#part-1-the-basics-of-r-and-dplyr",
    "title": "R, RStudio, and the tidyverse",
    "section": "Part 1: The basics of R and dplyr",
    "text": "Part 1: The basics of R and dplyr\n\nThe Basics\n\nVisualization Basics\nProgramming Basics\n\nWork with Data\n\nWorking with Tibbles\nIsolating Data with dplyr\nDeriving Information with dplyr\n\nVisualize Data\n\nExploratory Data Analysis\nBar Charts\nHistograms\nBoxplots and Counts\nScatterplots\nLine plots\nOverplotting and Big Data\nCustomize Your Plots\n\nTidy Your Data\n\nReshape Data\n\n\n\n\n\n\n\n\nNote\n\n\n\nRecent versions of tidyr have renamed these core functions: gather() is now pivot_longer() and spread() is now pivot_wider(). The syntax for these pivot_*() functions is slightly different from what it was in gather() and spread(), so you can’t just replace the names. Fortunately, both gather() and spread() still work and won’t go away for a while, so you can still use them as you learn about reshaping and tidying data. It would be worth learning how the newer pivot_*() functions work, eventually, though (see here for examples).\n\n\nThe content from these primers comes from the (free and online!) book R for Data Science by Garrett Grolemund and Hadley Wickham. I highly recommend the book as a reference and for continuing to learn and use R in the future."
  },
  {
    "objectID": "example/rstudio-tidyverse.html#part-2-getting-familiar-with-rstudio",
    "href": "example/rstudio-tidyverse.html#part-2-getting-familiar-with-rstudio",
    "title": "R, RStudio, and the tidyverse",
    "section": "Part 2: Getting familiar with RStudio",
    "text": "Part 2: Getting familiar with RStudio\nThe RStudio primers you just worked through are a great introduction to writing and running R code, but you typically won’t type code in a browser when you work with R. Instead, you’ll use a nicer programming environment like RStudio, which lets you type and save code in scripts, run code from those scripts, and see the output of that code, all in the same program.\nTo get familiar with RStudio, watch this video by Andrew Heiss:"
  },
  {
    "objectID": "example/rstudio-tidyverse.html#part-3-rstudio-projects",
    "href": "example/rstudio-tidyverse.html#part-3-rstudio-projects",
    "title": "R, RStudio, and the tidyverse",
    "section": "Part 3: RStudio Projects",
    "text": "Part 3: RStudio Projects\nOne of the most powerful and useful aspects of RStudio is its ability to manage projects.\nWhen you first open R, it is “pointed” at some folder on your computer, and anything you do will be relative to that folder. The technical term for this is a “working directory.”\nWhen you first open RStudio, look in the area right at the top of the Console pane to see your current working directory. Most likely you’ll see something cryptic: ~/\n\n\n\n\n\n\n\n\n\nThat tilde sign (~) is a shortcut that stands for your user directory. On Windows this is C:\\Users\\your_user_name\\; on macOS this is /Users/your_user_name/. With the working directory set to ~/, R is “pointed” at that folder, and anything you save will end up in that folder, and R will expect any data that you load to be there too.\nIt’s always best to point R at some other directory. If you don’t use RStudio, you need to manually set the working directory to where you want it with setwd(), and many R scripts in the wild include something like setwd(\"C:\\\\Users\\\\bill\\\\Desktop\\\\Important research project\") at the beginning to change the directory. THIS IS BAD THOUGH (see here for an explanation). If you ever move that directory somewhere else, or run the script on a different computer, or share the project with someone, the path will be wrong and nothing will run and you will be sad.\nThe best way to deal with working directories with RStudio is to use RStudio Projects. These are special files that RStudio creates for you that end in a .Rproj extension. When you open one of these special files, a new RStudio instance will open up and be pointed at the correct directory automatically. If you move the folder later or open it on a different computer, it will work just fine and you will not be sad.\nRead this super short chapter on RStudio projects to learn how to create and use them\nIn general, you can create a new project by going to File &gt; New Project &gt; New Directory &gt; Empty Project, which will create a new folder on your computer that is empty except for a single .Rproj file. Double click on that file to open an RStudio instance that is pointed at the correct folder."
  },
  {
    "objectID": "example/rstudio-tidyverse.html#part-4-getting-familiar-with-r-markdown",
    "href": "example/rstudio-tidyverse.html#part-4-getting-familiar-with-r-markdown",
    "title": "R, RStudio, and the tidyverse",
    "section": "Part 4: Getting familiar with R Markdown",
    "text": "Part 4: Getting familiar with R Markdown\nTo ensure that the analysis and graphics you make are reproducible, you’ll do the majority of your work in this class using R Markdown files.\nDo the following things:\n\nWatch this video:\n\n\n\n\n\n \n\nSkim through the content at these pages:\n\nUsing Markdown\nUsing R Markdown\nHow it Works\nCode Chunks\nInline Code\nMarkdown Basics (The R Markdown Reference Guide is super useful here.)\nOutput Formats"
  },
  {
    "objectID": "assignment/09-problem-set.html",
    "href": "assignment/09-problem-set.html",
    "title": "Problem set 9",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 9"
    ]
  },
  {
    "objectID": "assignment/09-problem-set.html#instructions",
    "href": "assignment/09-problem-set.html#instructions",
    "title": "Problem set 9",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 9"
    ]
  },
  {
    "objectID": "assignment/09-problem-set.html#questions",
    "href": "assignment/09-problem-set.html#questions",
    "title": "Problem set 9",
    "section": "Questions",
    "text": "Questions\nTBD",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 9"
    ]
  },
  {
    "objectID": "assignment/07-problem-set.html",
    "href": "assignment/07-problem-set.html",
    "title": "Problem set 7",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 7"
    ]
  },
  {
    "objectID": "assignment/07-problem-set.html#instructions",
    "href": "assignment/07-problem-set.html#instructions",
    "title": "Problem set 7",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 7"
    ]
  },
  {
    "objectID": "assignment/07-problem-set.html#questions",
    "href": "assignment/07-problem-set.html#questions",
    "title": "Problem set 7",
    "section": "Questions",
    "text": "Questions\nTBD",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 7"
    ]
  },
  {
    "objectID": "assignment/05-problem-set.html",
    "href": "assignment/05-problem-set.html",
    "title": "Problem set 5",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 5"
    ]
  },
  {
    "objectID": "assignment/05-problem-set.html#instructions",
    "href": "assignment/05-problem-set.html#instructions",
    "title": "Problem set 5",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 5"
    ]
  },
  {
    "objectID": "assignment/05-problem-set.html#questions",
    "href": "assignment/05-problem-set.html#questions",
    "title": "Problem set 5",
    "section": "Questions",
    "text": "Questions\nTBD",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 5"
    ]
  },
  {
    "objectID": "assignment/03-problem-set.html",
    "href": "assignment/03-problem-set.html",
    "title": "Problem set 3",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 3"
    ]
  },
  {
    "objectID": "assignment/03-problem-set.html#instructions",
    "href": "assignment/03-problem-set.html#instructions",
    "title": "Problem set 3",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 3"
    ]
  },
  {
    "objectID": "assignment/03-problem-set.html#questions",
    "href": "assignment/03-problem-set.html#questions",
    "title": "Problem set 3",
    "section": "Questions",
    "text": "Questions\nTBD",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 3"
    ]
  },
  {
    "objectID": "assignment/01-problem-set.html",
    "href": "assignment/01-problem-set.html",
    "title": "Problem set 1",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 1"
    ]
  },
  {
    "objectID": "assignment/01-problem-set.html#instructions",
    "href": "assignment/01-problem-set.html#instructions",
    "title": "Problem set 1",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 1"
    ]
  },
  {
    "objectID": "assignment/01-problem-set.html#questions",
    "href": "assignment/01-problem-set.html#questions",
    "title": "Problem set 1",
    "section": "Questions",
    "text": "Questions\n\nSuppose the globe tossing experiment turned out to be 3 water and 11 land. Construct the posterior distribution.\nUsing the posterior distribution from 1, compute the posterior predictive distribution for the next 5 tosses of the globe.\n\nYou can download a template file here to help you get started.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 1"
    ]
  },
  {
    "objectID": "resource/r.html",
    "href": "resource/r.html",
    "title": "R",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online.\nCSE 631: Principles & Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio."
  },
  {
    "objectID": "resource/r.html#learning-r",
    "href": "resource/r.html#learning-r",
    "title": "R",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online.\nCSE 631: Principles & Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio."
  },
  {
    "objectID": "resource/r.html#r-in-the-wild",
    "href": "resource/r.html#r-in-the-wild",
    "title": "R",
    "section": "R in the wild",
    "text": "R in the wild\nA popular (and increasingly standard) way for sharing your analyses and visualizations is to post an annotated explanation of your process somewhere online. RStudio allows you to publish knitted HTML files directly to RPubs, but you can also post your output to a blog or other type of website.1 Reading these kinds of posts is one of the best ways to learn R, since they walk you through each step of the process and show the code and output.\nHere are some of the best examples I’ve come across:\n\nText analysis of Trump’s tweets confirms he writes only the (angrier) Android half (with a follow-up)\nBob Ross - Joy of Painting\nBechdel analysis using the tidyverse: There are also a bunch of other examples using data from FiveThirtyEight.\nSexism on the Silver Screen: Exploring film’s gender divide\nComparison of Quentin Tarantino Movies by Box Office and the Bechdel Test\nWho came to vote in Utah’s caucuses?\nHealth care indicators in Utah counties\nSong lyrics across the United States\nA decade (ish) of listening to Sigur Rós\nWhen is Tom peeping these days?: There are a also bunch of final projects from other R and data visualization classes here and here.\nMapping Fall Foliage\nGeneral (Attys) Distributions\nDisproving Approval"
  },
  {
    "objectID": "resource/r.html#footnotes",
    "href": "resource/r.html#footnotes",
    "title": "R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to be really fancy, you can use blogdown, which makes a complete website with R Markdown files. That’s actually how this site is built (see the source code). You can build your own site with this tutorial.↩︎"
  },
  {
    "objectID": "resource/data.html",
    "href": "resource/data.html",
    "title": "Data",
    "section": "",
    "text": "There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n\nData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\nGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.\nKaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\nUS City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\nPolitical science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n\nFrançois Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.\nThomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).\nErik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)",
    "crumbs": [
      "Resources",
      "Overview",
      "Data"
    ]
  },
  {
    "objectID": "example/index.html",
    "href": "example/index.html",
    "title": "Code examples",
    "section": "",
    "text": "Visit this section after you have finished the readings and lecture videos. It contains fully annotated R code and other supplementary information and it will be indispensable as you work on your problem sets and project.\nMany sections also contain videos of me live coding the examples so you can see what it looks like to work with R in real time. You’ll notice me make all sorts of little errors, which is totally normal—everyone does!"
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Quizzes will be completed in class using Poll Everywhere.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#daily-quizzes",
    "href": "assignment/index.html#daily-quizzes",
    "title": "Assignments",
    "section": "",
    "text": "Quizzes will be completed in class using Poll Everywhere.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#problem-sets",
    "href": "assignment/index.html#problem-sets",
    "title": "Assignments",
    "section": "Problem sets",
    "text": "Problem sets\nThere will be a weekly problem set. These are due the Monday following each week. (For example, the problem set for Week 1 is due Monday of Week 2.) There is a one-week grace period for late assignments. All assignments are scored on completion, not on accuracy.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings, lectures, and videos",
    "section": "",
    "text": "Each class session has one or two videos that you should watch before coming to class. These videos are the lectures recorded by Richard McElreath.\nThe in-class lecture slides are special HTML files made with the R package xaringan (R can do so much!). On each class session page you’ll see buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n\n View all slides in new window  Download PDF of all slides\n\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes).\nI will note for each set of lecture slides which chapters in the 2nd edition of the McElreath book correspond with that day’s lesson. However, many of the functions we use – especially starting in week 4 – will not be from McElreath’s materials, but instead from the brms and tidybayes packages.",
    "crumbs": [
      "Content",
      "Overview",
      "Readings, lectures, and videos"
    ]
  },
  {
    "objectID": "resource/citations.html",
    "href": "resource/citations.html",
    "title": "Citations and bibliography",
    "section": "",
    "text": "You can download a BibTeX file of all the non-web-based readings in the course:\n\n references.bib\n\nYou can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online."
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Resources",
    "section": "",
    "text": "I have included a bunch of extra resources and guides related to causal inference, program evaluation, R, data, and other relevant topics. Enjoy!",
    "crumbs": [
      "Resources",
      "Overview",
      "Resources"
    ]
  },
  {
    "objectID": "assignment/02-problem-set.html",
    "href": "assignment/02-problem-set.html",
    "title": "Problem set 2",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 2"
    ]
  },
  {
    "objectID": "assignment/02-problem-set.html#instructions",
    "href": "assignment/02-problem-set.html#instructions",
    "title": "Problem set 2",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 2"
    ]
  },
  {
    "objectID": "assignment/02-problem-set.html#questions",
    "href": "assignment/02-problem-set.html#questions",
    "title": "Problem set 2",
    "section": "Questions",
    "text": "Questions\nTBD",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 2"
    ]
  },
  {
    "objectID": "assignment/04-problem-set.html",
    "href": "assignment/04-problem-set.html",
    "title": "Problem set 4",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 4"
    ]
  },
  {
    "objectID": "assignment/04-problem-set.html#instructions",
    "href": "assignment/04-problem-set.html#instructions",
    "title": "Problem set 4",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 4"
    ]
  },
  {
    "objectID": "assignment/04-problem-set.html#questions",
    "href": "assignment/04-problem-set.html#questions",
    "title": "Problem set 4",
    "section": "Questions",
    "text": "Questions\nTBD",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 4"
    ]
  },
  {
    "objectID": "assignment/06-problem-set.html",
    "href": "assignment/06-problem-set.html",
    "title": "Problem set 6",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 6"
    ]
  },
  {
    "objectID": "assignment/06-problem-set.html#instructions",
    "href": "assignment/06-problem-set.html#instructions",
    "title": "Problem set 6",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 6"
    ]
  },
  {
    "objectID": "assignment/06-problem-set.html#questions",
    "href": "assignment/06-problem-set.html#questions",
    "title": "Problem set 6",
    "section": "Questions",
    "text": "Questions\nTBD",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 6"
    ]
  },
  {
    "objectID": "assignment/08-problem-set.html",
    "href": "assignment/08-problem-set.html",
    "title": "Problem set 8",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 8"
    ]
  },
  {
    "objectID": "assignment/08-problem-set.html#instructions",
    "href": "assignment/08-problem-set.html#instructions",
    "title": "Problem set 8",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 8"
    ]
  },
  {
    "objectID": "assignment/08-problem-set.html#questions",
    "href": "assignment/08-problem-set.html#questions",
    "title": "Problem set 8",
    "section": "Questions",
    "text": "Questions\nTBD",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 8"
    ]
  },
  {
    "objectID": "assignment/10-problem-set.html",
    "href": "assignment/10-problem-set.html",
    "title": "Problem set 10",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 10"
    ]
  },
  {
    "objectID": "assignment/10-problem-set.html#instructions",
    "href": "assignment/10-problem-set.html#instructions",
    "title": "Problem set 10",
    "section": "",
    "text": "Please use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas.",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 10"
    ]
  },
  {
    "objectID": "assignment/10-problem-set.html#questions",
    "href": "assignment/10-problem-set.html#questions",
    "title": "Problem set 10",
    "section": "Questions",
    "text": "Questions\nTBD",
    "crumbs": [
      "Assignments",
      "Problem sets",
      "Problem set 10"
    ]
  },
  {
    "objectID": "news/index.html",
    "href": "news/index.html",
    "title": "Course News",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Bayesian Analysis\n        ",
    "section": "",
    "text": "Bayesian Analysis\n        \n        \n            Model building, estimation, interpretation, visualization\n        \n        \n            PSY 607 • Spring 2025 Psychology University of Oregon\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nInstructor\n\n   Dr. Sara Weston\n   Straub 325\n   sweston2@uoregon.edu\n   Schedule an appointment\n\n\n\nCourse details\n\n   Tuesdays and Thursdays\n   Spring 2025\n   2:00–3:20 PM\n   119 FEN\n   Slack\n\n\n\nContacting me\nEmail is the best way to get in contact with me. I will try to respond to all course-related emails within one business day during normal business hours (9am-5pm, Mon-Fri).\n\n\n```"
  },
  {
    "objectID": "resource/markdown.html",
    "href": "resource/markdown.html",
    "title": "Using Markdown",
    "section": "",
    "text": "Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)",
    "crumbs": [
      "Resources",
      "Guides",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#basic-markdown-formatting",
    "href": "resource/markdown.html#basic-markdown-formatting",
    "title": "Using Markdown",
    "section": "Basic Markdown formatting",
    "text": "Basic Markdown formatting\n\n\n\n\n\n\n\n\n\nType…\n\n\n…or…\n\n\n…to get\n\n\n\n\n\n\nSome text in a paragraph.\nMore text in the next paragraph. Always\nuse empty lines between paragraphs.\n\n\n\n\n\nSome text in a paragraph.\n\n\nMore text in the next paragraph. Always use empty lines between paragraphs.\n\n\n\n\n\nItalic\n\n\nItalic\n\n\nItalic\n\n\n\n\nBold\n\n\nBold\n\n\nBold\n\n\n\n\n# Heading 1\n\n\n\n\n\nHeading 1\n\n\n\n\n\n## Heading 2\n\n\n\n\n\nHeading 2\n\n\n\n\n\n### Heading 3\n\n\n\n\n\nHeading 3\n\n\n\n\n\n(Go up to heading level 6 with ######)\n\n\n\n\n\n\n\n\nLink text\n\n\n\n\nLink text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;code&gt;Inline code with backticks\n\n\n\n\nInline code with backticks\n\n\n\n\n&gt; Blockquote\n\n\n\n\n\n\nBlockquote\n\n\n\n\n\n\n- Things in\n- an unordered\n- list\n\n\n* Things in\n* an unordered\n* list\n\n\n\n\nThings in\n\n\nan unordered\n\n\nlist\n\n\n\n\n\n\n1. Things in\n2. an ordered\n3. list\n\n\n1) Things in\n2) an ordered\n3) list\n\n\n\n\nThings in\n\n\nan ordered\n\n\nlist\n\n\n\n\n\n\nHorizontal line\n\n---\n\n\nHorizontal line\n\n***\n\n\n\nHorizontal line",
    "crumbs": [
      "Resources",
      "Guides",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#math",
    "href": "resource/markdown.html#math",
    "title": "Using Markdown",
    "section": "Math",
    "text": "Math\n\nBasic math commands\nMarkdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here. In this class, these will be the most common things you’ll use:\n\n\n\n\n\nDescription\nCommand\nOutput\n\n\n\n\nLetters\n\n\nRoman letters\n&lt;code&gt;a b c d e f&lt;/code&gt;\n\\(a\\ b\\ c\\ d\\ e\\ f\\)\n\n\nGreek letters (see &lt;a href='https://www.overleaf.com/learn/latex/List_of_Greek_letters_and_math_symbols'&gt;this&lt;/a&gt; for all possible letters)\n&lt;code&gt;\\alpha \\beta \\Gamma \\gamma&lt;/code&gt; &lt;br&gt; &lt;code&gt;\\Delta \\delta \\epsilon&lt;/code&gt;\n\\(\\alpha\\ \\beta\\ \\Gamma\\ \\gamma\\ \\Delta\\ \\delta\\ \\epsilon\\)\n\n\nLetters will automatically be italicized and treated as math variables;&lt;br&gt;if you want actual text in the math, use &lt;code&gt;\\text{}&lt;/code&gt;\nEw: &lt;code&gt;Treatment = \\beta&lt;/code&gt; &lt;br&gt;Good: &lt;code&gt;\\text{Treatment} = \\beta&lt;/code&gt;\nEw: \\(Treatment = \\beta\\)&lt;br&gt;Good: \\(\\text{Treatment} = \\beta\\)\n\n\nExtra spaces will automatically be removed; if you want a space, use &lt;code&gt;\\ &lt;/code&gt;\nNo space: &lt;code&gt;x y&lt;/code&gt; &lt;br&gt; Space: &lt;code&gt;x\\ y&lt;/code&gt;\nNo space: \\(x y\\) &lt;br&gt;Space: \\(x \\ y\\)\n\n\nSuperscripts and subscripts\n\n\nUse &lt;code&gt;^&lt;/code&gt; to make one character superscripted.\n&lt;code&gt;x^2&lt;/code&gt;\n\\(x^2\\)\n\n\nWrap the superscripted part in &lt;code&gt;{}&lt;/code&gt; if there's more than one character\n&lt;code&gt;x^{2+y}&lt;/code&gt;\n\\(x^{2+y}\\)\n\n\nUse &lt;code&gt;_&lt;/code&gt; to make one character subscripted\n&lt;code&gt;\\beta_1&lt;/code&gt;\n\\(\\beta_1\\)\n\n\nWrap the subscripted part in &lt;code&gt;{}&lt;/code&gt; if there's more than one character\n&lt;code&gt;\\beta_{i, t}&lt;/code&gt;\n\\(\\beta_{i, t}\\)\n\n\nUse superscripts and subscripts simultaneously\n&lt;code&gt;\\beta_1^{\\text{Treatment}}&lt;/code&gt;\n\\(\\beta_1^{\\text{Treatment}}\\)\n\n\nYou can even nest them\n&lt;code&gt;x^{2^{2^2}}&lt;/code&gt;\n\\(x^{2^{2^2}}\\)\n\n\nMath operations\n\n\nAddition\n&lt;code&gt;2 + 5 = 7&lt;/code&gt;\n\\(2 + 5 = 7\\)\n\n\nSubtraction\n&lt;code&gt;2 - 5 = -3&lt;/code&gt;\n\\(2 + 5 = -3\\)\n\n\nMultiplication\n&lt;code&gt;x \\times y&lt;/code&gt; &lt;br&gt; &lt;code&gt;x \\cdot y&lt;/code&gt;\n\\(x \\times y\\) &lt;br&gt; \\(x \\cdot y\\)\n\n\nDivision\n&lt;code&gt;8 \\div 2&lt;/code&gt;\n\\(8 \\div 2\\)\n\n\nFractions\n&lt;code&gt;\\frac{8}{2}&lt;/code&gt;\n\\(\\frac{8}{2}\\)\n\n\nSquare roots; use &lt;code&gt;[3]&lt;/code&gt; for other roots\n&lt;code&gt;\\sqrt{81} = 9&lt;/code&gt; &lt;br&gt; &lt;code&gt;\\sqrt[3]{27} = 3&lt;/code&gt;\n\\(\\sqrt{81} = 9\\) &lt;br&gt; \\(\\sqrt[3]{27} = 3\\)\n\n\nSummation; use sub/superscripts for extra details\n&lt;code&gt;\\sum x&lt;/code&gt; &lt;br&gt; &lt;code&gt;\\sum_{n=1}^{\\infty} \\frac{1}{n}&lt;/code&gt;\n\\(\\sum x\\) &lt;br&gt; \\(\\sum_{n=1}^{\\infty} \\frac{1}{n}\\)\n\n\nProducts; use sub/superscripts for extra details\n&lt;code&gt;\\prod x&lt;/code&gt; &lt;br&gt; &lt;code&gt;\\prod_{n=1}^{5} n^2&lt;/code&gt;\n\\(\\prod x\\) &lt;br&gt; \\(\\prod_{n=1}^{5} n^2\\)\n\n\nIntegrals; use sub/superscripts for extra details\n&lt;code&gt;\\int x^2 \\ dx&lt;/code&gt; &lt;br&gt; &lt;code&gt;\\int_{1}^{100} x^2 \\ dx&lt;/code&gt;\n\\(\\int x^2 \\ dx\\) &lt;br&gt; \\(\\int_{1}^{100} x^2 \\ dx\\)\n\n\nExtra symbols\n\n\nAdd a bar for things like averages\n&lt;code&gt;\\bar{x}&lt;/code&gt;\n\\(\\bar{x}\\)\n\n\nUse an overline for longer things\nEw: &lt;code&gt;\\bar{abcdef}&lt;/code&gt; &lt;br&gt; Good: &lt;code&gt;\\overline{abcdef}&lt;/code&gt;\nEw: \\(\\bar{abcdef}\\) &lt;br&gt; Good: \\(\\overline{abcdef}\\)\n\n\nAdd a hat for things like estimates\n&lt;code&gt;\\hat{y}&lt;/code&gt;\n\\(\\hat{y}\\)\n\n\nUse a wide hat for longer things\nEw: &lt;code&gt;\\hat{abcdef}&lt;/code&gt; &lt;br&gt; Good: &lt;code&gt;\\widehat{abcdef}&lt;/code&gt;\nEw: \\(\\hat{abcdef}\\) &lt;br&gt; Good: \\(\\widehat{abcdef}\\)\n\n\nUse arrows for DAG-like things\n&lt;code&gt;Z \\rightarrow Y \\leftarrow X&lt;/code&gt;\n\\(Z \\rightarrow Y \\leftarrow X\\)\n\n\nBonus fun\n\n\nUse colors!; see &lt;a href='https://www.overleaf.com/learn/latex/Using_colours_in_LaTeX'&gt;here&lt;/a&gt; for more details and &lt;a href='https://www.overleaf.com/learn/latex/Using_colours_in_LaTeX#Reference_guide'&gt;here&lt;/a&gt; for a list of color names\n&lt;code&gt;\\color{red}{y} = \\color{blue}{\\beta_1 x_1}&lt;/code&gt;\n\\(\\color{red}{y}\\ \\color{black}{=}\\ \\color{blue}{\\beta_1 x_1}\\)\n\n\n\n\n\n\n\n\n\nUsing math inline\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n\n\n\n\n\n\n\n\nType…\n\n\n…to get\n\n\n\n\n\n\nBased on the DAG, the regression model for\nestimating the effect of education on wages\nis $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, \nor $\\text{Wages} = \\beta_0 + \n\\beta_1 \\text{Education} + \\epsilon$.\n\n\nBased on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).\n\n\n\n\n\n\nUsing math in a block\nTo put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was a way to solve for $x$ in high school math:\n\n$$\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n$$\n…to get…\n\nThe quadratic equation was a way to solve for \\(x\\) in high school math:\n\\[\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\\]\n\n\n\n\nDollar signs and math\nBecause dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs \\($5.75 and this other costs $40\\)”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.",
    "crumbs": [
      "Resources",
      "Guides",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#tables",
    "href": "resource/markdown.html#tables",
    "title": "Using Markdown",
    "section": "Tables",
    "text": "Tables\nThere are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like kableExtra (use kable()) or pander (use pandoc.table()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n  Right     Left     Center     Default\n-------     ------ ----------   -------\n     12     12        12            12\n    123     123       123          123\n      1     1          1             1\n\nTable: Caption goes here\n…to get…\n\nCaption goes here\n\n\nRight\nLeft\nCenter\nDefault\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\nFor pipe tables, type…\n| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\nTable: Caption goes here\n…to get…\n\nCaption goes here\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1",
    "crumbs": [
      "Resources",
      "Guides",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#footnotes",
    "href": "resource/markdown.html#footnotes",
    "title": "Using Markdown",
    "section": "Footnotes",
    "text": "Footnotes\nThere are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags].\n\n[^1]: This is a note.\n\n[^note-on-dags]: DAGs are neat. \n\nAnd here's more of the document.\n…to get…\n\nHere is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n\n\n\n\n\nThis is a note.↩︎\n\n\n\n\nDAGs are neat.↩︎\n\n\n\n\n\n\nYou can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!]\n…to get…\n\nCausal inference is neat.1\n\n\n\n\n\nBut it can be hard too!↩︎",
    "crumbs": [
      "Resources",
      "Guides",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#front-matter",
    "href": "resource/markdown.html#front-matter",
    "title": "Using Markdown",
    "section": "Front matter",
    "text": "Front matter\nYou can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n---\ntitle: Title of your document\ndate: \"January 13, 2020\"\nauthor: \"Your name\"\n---\nYou can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n---\ntitle: \"My cool title: a subtitle\"\n---\nIf you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n---\ntitle: 'An evaluation of \"scare quotes\"'\n---",
    "crumbs": [
      "Resources",
      "Guides",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#citations",
    "href": "resource/markdown.html#citations",
    "title": "Using Markdown",
    "section": "Citations",
    "text": "Citations\nOne of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\n\nAdd a bibliography: entry to the YAML metadata:\n---\ntitle: Title of your document\ndate: \"January 13, 2020\"\nauthor: \"Your name\"\nbibliography: name_of_file.bib\n---\nChoose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n---\ntitle: Title of your document\ndate: \"January 13, 2020\"\nauthor: \"Your name\"\nbibliography: name_of_file.bib\ncsl: \"https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\"\n---\nSome of the most common CSLs are:\n\nChicago author-date\nChicago note-bibliography\nChicago full note-bibliography (no shortened notes or ibids)\nAPA 7th edition\nMLA 8th edition\n\nCite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n\n\n\n\n\n\n\n\nType…\n…to get…\n\n\n\n\nCausal inference is neat [@Rohrer:2018;\n@AngristPischke:2015].\nCausal inference is neat (Rohrer:2018?; AngristPischke:2015?).\n\n\nCausal inference is neat [see @Rohrer:2018,\np. 34; also @AngristPischke:2015, chapter 1]\nCausal inference is neat (see Rohrer:2018?; also AngristPischke:2015?)\n\n\nAngrist and Pischke say causal inference\nis neat [-@AngristPischke:2015; see also\n@Rohrer:2018].\nAngrist and Pischke say causal inference is neat (AngristPischke:2015?; see also Rohrer:2018?).\n\n\n@AngristPischke:2015 [chapter 1] say causal\ninference is neat, and @Rohrer:2018 agrees.\n(AngristPischke:2015?) say causal inference is neat, and (Rohrer:2018?) agrees.\n\n\n\nAfter compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.",
    "crumbs": [
      "Resources",
      "Guides",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#other-references",
    "href": "resource/markdown.html#other-references",
    "title": "Using Markdown",
    "section": "Other references",
    "text": "Other references\nThese websites have additional details and examples and practice tools:\n\nCommonMark’s Markdown tutorial: A quick interactive Markdown tutorial.\nMarkdown tutorial: Another interactive tutorial to practice using Markdown.\nMarkdown cheatsheet: Useful one-page reminder of Markdown syntax.\nThe Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.",
    "crumbs": [
      "Resources",
      "Guides",
      "Using Markdown"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Below is the schedule for the course. Note that this schedule may change as the course progresses.\n\n\n\n\n\n\nSchedule Notes\n\n\n\n\nAll deadlines are at 11:59 PM Pacific Time on the specified date\nSchedule is subject to change based on course needs\nAdditional readings and resources will be posted on the course website\n\n\n\n\nHere’s your roadmap for the semester!\n\nContent (): This page contains the recorded lectures and slides for the week. Watch the lectures before our in-person class.\nAssignment (): This page contains the instructions for each assignment.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nAssignment\n\n\n\n\n\n\nWeek 1\n\n\n\n\nApril 1\n\n\nIntroduction and Bayes’ theorem\n\n\n\n\n\n\n\n\n\n\nApril 3\n\n\nBayes as counting\n\n\n\n\n\n\n\n\n\n\nApril 7\n\n\nProblem set 1  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\nApril 8\n\n\nLinear models\n\n\n\n\n\n\n\n\n\n\nApril 10\n\n\nCategories\n\n\n\n\n\n\n\n\n\n\nApril 14\n\n\nProblem set 2  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\nApril 15\n\n\nDAG models\n\n\n\n\n\n\n\n\n\n\nApril 17\n\n\nMore DAG moodels\n\n\n\n\n\n\n\n\n\n\nApril 21\n\n\nProblem set 3  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\nApril 22\n\n\nGood prediction\n\n\n\n\n\n\n\n\n\n\nApril 24\n\n\nMCMC\n\n\n\n\n\n\n\n\n\n\nApril 28\n\n\nProblem set 4  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nAssignment\n\n\n\n\n\n\nWeek 5\n\n\n\n\nApril 29\n\n\nBinomial regression\n\n\n\n\n\n\n\n\n\n\nMay 1\n\n\nPoisson regression\n\n\n\n\n\n\n\n\n\n\nMay 5\n\n\nProblem set 5  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\nMay 6\n\n\nMultinomial regression\n\n\n\n\n\n\n\n\n\n\nMay 8\n\n\nMultilevel models\n\n\n\n\n\n\n\n\n\n\nMay 12\n\n\nProblem set 6  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\nMay 13\n\n\nMore multilevel models\n\n\n\n\n\n\n\n\n\n\nMay 15\n\n\nMultivariate priors\n\n\n\n\n\n\n\n\n\n\nMay 19\n\n\nProblem set 7  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nAssignment\n\n\n\n\n\n\nWeek 8\n\n\n\n\nMay 20\n\n\nSocial networks\n\n\n\n\n\n\n\n\n\n\nMay 22\n\n\nGaussian processes\n\n\n\n\n\n\n\n\n\n\nMay 26\n\n\nProblem set 8  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\nMay 27\n\n\nMeasurement\n\n\n\n\n\n\n\n\n\n\nMay 29\n\n\nMissing data\n\n\n\n\n\n\n\n\n\n\nJune 2\n\n\nProblem set 9  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\nJune 3\n\n\nGeneralized linear models\n\n\n\n\n\n\n\n\n\n\nJune 5\n\n\nTBD\n\n\n\n\n\n\n\n\n\n\nJune 9\n\n\nProblem set 10  (submit by 23:59:00)"
  },
  {
    "objectID": "schedule.html#course-schedule",
    "href": "schedule.html#course-schedule",
    "title": "Schedule",
    "section": "",
    "text": "Below is the schedule for the course. Note that this schedule may change as the course progresses.\n\n\n\n\n\n\nSchedule Notes\n\n\n\n\nAll deadlines are at 11:59 PM Pacific Time on the specified date\nSchedule is subject to change based on course needs\nAdditional readings and resources will be posted on the course website\n\n\n\n\nHere’s your roadmap for the semester!\n\nContent (): This page contains the recorded lectures and slides for the week. Watch the lectures before our in-person class.\nAssignment (): This page contains the instructions for each assignment.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nAssignment\n\n\n\n\n\n\nWeek 1\n\n\n\n\nApril 1\n\n\nIntroduction and Bayes’ theorem\n\n\n\n\n\n\n\n\n\n\nApril 3\n\n\nBayes as counting\n\n\n\n\n\n\n\n\n\n\nApril 7\n\n\nProblem set 1  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\nApril 8\n\n\nLinear models\n\n\n\n\n\n\n\n\n\n\nApril 10\n\n\nCategories\n\n\n\n\n\n\n\n\n\n\nApril 14\n\n\nProblem set 2  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\nApril 15\n\n\nDAG models\n\n\n\n\n\n\n\n\n\n\nApril 17\n\n\nMore DAG moodels\n\n\n\n\n\n\n\n\n\n\nApril 21\n\n\nProblem set 3  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\nApril 22\n\n\nGood prediction\n\n\n\n\n\n\n\n\n\n\nApril 24\n\n\nMCMC\n\n\n\n\n\n\n\n\n\n\nApril 28\n\n\nProblem set 4  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nAssignment\n\n\n\n\n\n\nWeek 5\n\n\n\n\nApril 29\n\n\nBinomial regression\n\n\n\n\n\n\n\n\n\n\nMay 1\n\n\nPoisson regression\n\n\n\n\n\n\n\n\n\n\nMay 5\n\n\nProblem set 5  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\nMay 6\n\n\nMultinomial regression\n\n\n\n\n\n\n\n\n\n\nMay 8\n\n\nMultilevel models\n\n\n\n\n\n\n\n\n\n\nMay 12\n\n\nProblem set 6  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\nMay 13\n\n\nMore multilevel models\n\n\n\n\n\n\n\n\n\n\nMay 15\n\n\nMultivariate priors\n\n\n\n\n\n\n\n\n\n\nMay 19\n\n\nProblem set 7  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nAssignment\n\n\n\n\n\n\nWeek 8\n\n\n\n\nMay 20\n\n\nSocial networks\n\n\n\n\n\n\n\n\n\n\nMay 22\n\n\nGaussian processes\n\n\n\n\n\n\n\n\n\n\nMay 26\n\n\nProblem set 8  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\nMay 27\n\n\nMeasurement\n\n\n\n\n\n\n\n\n\n\nMay 29\n\n\nMissing data\n\n\n\n\n\n\n\n\n\n\nJune 2\n\n\nProblem set 9  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\nJune 3\n\n\nGeneralized linear models\n\n\n\n\n\n\n\n\n\n\nJune 5\n\n\nTBD\n\n\n\n\n\n\n\n\n\n\nJune 9\n\n\nProblem set 10  (submit by 23:59:00)"
  },
  {
    "objectID": "resource/install.html",
    "href": "resource/install.html",
    "title": "Installing R, RStudio, tidyverse, and tinytex",
    "section": "",
    "text": "You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.",
    "crumbs": [
      "Resources",
      "Guides",
      "Installing R, RStudio, tidyverse, and tinytex"
    ]
  },
  {
    "objectID": "resource/install.html#rstudio.cloud",
    "href": "resource/install.html#rstudio.cloud",
    "title": "Installing R, RStudio, tidyverse, and tinytex",
    "section": "RStudio.cloud",
    "text": "RStudio.cloud\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service initially, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for labs and problem sets.\nGo to https://rstudio.cloud/ and create an account. You’ll receive a link to join the shared class workspace separately. If you don’t get this link, let me know and I will invite you.",
    "crumbs": [
      "Resources",
      "Guides",
      "Installing R, RStudio, tidyverse, and tinytex"
    ]
  },
  {
    "objectID": "resource/install.html#rstudio-on-your-computer",
    "href": "resource/install.html#rstudio-on-your-computer",
    "title": "Installing R, RStudio, tidyverse, and tinytex",
    "section": "RStudio on your computer",
    "text": "RStudio on your computer\nRStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\n\nInstall R\nFirst you need to install R itself (the engine).\n\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\nClick on “Download R for XXX”, where XXX is either Mac or Windows:\n\n\n\n\n\n\n\n\n\n\nIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is 4.2.1) and download it.\n\n\n\n\n\n\n\n\n\nIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n\n\n\n\n\n\n\n\n\n\nDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\nIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\n\n\nInstall RStudio\nNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\n\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\nThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\n\n\n\n\n\n\n\n\n\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n\n\n\n\n\n\n\n\n\nDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\nDouble click on RStudio to run it (check your applications folder or start menu).\n\n\nInstall tidyverse\nR packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\n\n\n\n\n\n\n\n\n\n\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\n\n\n\n\n\n\n\n\n\n\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n\n\nInstall tinytex\nWhen you knit to PDF, R uses a special scientific typesetting program named LaTeX (pronounced “lay-tek” or “lah-tex”; for goofy nerdy reasons, the x is technically the “ch” sound in “Bach”, but most people just say it as “k”—saying “layteks” is frowned on for whatever reason).\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB! To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install tinytex so you can knit to pretty PDFs:\n\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console.\nRun tinytex::install_tinytex() in the console.\nWait for a bit while R downloads and installs everything you need.\nThe end! You should now be able to knit to PDF.",
    "crumbs": [
      "Resources",
      "Guides",
      "Installing R, RStudio, tidyverse, and tinytex"
    ]
  },
  {
    "objectID": "resource/install.html#footnotes",
    "href": "resource/install.html#footnotes",
    "title": "Installing R, RStudio, tidyverse, and tinytex",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎",
    "crumbs": [
      "Resources",
      "Guides",
      "Installing R, RStudio, tidyverse, and tinytex"
    ]
  },
  {
    "objectID": "resource/style.html",
    "href": "resource/style.html",
    "title": "R style suggestions",
    "section": "",
    "text": "R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\n\nmpg %&gt;% \n  filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% \n  filter(cty &gt; 10, \n         class == \"compact\")\n\nmpg %&gt;% filter(cty&gt;10, class==\"compact\")\n\nfilter(mpg,cty&gt;10,class==\"compact\")\n\nmpg %&gt;% \nfilter(cty &gt; 10, \n                        class == \"compact\")\n\nfilter ( mpg,cty&gt;10,     class==\"compact\" )\n\nBut you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.",
    "crumbs": [
      "Resources",
      "Overview",
      "R style suggestions"
    ]
  },
  {
    "objectID": "resource/style.html#r-style-conventions",
    "href": "resource/style.html#r-style-conventions",
    "title": "R style suggestions",
    "section": "",
    "text": "R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\n\nmpg %&gt;% \n  filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% \n  filter(cty &gt; 10, \n         class == \"compact\")\n\nmpg %&gt;% filter(cty&gt;10, class==\"compact\")\n\nfilter(mpg,cty&gt;10,class==\"compact\")\n\nmpg %&gt;% \nfilter(cty &gt; 10, \n                        class == \"compact\")\n\nfilter ( mpg,cty&gt;10,     class==\"compact\" )\n\nBut you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.",
    "crumbs": [
      "Resources",
      "Overview",
      "R style suggestions"
    ]
  },
  {
    "objectID": "resource/style.html#main-style-things-to-pay-attention-to-for-this-class",
    "href": "resource/style.html#main-style-things-to-pay-attention-to-for-this-class",
    "title": "R style suggestions",
    "section": "Main style things to pay attention to for this class",
    "text": "Main style things to pay attention to for this class\n\nImportant note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty&gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n\n\nSpacing\n\nSee the “Spacing” section in the tidyverse style guide.\n\nPut spaces after commas (like in regular English):\n\n# Good\nfilter(mpg, cty &gt; 10)\n\n# Bad\nfilter(mpg , cty &gt; 10)\nfilter(mpg ,cty &gt; 10)\nfilter(mpg,cty &gt; 10)\n\nPut spaces around operators like +, -, &gt;, =, etc.:\n\n# Good\nfilter(mpg, cty &gt; 10)\n\n# Bad\nfilter(mpg, cty&gt;10)\nfilter(mpg, cty&gt; 10)\nfilter(mpg, cty &gt;10)\n\nDon’t put spaces around parentheses that are parts of functions:\n\n# Good\nfilter(mpg, cty &gt; 10)\n\n# Bad\nfilter (mpg, cty &gt; 10)\nfilter ( mpg, cty &gt; 10)\nfilter( mpg, cty &gt; 10 )\n\n\n\nLong lines\n\nSee the “Long lines” section in the tidyverse style guide.\n\nIt’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” &gt; “Global Options” &gt; “Code” &gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n\n# Good\nfilter(mpg, cty &gt; 10, class == \"compact\")\n\n# Good\nfilter(mpg, cty &gt; 10, \n       class == \"compact\")\n\n# Good\nfilter(mpg,\n       cty &gt; 10,\n       class == \"compact\")\n\n# Bad\nfilter(mpg, cty &gt; 10, class %in% c(\"compact\", \"pickup\", \"midsize\", \"subcompact\", \"suv\", \"2seater\", \"minivan\"))\n\n# Good\nfilter(mpg, \n       cty &gt; 10, \n       class %in% c(\"compact\", \"pickup\", \"midsize\", \"subcompact\", \n                    \"suv\", \"2seater\", \"minivan\"))\n\n\n\nPipes (%&gt;%) and ggplot layers (+)\nPut each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n\n# Good\nggplot(mpg, aes(x = cty, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw()\n\n# Bad\nggplot(mpg, aes(x = cty, y = hwy, color = class)) +\n  geom_point() + geom_smooth() +\n  theme_bw()\n\n# Super bad\nggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw()\n\n# Super bad and won't even work\nggplot(mpg, aes(x = cty, y = hwy, color = class))\n  + geom_point()\n  + geom_smooth() \n  + theme_bw()\n\nPut each step in a dplyr pipeline on separate lines, with the %&gt;% at the end of the line, indented with two spaces:\n\n# Good\nmpg %&gt;% \n  filter(cty &gt; 10) %&gt;% \n  group_by(class) %&gt;% \n  summarize(avg_hwy = mean(hwy))\n\n# Bad\nmpg %&gt;% filter(cty &gt; 10) %&gt;% group_by(class) %&gt;% \n  summarize(avg_hwy = mean(hwy))\n\n# Super bad\nmpg %&gt;% filter(cty &gt; 10) %&gt;% group_by(class) %&gt;% summarize(avg_hwy = mean(hwy))\n\n# Super bad and won't even work\nmpg %&gt;% \n  filter(cty &gt; 10)\n  %&gt;% group_by(class)\n  %&gt;% summarize(avg_hwy = mean(hwy))\n\n\n\nComments\n\nSee the “Comments” section in the tidyverse style guide.\n\nComments should start with a comment symbol and a single space: #\n\n# Good\n\n#Bad\n\n    #Bad\n\nIf the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\n\nmpg %&gt;% \n  filter(cty &gt; 10) %&gt;%  # Only rows where cty is 10 +\n  group_by(class) %&gt;%  # Divide into class groups\n  summarize(avg_hwy = mean(hwy))  # Find the average hwy in each group\n\nYou can add extra spaces to get inline comments to align, if you want:\n\nmpg %&gt;% \n  filter(cty &gt; 10) %&gt;%            # Only rows where cty is 10 +\n  group_by(class) %&gt;%             # Divide into class groups\n  summarize(avg_hwy = mean(hwy))  # Find the average hwy in each group\n\nIf the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” &gt; “Reflow comment”\n\n# Good\n# Happy families are all alike; every unhappy family is unhappy in its own way.\n# Everything was in confusion in the Oblonskys’ house. The wife had discovered\n# that the husband was carrying on an intrigue with a French girl, who had been\n# a governess in their family, and she had announced to her husband that she\n# could not go on living in the same house with him. This position of affairs\n# had now lasted three days, and not only the husband and wife themselves, but\n# all the members of their family and household, were painfully conscious of it.\n\n# Bad\n# Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.\n\nThough, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.",
    "crumbs": [
      "Resources",
      "Overview",
      "R style suggestions"
    ]
  },
  {
    "objectID": "example/dags.html",
    "href": "example/dags.html",
    "title": "DAGs",
    "section": "",
    "text": "The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:"
  },
  {
    "objectID": "example/dags.html#dags-with-dagitty.net",
    "href": "example/dags.html#dags-with-dagitty.net",
    "title": "DAGs",
    "section": "",
    "text": "The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:"
  },
  {
    "objectID": "example/dags.html#dags-with-r-ggdag-and-dagitty",
    "href": "example/dags.html#dags-with-r-ggdag-and-dagitty",
    "title": "DAGs",
    "section": "DAGs with R, ggdag, and dagitty",
    "text": "DAGs with R, ggdag, and dagitty\nYou can use the ggdag and dagitty packages in R to build and work with DAGs too. I typically draw an initial DAG in my browser with dagitty.net and then I rewrite it in code in R so that it’s more official and formal and reproducible.\n\nLive coding example\n\n\n\n\n\n\nBasic DAGs\n(This is a heavily cleaned up and annotated version of the code from the video.)\n\n\nCode\n# Load packages\nlibrary(tidyverse)  # For dplyr, ggplot, and friends\nlibrary(ggdag)      # For plotting DAGs\nlibrary(dagitty)    # For working with DAG logic\n\n\nThe general process for making and working with DAGs in R is to create a DAG object with dagify() and then plot it with ggdag(). The documentation for ggdag is really good and helpful and full of examples. Check these pages for all sorts of additional details:\n\n“An introduction to ggdag”\n“An introduction to directed acyclic graphs”\nList of all ggdag-related functions\n\nThe syntax for dagify() is similar to the formula syntax you’ve been using for building regression models with lm(). The formulas you use in dagify() indicate the relationships between nodes.\nFor instance, in this DAG, y is caused by x, a, and b (y ~ x + a + b), and x is caused by a and b (x ~ a + b). This means that a and b are confounders. Use the exposure and outcome arguments to specify which nodes are the exposure (or treatment/intervention/program) and the outcome.\n\n\nCode\n# Create super basic DAG\nsimple_dag &lt;- dagify(\n  y ~ x + a + b,\n  x ~ a + b,\n  exposure = \"x\",\n  outcome = \"y\"\n)\n\n# Adding a theme_dag() layer to the plot makes it have a white background with no axis labels\nggdag(simple_dag) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nIf you want to plot which nodes are the exposure and outcome, use ggdag_status() instead:\n\n\nCode\nggdag_status(simple_dag) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\n\nLayouts and manual coordinates\nNotice how the layout is different in both of those graphs. By default, ggdag() positions the nodes randomly every time using a network algorithm. You can change the algorithm by using the layout argument: ggdag(simple_dag, layout = \"nicely\"). You can see a full list of possible algorithms by running ?layout_tbl_graph_igraph in the console.\nAlternatively, you can specify your own coordinates so that the nodes are positioned in the same place every time. Do this with the coords argument in dagify().\nThe best way to figure out what these numbers should be is to draw the DAG on paper or on a whiteboard and add a grid to it and then figure out the coordinates. For instance, in this DAG there are three rows and three columns: x and y go in the middle row (row 2) while a and b go in the middle column (column 2). It can also be helpful to not include theme_dag() at first so you can see the numbers for the underlying grid. Once you have everything positioned correctly, add theme_dag() to clean it up.\n\n\nCode\nsimple_dag_with_coords &lt;- dagify(\n  y ~ x + a + b,\n  x ~ a + b,\n  exposure = \"x\",\n  outcome = \"y\",\n  coords = list(x = c(x = 1, a = 2, b = 2, y = 3),\n                y = c(x = 2, a = 1, b = 3, y = 2))\n)\n\nggdag_status(simple_dag_with_coords) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\n\nNode names and labels\nThe variable names you use do not have to be limited to just x, y, and other lowercase letters. You can any names you want, as long as there are no spaces.\n\n\nCode\ndag_with_var_names &lt;- dagify(\n  outcome ~ treatment + confounder1 + confounder2,\n  treatment ~ confounder1 + confounder2,\n  exposure = \"treatment\",\n  outcome = \"outcome\"\n)\n\nggdag_status(dag_with_var_names) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nHowever, unless you use very short names, it is likely that the text will not fit inside the nodes. To get around this, you can add labels to the nodes using the labels argument in dagify(). Plot the labels by setting use_labels = \"label\" in ggdag(). You can turn off the text in the nodes with text = FALSE in ggdag().\n\n\nCode\ndag_with_coords_and_labels &lt;- dagify(\n  y ~ x + a + b,\n  x ~ a + b,\n  exposure = \"x\",\n  outcome = \"y\",\n  labels = c(y = \"Outcome\", x = \"Treatment\",\n             a = \"Confounder 1\", b = \"Confounder 2\"),\n  coords = list(x = c(x = 1, a = 2, b = 2, y = 3),\n                y = c(x = 2, a = 1, b = 3, y = 2))\n)\n\nggdag_status(dag_with_coords_and_labels,\n             use_labels = \"label\", text = FALSE) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\n\nPaths and adjustment sets\nR can also perform analysis on DAG objects. For example, we can find all the testable implications from the DAG using the impliedConditionalIndependencies() function from the dagitty package. For this simple DAG, there is only one: a should be independent of b. If we had a dataset with columns for each of these variables, we could check if this is true by running cor(a, b) to see if the two are related.\n\n\nCode\nimpliedConditionalIndependencies(simple_dag)\n## a _||_ b\n\n\nWe can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\n\n\nCode\npaths(simple_dag)\n## $paths\n## [1] \"x -&gt; y\"      \"x &lt;- a -&gt; y\" \"x &lt;- b -&gt; y\"\n## \n## $open\n## [1] TRUE TRUE TRUE\n\n\nThe first open path is fine—we want a single d-connected relationship between treatment and outcome—but the other two indicate that there is confounding from a and b. We can see what each of these paths are with the ggdag_paths() function from the ggdag package:\n\n\nCode\nggdag_paths(simple_dag_with_coords) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nInstead of listing out all the possible paths and identifying backdoors by hand, you can use the adjustmentSets() function in the dagitty package to programmatically find all the nodes that need to be adjusted. Here we see that both a and b need to be controlled for to isolate the x -&gt; y relationship.\n\n\nCode\nadjustmentSets(simple_dag)\n## { a, b }\n\n\nYou can also visualize the adjustment sets with ggdag_adjustment_set() in the ggdag package. Make sure you set shadow = TRUE to draw the arrows coming out of the adjusted nodes—by default, those are not included.\n\n\nCode\nggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nR will find minimally sufficient adjustment sets, which includes the fewest number of adjustments needed to close all backdoors between x and y. In this example DAG there was only one set of variables (a and b), but in other situations there could be many possible sets, or none if the causal effect is not identifiable.\n\n\nPlot DAG from dagitty.net with ggdag()\nIf you use dagitty.net to draw a DAG, you’ll notice that it generates some code for you in the model code section:\n\n\n\n\n\n\n\n\n\nYou can copy that dag{ ... } code and paste it into R to define a DAG object rather than use the dagify() function. To do this, use the dagitty() function from the dagitty library and include the whole generated model code in single quotes (''):\n\n\nCode\nmodel_from_dagitty &lt;- dagitty('dag {\nbb=\"0,0,1,1\"\n\"A confounder\" [pos=\"0.809,0.306\"]\n\"Another confounder\" [pos=\"0.810,0.529\"]\n\"Some outcome\" [outcome,pos=\"0.918,0.432\"]\n\"Some treatment\" [exposure,pos=\"0.681,0.426\"]\n\"A confounder\" -&gt; \"Some outcome\"\n\"A confounder\" -&gt; \"Some treatment\"\n\"Another confounder\" -&gt; \"Some outcome\"\n\"Another confounder\" -&gt; \"Some treatment\"\n\"Some treatment\" -&gt; \"Some outcome\"\n}\n')\n\nggdag(model_from_dagitty) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nBy default it’s going to look ugly because (1) the node labels don’t fit, and (2) slight differences in the coordinates make it so the nodes don’t perfectly align with each other. To fix coordinate alignment, you can modify the numbers in the generated DAG code. Here I rounded the numbers so that they’re all 0.3, 0.8, etc.\nTo fix the label issue, you can add the use_labels argument like normally. Only here, you can’t specify use_labels = \"label\". Instead, when you specify a DAG using dagitty’s code like this, the column in the underlying dataset that contains the labels is named name, so you need to use use_labels = \"name\".\nOther ggdag() variations like ggdag_status() will still work fine.\n\n\nCode\nmodel_from_dagitty_rounded &lt;- dagitty('dag {\nbb=\"0,0,1,1\"\n\"A confounder\" [pos=\"0.8,0.3\"]\n\"Another confounder\" [pos=\"0.8,0.5\"]\n\"Some outcome\" [outcome,pos=\"0.9,0.4\"]\n\"Some treatment\" [exposure,pos=\"0.7,0.4\"]\n\"A confounder\" -&gt; \"Some outcome\"\n\"A confounder\" -&gt; \"Some treatment\"\n\"Another confounder\" -&gt; \"Some outcome\"\n\"Another confounder\" -&gt; \"Some treatment\"\n\"Some treatment\" -&gt; \"Some outcome\"\n}\n')\n\nggdag_status(model_from_dagitty_rounded, text = FALSE, use_labels = \"name\") +\n  guides(color = \"none\") +  # Turn off legend\n  theme_dag()"
  },
  {
    "objectID": "example/dags.html#mosquito-net-example",
    "href": "example/dags.html#mosquito-net-example",
    "title": "DAGs",
    "section": "Mosquito net example",
    "text": "Mosquito net example\n\nConditional independencies\nYou can test if your stated relationships are correct by looking at the conditional independencies that are implied by the DAG. In dagitty.net, these appear in the sidebar in the “Testable implications” section. To show how this works, we’ll use a simulated dataset that I generated about a fictional mosquito net program. Download the data here if you want to follow along:\n\n mosquito_nets.csv\n\nResearchers are interested in whether using mosquito nets decreases an individual’s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. Additionally, this country has a special government program that provides free mosquito nets to households that meet specific requirements: to qualify for the program, there must be more than 4 members of the household, and the household’s monthly income must be lower than $700 a month. Households are not automatically enrolled in the program, and many do not use it. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\n\n\nCode\nmosquito_dag &lt;- dagify(\n  malaria_risk ~ net + income + health + temperature + resistance,\n  net ~ income + health + temperature + eligible + household,\n  eligible ~ income + household,\n  health ~ income,\n  exposure = \"net\",\n  outcome = \"malaria_risk\",\n  coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5,\n                      temperature = 6, resistance = 8.5, eligible = 2, household = 1),\n                y = c(malaria_risk = 2, net = 2, income = 3, health = 1,\n                      temperature = 3, resistance = 2, eligible = 3, household = 2)),\n  labels = c(malaria_risk = \"Risk of malaria\", net = \"Mosquito net\", income = \"Income\",\n             health = \"Health\", temperature = \"Nighttime temperatures\",\n             resistance = \"Insecticide resistance\",\n             eligible = \"Eligible for program\", household = \"Number in household\")\n)\n\nggdag_status(mosquito_dag, use_labels = \"label\", text = FALSE) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  theme_dag()\n\n\n\n\n\n\n\n\n\nThe causal graph above outlines the complete relationship between mosquito net use and risk of malaria. Each node in the DAG is a column in the dataset collected by the researchers, and includes the following:\n\nMalaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk.\nMosquito net (net and net_num): A binary variable indicating if the household used mosquito nets.\nEligible for program (eligible): A binary variable indicating if the household is eligible for the free net program.\nIncome (income): The household’s monthly income, in US dollars.\nNighttime temperatures (temperature): The average temperature at night, in Celsius.\nHealth (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health.\nNumber in household (household): Number of people living in the household.\nInsecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.\n\nAccording to the DAG, malaria risk is caused by income, temperatures, health, insecticide resistance, and mosquito net use. People who live in hotter regions, have lower incomes, have worse health, are surrounded by mosquitoes with high resistance to insecticide, and who do not use mosquito nets are at higher risk of contracting malaria than those who do not. Mosquito net use is caused by income, nighttime temperatures, health, the number of people living in the house, and eligibility for the free net program. People who live in areas that are cooler at night, have higher incomes, have better health, have more people in the home, and are eligible for free government nets are more likely to regularly use nets than those who do not. The DAG also shows that eligibility for the free net program is caused by income and household size, since households must meet specific thresholds to qualify.\nFirst, let’s download the dataset, put in a folder named data, and load it:\n\n mosquito_nets.csv\n\n\n\nCode\n# Load the data.\n# It'd be a good idea to click on the \"mosquito_nets.csv\" object in the\n# Environment panel in RStudio to see what the data looks like after you load it\nmosquito_nets &lt;- read_csv(\"data/mosquito_nets.csv\")\n\n\nWe can use this data to check if the relationships defined by our DAG reflect reality. Recall that d-separation implies that nodes are statistically independent of each other and do not transfer associational information. If you draw the mosquito net DAG with dagitty.net, or if you run impliedConditionalIndependencies() in R, you can see a list of all the implied conditional independencies.\n\n\nCode\nimpliedConditionalIndependencies(mosquito_dag)\n## elgb _||_ hlth | incm\n## elgb _||_ mlr_ | hlth, incm, net, tmpr\n## elgb _||_ rsst\n## elgb _||_ tmpr\n## hlth _||_ hshl\n## hlth _||_ rsst\n## hlth _||_ tmpr\n## hshl _||_ incm\n## hshl _||_ mlr_ | hlth, incm, net, tmpr\n## hshl _||_ rsst\n## hshl _||_ tmpr\n## incm _||_ rsst\n## incm _||_ tmpr\n## net _||_ rsst\n## rsst _||_ tmpr\n\n\nThe _||_ symbol in the output here is the \\(\\perp\\) symbol, which means “independent of”. The | in the output means “given”.\nIn the interest of space, we will not verify all these implied independencies, but we can test a few of them:\n\n\\(\\text{Health} \\perp \\text{Household members}\\): (Read as “Health is independent of household member count”.) Health should be independent of the number of people in each household. In the data, the two variables should not be correlated. This is indeed the case:\n\n\nCode\ncor(mosquito_nets$health, mosquito_nets$household)\n## [1] 9.8e-05\n\n\n\\(\\text{Income} \\perp \\text{Insecticide resistance}\\): (Read as “Income is independent of insecticide resistance”.) Income should be independent of insecticide resistance. This is again true:\n\n\nCode\ncor(mosquito_nets$income, mosquito_nets$resistance)\n## [1] 0.014\n\n\n\\(\\text{Malaria risk} \\perp \\text{Household members}\\ |\\ \\text{Health, Income, Bed net use, Temperature}\\): (Read as “Malaria risk is independent of house member count given health, income, bed net use, and temperature”.) Malaria risk should be independent of the number of household members given similar levels of health, income, mosquito net use, and nighttime temperatures. We cannot use cor() to test this implication, since there are many variables involved, but we can use a regression model to check if the number of household members is significantly related to malaria risk. It is not significant (\\(t = -0.17\\), \\(p = 0.863\\)), which means the two are independent, as expected.\n\n\nCode\nlm(malaria_risk ~ household + health + income + net + temperature,\n   data = mosquito_nets) %&gt;%\n  broom::tidy()\n## # A tibble: 6 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)  76.2      0.966      78.9   0        \n## 2 household    -0.0155   0.0893     -0.173 8.63e-  1\n## 3 health        0.148    0.0107     13.9   9.75e- 42\n## 4 income       -0.0751   0.00104   -72.6   0        \n## 5 netTRUE     -10.4      0.266     -39.2   2.63e-241\n## 6 temperature   1.01     0.0310     32.5   1.88e-181\n\n\n\nWe can check all the other conditional independencies to see if the DAG captures the reality of the full system of factors that influence mosquito net use and malaria risk. If there are substantial and significant correlations between nodes that should be independent, there is likely an issue with the specification of the DAG. Return to the theory of how the phenomena are generated and refine the DAG more.\n\n\nMosquito net adjustment sets\nThere is a direct path between mosquito net use and the risk of malaria, but the effect is not causally identified due to several other open paths. We can either list out all the paths and find which open paths have arrows pointing backwards into the mosquito net node (run paths(mosquito_dag) to see these results), or we can let R find the appropriate adjustment sets automatically:\n\n\nCode\nadjustmentSets(mosquito_dag)\n## { health, income, temperature }\n\n\nBased on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk. Importantly, we do not need to worry about any of the nodes related to the government program for free nets, since those nodes are not d-connected to malaria risk. We only need to worry about confounding relationships.\nWe can confirm this graphically with ggdag_adjustment_set():\n\n\nCode\nggdag_adjustment_set(mosquito_dag, shadow = TRUE,\n                     use_labels = \"label\", text = FALSE) +\n  theme_dag()"
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "PSY 607 Bayesian Analysis",
    "section": "",
    "text": "Instructor: Dr. Sara Weston\nEmail: sweston2@uoregon.edu\nOffice: Straub 325\nOffice Hours: Friday 10:00 AM - 12:00 PM, and by appointment (schedule here)\nClass Meetings: Tuesday/Thursday 2:00-3:20 PM\nLocation: 119 FEN\nCourse Website: uobayes.netlify.app"
  },
  {
    "objectID": "syllabus.html#assignment-submission",
    "href": "syllabus.html#assignment-submission",
    "title": "PSY 607 Bayesian Analysis",
    "section": "Assignment Submission",
    "text": "Assignment Submission\n\nAll assignments should be submitted through Canvas\nSubmissions must be in PDF format, knitted from R Markdown\nAssignments are posted on Friday and due the following Monday at 11:59pm\nThere is a one-week grace period for each assignment"
  },
  {
    "objectID": "syllabus.html#slack",
    "href": "syllabus.html#slack",
    "title": "PSY 607 Bayesian Analysis",
    "section": "Slack",
    "text": "Slack\nI have created a Slack workspace for this course, open to anyone (registerd or not) who is attending. The goal of this space is for us to support each other as we learn the material. Anyone can post a question, and anyone can answer a question. I will not be as active here as I am in class, nor as responsive as I am over email. But if I see a question has the class stumped or it feels like an important concept hasn’t clicked yet, I will update lecture materials to help us get on track.\nI especially encourage using this space to coordinate working on probelm sets together, sharing resources you find online, and sharing ways you’ve used technology (R/RMarkdown/AI) that have supported your research. I also encourage memes."
  }
]