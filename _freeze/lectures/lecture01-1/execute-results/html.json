{
  "hash": "5c2673f7f132389f7070f8b0dead4d12",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 1: Introduction to Bayesian Analysis\"\nsubtitle: \"An Overview of Bayesian Thinking\"\nformat: \n  revealjs:\n    css: xaringan-themer2.css\n    nature:\n      highlightStyle: solarized-dark\n      highlightLines: true\n      countIncrementalSlides: false\n      mathjax: \"default\"\n    self-contained: false  # Ensures correct embedding\n    embed-resources: true  # Embeds required assets\n    slide-number: true\nexecute:\n  echo: false  \n---\n\n\n\n## welcome!\n\nI'm excited to be here.\n\n-   I'm learning the material along with you.\n-   Drawing on materials and resources made by a master (Richard McElreath)\n\n\n\n::::: columns\n::: {.column width=\"50%\"}\nGoals of the class[^lecture01-1-1]:\n\n-   Develop an intuition for statistical modeling using a Bayesian framework.\n-   Learn how to execute Bayesian statistics using R.\n:::\n\n::: {.column width=\"50%\"}\nMy commitments:\n\n-   Be here[^lecture01-1-2], be excited, be patient, be flexible.\n:::\n:::::\n\n---\n\n**COURSE FORMAT**: Flipped class style\n\n::::: columns\n::: {.column width=\"50%\"}\n  - Before class (2x/week)\n    - watch video lectures by McElreath\n    \n  - During class (2x/week)\n    - comprehension quiz (5% of grade)\n    - review major concepts\n    - practice code\n:::\n\n::: {.column width=\"50%\"}\n  - After class (1x/week)\n    - weekly homework assignment (95% of grade) (due Mondays)\n      - graded on completion, not accuracy\n      - one-week grace period\n      - solutions posted on Tuesday -- up to you to score your assignment and review missed questions\n:::\n\n:::::\n\n------------------------------------------------------------------------\n\nWorkspace setup:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cowplot)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n# probability\n\nFor today, we'll build a foundation by reviewing probability (covered in PSY 611 -- remember?) and connecting these ideas to Bayesian frameworks for calculating and thinking about probability.\n\nI'll draw on an article, [Introduction to Bayesian Inference](../readings/Etz%20and%20Vandekerckhove%20-%202018%20-%20Introduction%20to%20Bayesian%20Inference%20for%20Psychology.pdf) by Alex Etz and Joachim Vandekerckove (2018, *Psychon Bull Review*).\n\n------------------------------------------------------------------------\n\n## Interpretation\n\n-   **Epistemic:** probability is the *degree of belief*.\n    -   A number between 0 and 1 that quantifies how strongly we think something is true based on relevant information.\n    -   There is no such thing as *the* probability. There is only *your* probability.\n    -   BUT probability is not *arbitrary*.\n-   **Aleatory:** probability is a statement of the expected frequency over many repetitions of a procedure.\n    -   Cannot speak to singular events.\n    -   Assumes independence among repetitions.\n    -   Can be a valid conceptual interpretation but is rarely ever an operational one.\n\nIn the vast majority of cases, psychologists are trying to make statements about singular events: - *this* theory is true or not. - *this* effect is positive or negative. - *this* model or *that* model is more likely.\n\n------------------------------------------------------------------------\n\n## Notation\n\n-   $P(A)$ is the probability of event A.\n-   $P(A, B)$ is the probability that both A and B happen.\n-   $P(B|A)$ is the probability of event B given that A is true.\n\n------------------------------------------------------------------------\n\n## Product Rule: Basics\n\n-   **Formula**:\n\n    $$\n    P(A, B) = P(A)P(B|A) = P(B)P(A|B)\n    $$\n\n-   **Meaning**:\n\n    -   The probability of $(A)$ and $(B)$ occurring together -- $P(A, B)$ -- can be calculated using:\n        -   $P(A)$: Probability of $(A)$.\n        -   $P(B|A)$: Probability of $(B)$ given $(A)$, or vice versa.\n\n------------------------------------------------------------------------\n\n## Product Rule: Example\n\n-   **Scenario**:\n    -   Toss a coin twice.\n    -   $(A)$: First toss is heads.\n    -   $(B)$: Second toss is heads.\n-   **Given**:\n    -   $P(A) = 0.5$ (fair coin).\n    -   $P(B|A) = 0.5$ (independent tosses).\n-   **Using the product rule**: $$\n    P(A, B) = P(A)P(B|A) = 0.5 \\times 0.5 = 0.25\n    $$\n\n------------------------------------------------------------------------\n\n## Product Rule: Intuition\n\n-   The joint probability -- $P(A, B)$ -- reflects:\n\n    -   The likelihood of one event.\n    -   Adjusted by how the second event depends on the first.\n\n-   **For Independent Events**: $$\n    P(A, B) = P(A)P(B)\n    $$\n\n-   **For Dependent Events**: $$\n    P(A, B) = P(A)P(B|A)\n    $$\n\n------------------------------------------------------------------------\n\nLet's say A is the event that it rains today and B is the event that it rains tomorrow. There's a 60% chance it will rain today. If it does rain today, it'll probably rain tomorrow (let's say 2/3 chance). But if it doesn't rain today, it probably won't rain tomorrow (p = .625).\n\n-   $P(A) = .6$\n-   $P(B|A) = .667$\n-   $P(\\neg B|\\neg A) = .625$\n\n------------------------------------------------------------------------\n\n![](images/image1-1/Slide1.png)\n\nThe probability of the joint events are found by multiplying the values along a path.\n\n------------------------------------------------------------------------\n\n## Sum Rule: Basics\n\n-   **Formula**: $$\n    P(A) = P(A, B) + P(A, \\neg B)\n    $$\n\n-   **Meaning**:\n\n    -   The probability of $(A)$ happening is the sum of:\n        -   $P(A, B)$: Probability of $(A)$ and $(B)$ both happening.\n        -   $P(A, \\neg B)$: Probability of $(A)$ happening without $(B)$.\n\n-   **Disjoint set**:\n\n    -   A collection of mutually exclusive events.\n\n------------------------------------------------------------------------\n\n## Sum Rule: Example\n\n-   **Scenario**:\n\n    -   Drawing a card from a deck.\n    -   $(A)$: The card is red.\n    -   $(B)$: The card is a heart.\n    -   $(\\neg B)$: The card is a diamond.\n\n-   **Using the Sum Rule**: $$\n    P(A) = P(A, B) + P(A, \\neg B)\n    $$\n\n-   **Given**:\n\n    -   $P(A, B) = \\frac{13}{52}$ (hearts).\n    -   $P(A, \\neg B) = \\frac{13}{52}$ (diamonds).\n\n-   **Result**: $$\n    P(A) = \\frac{13}{52} + \\frac{13}{52} = 0.5\n    $$\n\n------------------------------------------------------------------------\n\n## Sum Rule: Intuition\n\n-   The sum rule finds **total probability** by accounting for all disjoint ways $(A)$ can occur.\n\n-   **General Formula**: $$\n    P(A) = \\sum_{i} P(A, B_i)\n    $$\n\n-   Ensures that no possibilities are overlooked.\n\n------------------------------------------------------------------------\n\n### Exercise\n\n![](images/image1-1/Slide1.png)\n\nConstruct the equivalent path diagram starting on the left with a fork that depends on event B, instead of event A.\n\n------------------------------------------------------------------------\n\n### Exercise: Solution\n\n![](images/image1-1/Slide2.png)\n\n------------------------------------------------------------------------\n\n## Bayesian inference\n\nBayesian inference is the application of the product and sum rules to real problems of inference.\n\n-   Consider $H$ to be a hypothesis and $\\neg H$ to be the competing hypothesis.\n-   Before any data are collected, the researcher has some belief in these hypotheses. These are priors or **prior probabilities**, $P(H)$ and $P(\\neg H)$.\n-   These hypotheses are well-defined if they make a specific prediction about each experimental outcome (D) through a *likelihood function* like $P(D|H)$ and $P(D|\\neg H)$.\n    -   Likelihoods are how strongly data (D) are implied by a hypothesis.\n    -   Think NHST: $P(D|H_0)$.\n\n------------------------------------------------------------------------\n\n### Bayes' Rule\n\nIf\n\n$$\nP(H, D) = P(D)P(H|D) = P(H)P(D|H)\n$$ then: $$\nP(H|D) = \\frac{P(H)P(D|H)}{P(D)}\n$$\n\nThis is Bayes' Rule!\n\n-   **prior probability**: $P(H)$\n\n-   **likelihood function**: $P(D|H)$\n\n-   **posterior probabilities**: $P(H|D)$\n\n::: notes\nOn the board\n\nThe Product Rule states that $$\nP(H, D) = P(D)P(H|D)\n$$ therefore: $$\nP(H|D) = \\frac{P(H, D)}{P(D)}\n$$ In addition, $$\nP(H, D) = P(H)P(D|H)\n$$ so we can replace the numerator: $$\nP(H|D) = \\frac{P(H)P(D|H)}{P(D)}\n$$\n:::\n\n------------------------------------------------------------------------\n\n### Prior predictive probability $P(D)$\n\nHow do we calculate this probability? We use the Sum Rule.\n\n$$\nP(D)  = P(D, H) + P(D, \\neg H) \\\\\n      = P(H)P(D|H) + P(\\neg H)P(D|\\neg H)\n$$ Now, we can rewrite Bayes' Rule using only prior probabilities and likelihoods.\n\n$$\nP(H|D) = \\frac{P(H)P(D|H)}{P(H)P(D|H) + P(\\neg H)P(D|\\neg H)}\n$$ And we can express our posterior in any case with K competing and mutually-exclusive hypotheses.\n\n$$\nP(H|D) = \\frac{P(H)P(D|H)}{\\sum_{k = 1}^K P(H_k)P(D|H_k)}\n$$\n\n------------------------------------------------------------------------\n\n### Bayes' Rule Terminology\n\n-   **prior probabilities:** $P(H)$ and $P(\\neg H)$\n    -   Probability prior to seeing data.\n-   **likelihood functions:** $P(D|H)$ and $P(D|\\neg H)$\n    -   function showing how likely an outcome/data are given a specific hypothesis.\n-   **posterior probabilities:**: $P(H|D)$ and $P(\\neg H|D)$\n    -   probability of a hypothesis given data. A combination of prior probabilities and likelihood functions.\n\n------------------------------------------------------------------------\n\n### Quantifying evidence using Bayes' rule\n\nWe form a ratio of relative belief in one hypothesis vis-a-vis another by comparing their posterior odds:\n\n$$\n\\frac{P(H|D)}{P(\\neg H|D)}\n$$ We can insert the equations for posterior odds and find that this reduces to:\n\n$$\n\\frac{P(H)}{P(\\neg H)} \\times \\frac{P(D|H)}{P(D|\\neg H)}\n$$ The first part is called the *prior odds* and the second is called *Bayes factor*.\n\n**Bayes Factor:** the extent to which the data sway our relative belief from one hypothesis to the other.\n\n::: notes\nFull equation: $$\n\\frac{\\frac{P(H)P(D|H)}{P(H)P(D|H) + P(\\neg H)P(D|\\neg H)}}{\\frac{P(\\neg H)P(D|\\neg H)}{P(H)P(D|H) + P(\\neg H)P(D|\\neg H)}}\n$$ Denominators cancel out.\n:::\n\n------------------------------------------------------------------------\n\nBayes factors are **not** the same as posterior probabilities.\n\n-   Bayes factors: a \"learning factor\" -- tells us how much evidence the data have delivered.\n-   posterior probabilities: our total belief after taking into account the data and our prior beliefs.\n\n------------------------------------------------------------------------\n\n## Example: Professor Sprout\n\nAt Hogwarts, professor Sprout leads the Herbology Department. In the Department’s greenhouses, she cultivates a magical plant that when consumed causes a witch or wizard to feel euphoric and relaxed. Professor Trelawney, the professor of Divination, is an avid user of this plant and frequently visits Professor Sprout’s laboratory to sample the latest harvest.\n\nHowever, it has turned out that one in a thousand codacle plants is afflicted with a mutation that changes its effects: Consuming mutated plants causes unpleasant side effects such as paranoia, anxiety, and spontaneous levitation.\n\nIn order to evaluate the quality of her crops, Professor Sprout has developed a mutation-detecting spell. The new spell has a 99% chance to accurately detect an existing mutation, but also has a 2% chance to falsely indicate that a healthy plant is a mutant. When Professor Sprout presents her results at a School colloquium, Trelawney asks two questions: What is the probability that a plant is a mutant, when your spell says that it is? And what is the probability the plant is a mutant, when your spell says that it is healthy?\n\n------------------------------------------------------------------------\n\n-   The Professor Sprout example illustrates **Bayesian reasoning**.\n-   We calculate probabilities of plant mutations based on:\n    -   Prior probability of mutation.\n    -   Diagnostic spell's sensitivity and specificity.\n-   Key questions:\n    1.  What is the probability the plant is a mutant, given a \"mutant\" diagnosis, $P(M|D)$?\n    2.  What is the probability the plant is a mutant, given a \"not mutant\" diagnosis, $P(M|\\neg D)$?\n\n------------------------------------------------------------------------\n\n### Question 1: $P(M|D)$\n\n-   **Bayes' Rule**: $$\n    P(M|D) = \\frac{P(M)P(D|M)}{P(M)P(D|M) + P(\\neg M)P(D|\\neg M)}\n    $$\n-   **Values**:\n    -   $P(M) = 0.001$, $P(D|M) = 0.99$\n    -   $P(\\neg M) = 0.999$, $P(D|\\neg M) = 0.02$\n-   Plugging in the values: $$\n    P(M|D) = \\frac{0.001 \\times 0.99}{(0.001 \\times 0.99) + (0.999 \\times 0.02)}\n    $$\n-   Despite the spell's high sensitivity and specificity, $P(M|D) \\approx 4.7\\%$.\n-   Why? **Mutations are extremely rare**.\n    -   Even with an accurate test, most positive results are false positives.\n\n------------------------------------------------------------------------\n\n#### An intuitive understanding\n\nInstead of reporting probabilities, let's summarize the problem another way:\n\n1.  In a greenhouse of 100,000 plants, 100 are mutants.\n2.  Of the 100 mutants, 99 will be detected.\n3.  Of the 99,900 healthy plants, 1,998 will be falsely detected by the spell.\n\n$$\n\\frac{99}{99+1998} \\approx .047\n$$\n\n------------------------------------------------------------------------\n\n### Question 2: $$P(M|\\neg D)$$\n\n-   **Bayes' Rule**: $$\n    P(M|\\neg D) = \\frac{P(M)P(\\neg D|M)}{P(M)P(\\neg D|M) + P(\\neg M)P(\\neg D|\\neg M)}\n    $$\n-   **Values**:\n    -   $P(\\neg D|M) = 1 - P(D|M) = 0.01$\n    -   $P(\\neg D|\\neg M) = 0.98$\n\n------------------------------------------------------------------------\n\n-   Plugging in the values: $$\n    P(M|\\neg D) = \\frac{0.001 \\times 0.01}{(0.001 \\times 0.01) + (0.999 \\times 0.98)}\n    $$\n\n-   If the spell indicates \"not mutant,\" the plant is almost certainly not a mutant: $$\n    P(M|\\neg D) \\approx 0.001\\%\n    $$\n\n-   Why? The specificity ($P(\\neg D|\\neg M) = 98\\%$) ensures most negatives are true negatives.\n\n------------------------------------------------------------------------\n\n### Exercise\n\nDiagram this example (similar to the rain/no rain diagram).\n\n-   $P(M) = 0.001$, $P(D|M) = 0.99$\n-   $P(\\neg M) = 0.999$, $P(D|\\neg M) = 0.02$\n\n------------------------------------------------------------------------\n\n### Exercise: Solution\n\nDiagram this example (similar to the rain/no rain diagram).\n\n-   $P(M) = 0.001$, $P(D|M) = 0.99$\n-   $P(\\neg M) = 0.999$, $P(D|\\neg M) = 0.02$\n\n![](images/image1-1/Slide3.png)\n\n------------------------------------------------------------------------\n\n### Exercise\n\nSuppose, however, that Trelawney knows that Professor Sprout’s diagnosis $(H_S)$ is statistically independent from the diagnosis of her talented research associate Neville Longbottom $(D_L)$ — meaning that for any given state of nature $M$ or $\\neg M$, Longbottom’s diagnosis does not depend on Sprout’s. Further suppose that both Sprout and Longbottom return the mutant diagnosis (and for simplicity we also assume Longbottom’s spells are equally as accurate as Sprout’s). To find the posterior probability the plant is a mutant after two independent mutant diagnoses, $P (M|H_S, D_L)$, Trelawney can apply a fundamental principle in Bayesian inference: Yesterday’s posterior is today’s prior.\n\n**What is the probability the plant is mutant after two independent mutant diagnoses?**\n\n------------------------------------------------------------------------\n\n### Exercise: Solution\n\nBecause diagnosis $H_S$ and diagnosis $D_L$ are independent, we know that: $$\nP(D_L|M, H_S) = P(D_L|M)\n$$ and $$\nP(D_L|\\neg M, H_S) = P(D_L|\\neg M)\n$$ therefore\n\n$$\nP(M|H_S, D_L) = \\frac{P(M|H_S)P(D_L|M)}{P(M|H_S)P(D_L|M) + P(\\neg M|H_S)P(D_L|\\neg M)}\n$$\n\n$$\n= \\frac{.047 \\times .99}{.047 \\times .99 + .953 \\times .02} \\approx .71\n$$\n\n------------------------------------------------------------------------\n\n### Key Takeaways\n\n1.  **Posterior probabilities depend heavily on prior probabilities**.\n\n    -   Rare conditions remain unlikely even with positive test results.\n    -   Accurate tests reduce uncertainty but don’t guarantee certainty.\n\n2.  **Bayes' Rule updates prior beliefs with evidence**: $$\n    P(H|D) \\propto P(H) \\times P(D|H)\n    $$\n\n3.  **There is value in multiple independent sources of evidence**:\n\n-   a plant only once diagnosed has a small chance of being a mutant.\n-   a plant that has twice been independently diagnosed as a mutant is quite likely to be one.\n\n4.  Practical applications:\n    -   Diagnostic testing in medicine.\n    -   Risk assessment in rare-event scenarios.\n\n------------------------------------------------------------------------\n\n## Example: Sorting Hat\n\n-   Hogwarts' Sorting Hat was damaged by a curse during the battle against You-Know-Who.\n-   It now assigns students to **Slytherin** 40% of the time, regardless of their true house.\n    -   40% of students are assigned to Slytherin.\n    -   Only 20% each to Gryffindor, Ravenclaw, and Hufflepuff.\n-   Professor Binns develops a **Diagnostic Test**:\n    -   PARSEL test (Placement Accuracy Remedy for Students Erroneously Labeled):\n        -   Scores predict true house:\n            -   Excellent (E): Likely Slytherin.\n            -   Other scores (Outstanding, Acceptable, Poor) indicate other houses.\n\n------------------------------------------------------------------------\n\n### Data from the PARSEL Test\n\n-   **Benchmark results** from students sorted before the Battle of Hogwarts:\n    -   Probability of test scores by true house:\n\n| Score       | Slytherin | Gryffindor | Ravenclaw | Hufflepuff |\n|-------------|-----------|------------|-----------|------------|\n| Excellent   | 0.80      | 0.05       | 0.05      | 0.00       |\n| Outstanding | 0.10      | 0.20       | 0.80      | 0.10       |\n| Acceptable  | 0.05      | 0.70       | 0.15      | 0.25       |\n| Poor        | 0.05      | 0.05       | 0.00      | 0.65       |\n\n------------------------------------------------------------------------\n\n## Question: Is the Student a Gryffindor?\n\nProfessor McGonigall wants to know how likely it would be that a true Gryffindor will end up in Slytherin after taking this test.\n\n-   **Goal**:\n    -   Calculate $P(\\text{Gryffindor}|H_S, S_E)$.\n\n------------------------------------------------------------------------\n\n### Step 1: Bayes' Rule for Gryffindor\n\n$$\nP(\\text{Gryffindor}|H_S, S_E) = \\frac{P(\\text{Gryffindor})P(H_S|\\text{Gryffindor})P(S_E|\\text{Gryffindor})}{P(H_S, S_E)}\n$$\n\n-   Known probabilities:\n\n    -   $P(\\text{Gryffindor}) = 0.25$ (prior probability of Gryffindor).\n    -   $P(H_S|\\text{Gryffindor}) = 0.20$ (damaged Hat assigns 20% of Gryffindors to Slytherin).\n    -   $P(S_E|\\text{Gryffindor}) = 0.05$.\n\n------------------------------------------------------------------------\n\n### Step 2: Calculate Marginal Probability\n\nWhat is the probability a student is sorted into Slytherin and also scores Excellent?\n\n$P(H_S, S_E) = \\sum_{i} P(\\text{House}_i)P(H_S|\\text{House}_i)P(S_E|\\text{House}_i)$\n\n-   Contribution from all houses:\n\n| House | Prior $P(\\text{House})$ | $P(H_S \\text{given House})$ | $P(S_E\\text{given House})$ | Joint $P(H_S, S_E)$ |\n|---------------|---------------|---------------|---------------|---------------|\n| Slytherin | 0.25 | 1.00 | 0.80 | 0.2000 |\n| Gryffindor | 0.25 | 0.20 | 0.05 | 0.0025 |\n| Ravenclaw | 0.25 | 0.20 | 0.05 | 0.0025 |\n| Hufflepuff | 0.25 | 0.20 | 0.00 | 0.0000 |\n\n------------------------------------------------------------------------\n\n### Step 3: Calculate Posterior for Gryffindor\n\n-   Marginal probability: $$\n    P(\\text{Slytherin, Excellent}) = 0.2000 + 0.0025 + 0.0025 + 0.0000 = 0.2050\n    $$\n\n-   Posterior probability: $$\n    P(\\text{Gryffindor}|(H_S, S_E)) = \\frac{P(\\text{Gryffindor})P(H_S|\\text{Gryffindor})P(S_E|\\text{Gryffindor})}{P(H_S, S_E)}\n    $$\n\n-   Calculation: $$\n    P(\\text{Gryffindor}|(H_S, S_E)) = \\frac{0.25 \\times 0.20 \\times 0.05}{0.2050} = \\frac{0.0025}{0.2050} \\approx 0.0122\n    $$\n\n------------------------------------------------------------------------\n\n### Interpretation\n\n-   The student has only about a **1.2% probability** of being a true Gryffindor.\n-   Most students sorted into Slytherin with an Excellent PARSEL score are true Slytherins:\n    -   $P(\\text{Slytherin}|(H_S, S_E)) \\approx 0.975$.\n\n------------------------------------------------------------------------\n\n## Bayes in the continuous case\n\n::::: columns\n::: {.column width=\"50%\"}\nMost of our research questions are about parameters in the continuous case. For this, we make use of probability *density* functions. Densities:\n\n-   express how much a probability exists \"near\" *a* particular value of *a*, while the probability of any particular value is zero.\n-   probability is the integral of the density function over a certain interval:\n\n$P(a_1 < A < a_2) = \\int_{a_1}^{a_2}p(a)da$\n\n-   the total area under the density curve is 1.\n\n$P(-\\ < A < a_2) = \\int_{a_1}^{a_2}p(a)da$\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture01-1_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n:::\n:::::\n\n::: notes\nArea in both is .10 or 10% Below 81 between 108 and 113\n:::\n\n------------------------------------------------------------------------\n\nThe product and sum rules have continuous analogues:\n\n-   Product rule:\n\n$$\np(a,b) = p(a)p(b|a)\n$$\n\nWhere $p(a)$ is the density of the continuous parameter *a*, and $p(b|a)$ is the conditional density of *b* (assuming a value of *a*).\n\n-   Sum rule:\n\n$$\np(a) = \\int_Bp(a,b)db\n$$\n\n$db$ -- the *differential* -- represents an infinitesimally small interval of the variable $b$. It is part of the integral that sums (or integrates) the joint probability $p(a, b)$ over all possible values of $b$ within the range $B$. - In other words, the $db$ indicates that we're integrating over values of $b$ (not values of $a$).\n\n------------------------------------------------------------------------\n\n### Bayes' Rule in Continuous Case\n\n-   Derived from the product rule: $$\n    p(a | b) = \\frac{p(a, b)}{p(b)} = \\frac{p(a)p(b | a)}{p(b)}\n    $$\n\n-   **Numerator**:\n\n    -   Combines prior $p(a)$ and likelihood $p(b | a)$.\n\n-   **Denominator**:\n\n    -   Marginal likelihood $p(b)$, ensuring the posterior is a proper probability density:\n\n        $$\n        p(b) = \\int_A p(a)p(b | a) da\n        $$\n\n------------------------------------------------------------------------\n\n### Bayesian Parameter Estimation\n\n-   Posterior density:\n\n    $$\n    p(\\theta | x) = \\frac{p(\\theta) p(x | \\theta)}{\\int_\\Theta p(\\theta) p(x | \\theta) \\, d\\theta}\n    $$\n\n    -   $\\theta$: Parameter of interest.\n    -   $p(\\theta)$: Prior belief.\n    -   $p(x | \\theta)$: Likelihood based on data $x$.\n\n-   **Numerator**:\n\n    -   $p(\\theta)p(x | \\theta)$: Unnormalized posterior.\n\n-   **Denominator**:\n\n    -   Normalizing constant (ensures posterior sums to 1).\n\n------------------------------------------------------------------------\n\n## Example: Puking pastilles\n\n-   **Context**:\n    -   George Weasley is developing a new gag product: Puking Pastilles.\n    -   The pastilles cause multiple \"expulsion events\" (vomiting) over time.\n    -   George wants to estimate the **expulsion rate** ($\\lambda$) more precisely.\n-   **Objective**:\n    -   Use Bayesian parameter estimation to determine the most plausible values of $\\lambda$.\n    -   Incorporate prior beliefs and observed data.\n\n------------------------------------------------------------------------\n\n### Model: Poisson Distribution\n\n-   Expulsion events are modeled with a **Poisson distribution** [^lecture01-1-3] :\n\n$$\np(x | \\lambda) = \\frac{1}{x!} \\lambda^x \\text{exp}(-\\lambda)\n$$ - $\\lambda$: The expected number of events per time interval. - $x$: Observed number of events in an interval.\n\n-   **Key property**:\n    -   The Poisson distribution is parameterized by $\\lambda$, representing the **rate**.\n\n------------------------------------------------------------------------\n\n### Example Poisson distributions\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture01-1_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### Prior Belief\n\n-   George's prior belief about $\\lambda$:\n    -   Based on customer feedback, expulsion rates likely range between **3 and 5** events/hour.\n    -   Chosen prior distribution: **Gamma distribution**:\n\n$$\np(\\lambda | a, b) = \\frac{b^a}{\\Gamma(a)} \\text{exp}(-b \\lambda)\\lambda^{a - 1}\n$$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture01-1_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### Prior distribution\n\n-   Parameters: $a = 2$, $b = 0.2$. - How were these chosen? Some knowledge about the gamma distribution and some trial and error.\n    -   The Gamma distribution ensures $\\lambda > 0$ and is conjugate to the Poisson likelihood.\n\n::::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture01-1_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n-   priors are not point estimates.\n    -   \"I think lambda is 5.\" -- this is *not* a prior.\n-   priors are distributions: the relatively likelihood of every single possible value of the parameter.\n    -   lambda is most likely between 3 and 7, unlikely to be 1 or 2, possibly above 10...\n:::\n:::::\n\n------------------------------------------------------------------------\n\nGeorge collects data from three experiments:\n\n-   $x_1 = 7$, $x_2 = 8$, $x_3 = 19$.\n\nLikelihood for the data:\n\n$p(X_n | \\lambda) = \\prod^n_I \\frac{1}{x_i!} \\lambda^{x_i} \\text{exp}(-\\lambda)$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture01-1_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n**Posterior Distribution**\n\nBayes’ Rule:\n\n$$\np(\\lambda | X_n) \\propto p(\\lambda) p(X_n | \\lambda)\n$$\n\nConjugacy simplifies the posterior to another Gamma distribution:\n\n$$\np(\\lambda | X_n) = \\text{Gamma}\\left(a + \\sum x_i, b + n \\right)\n$$\n\n-   Parameters of the posterior:\n\n-   $a_{\\text{post}} = a + \\sum x_i = 2 + (7 + 8 + 19) = 36$\n\n-   $b_{\\text{post}} = b + n = 0.2 + 3 = 3.2$\n\n------------------------------------------------------------------------\n\n**Results**\n\n::::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture01-1_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\nPosterior Mean:\n\n$\\mathbb{E}[\\lambda | X_n] = \\frac{a_{\\text{post}}}{b_{\\text{post}}} = \\frac{36}{3.2} \\approx 11.25$\n\n**Posterior Mode:**\n\n$\\lambda_{\\text{mode}} = \\frac{a_{\\text{post}} - 1}{b_{\\text{post}}} = \\frac{36 - 1}{3.2} \\approx 10.94$\n\n-   90% credible interval:\n-   Calculated from the Gamma distribution:\n-   Lower bound: $8.3$\n-   Upper bound: $14.5$\n:::\n:::::\n\n------------------------------------------------------------------------\n\n## Formulas and equations\n\nBayesian analysis is the act of applying product and sum rules to probability.\n\nIs that not your cup of tea?\n\nDon't worry! Most of this term, we'll be abandoning the formulas altogheter -- there are easier and more intuitive ways to fit and use these models.\n\nSo why did we just go through all of this? - So you can appreciate the work done by others to make this more approachable. - Because you will need to get familiar with different probability distributions (Poisson, gamma, Cauchy, etc). These form the basis of your prior distributions, so knowing what they look like and how to set good priors using them is essential.\n\n[^lecture01-1-1]: These goals are in tension with each other. Good pedigogical code is bad application code. We'll start with the former, move to the latter, but sometimes return to pedigogy.\n\n[^lecture01-1-2]: With the important caveat that I have a one-year-old in daycare, so on any given day, he or I or both of us are sick. My hope is the planned course structure is forgiving of missed days without\n\n[^lecture01-1-3]: $\\text{exp}(-\\lambda)$ is a clearer way to write $e^{-\\lambda}$\n",
    "supporting": [
      "lecture01-1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}