{
  "hash": "831918c004693521d2177a1f25f9cf8b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 4: Overfitting/MCMC\"\nsubtitle: \"Overfitting\"\nformat: \n  revealjs:\n    css: xaringan-themer2.css\n    nature:\n      highlightStyle: solarized-dark\n      highlightLines: true\n      countIncrementalSlides: false\n      mathjax: \"default\"\n    self-contained: false  # Ensures correct embedding\n    embed-resources: true  # Embeds required assets\n    slide-number: true\nexecute:\n  echo: false  \n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n------------------------------------------------------------------------\n\nA central tension in our modeling is the one between explanation -- good causal models -- and prediction. In McElreath's lecture, he leads us to the intuition that predictive models are generally those that do a terrible job of representing the causal model. So the tools covered in this lecture should be considered tools for prediction, but *not* for identifying causal models.\n\nWhen trying to maximize prediction, we need to be wary of **OVERFITTING** -- when the model learns too much from the sample. Methods for avoiding overfitting favor simpler models. However, we must also be wary of **UNDERFITTING** or learning too little. \n\nThere are two common famililes of approches:\n\n  1. Use of a **REGULARIZING** prior, which helps stop the model from becoming too excited about any one data point. \n  \n  2. Use of a scoring device, like **INFORMATION CRITERIA** and **CROSS-VALIDATION**, to estimate predictive accuracy. \n\n------------------------------------------------------------------------\n\n\n### the problem with parameters\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsppnames <- c( \"afarensis\",\"africanus\",\"habilis\",\"boisei\",\"rudolfensis\",\"ergaster\",\"sapiens\")\nbrainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )\nmasskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )\nd <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )\nbase = d %>% \n  ggplot(aes(x=masskg, y=brainvolcc)) +\n  geom_point() +\n  geom_text(aes(label=sppnames), hjust=0, nudge_x = 1) +\n  labs(x=\"body mass (kg)\", y=\"brain volume (cc)\")\np1 = base + geom_smooth(method='lm', se =F) +ggtitle(\"Simple linear model\")\np2 = base + geom_smooth(method='lm', se =F, formula=y~poly(x, 6)) +ggtitle(\"6th degree polynomial\")\n(p1 | p2)\n```\n\n::: {.cell-output-display}\n![](lecture04-1_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\nIf your goal is simply prediction, not (causal) explanation, why not simply add as many variables into the model as possible? \n\n  * Adding parameters nearly always increases measures of model fit, like $R^2$.\n  * More complex models fit the data in-hand better, but often predict new data worse. \n\n::: notes\nmodels = data compression when there are as many parameters as data points, we haven't compressed the data. we've just encoded the raw data into a different form.\n:::\n\n------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nd$mass_std <- (d$mass - mean(d$mass))/sd(d$mass)\nd$brain_std <- d$brain / max(d$brain)\n\nm7.1 <- quap(\n  alist(\n    brain_std ~ dnorm( mu , exp(log_sigma) ),\n    mu <- a + b*mass_std,\n    a ~ dnorm( 0.5 , 1 ),\n    b ~ dnorm( 0 , 10 ),\n    log_sigma ~ dnorm( 0 , 1 )\n  ), data=d )\n\nm7.6 <- quap(\n  alist(\n  brain_std ~ dnorm( mu , 0.001 ),\n  mu <- a + b[1]*mass_std + b[2]*mass_std^2 +\n    b[3]*mass_std^3 + b[4]*mass_std^4 +\n    b[5]*mass_std^5 + b[6]*mass_std^6,\n  a ~ dnorm( 0.5 , 1 ),\n  b ~ dnorm( 0 , 10 )\n ), data=d , start=list(b=rep(0,6)) )\n\npar(mfrow=c(1,2))\nbrain_loo_plot(m7.1)\nbrain_loo_plot(m7.6)\n```\n\n::: {.cell-output-display}\n![](lecture04-1_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n------------------------------------------------------------------------\n\n::::: columns\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Set up plotting area\nplot(0, 0, type = \"n\", xlim = c(-3, 3), ylim = c(0, 2), \n     xlab = \"parameter value\", ylab = \"Density\",\n     main = \"\")\n\n# Create x-values for plotting\nx <- seq(-3, 3, length.out = 1000)\n\n# Generate three density curves with different spreads\n# Thick curve - very peaked (high kurtosis)\ny1 <- dnorm(x, mean = 0, sd = 0.3)\n# Scale to match the peak height in the original\ny1 <- y1 * (2/max(y1))\n\n# Medium curve - moderate spread\ny2 <- dnorm(x, mean = 0, sd = 0.6)\n# Scale to match the peak height in the original\ny2 <- y2 * (0.8/max(y2))\n\n# Dashed curve - most spread (normal distribution)\ny3 <- dnorm(x, mean = 0, sd = 1)\n# Scale to match the peak height in the original\ny3 <- y3 * (0.4/max(y3))\n\n# Add the curves to the plot\nlines(x, y1, lwd = 3)\nlines(x, y2, lwd = 1)\nlines(x, y3, lwd = 1, lty = 2)\n```\n\n::: {.cell-output-display}\n![](lecture04-1_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\nOne tool in our toolbelt is regularizing priors. **REGULARIZATION** is a means by which you prevent the model from being \"too excited\" by the training sample, or to fit too closely to the specific patterns in that sample. There are many tools for regularization (ridge regression, lasso regression), but they all have the effect of downweighting regression parameters towards 0. \n\nAs a Bayesian, you also have the tool of regularizing priors. As your priors become more \"skeptical\" (usually, closer to 0), your model will adapt less to the data. Be wary of having priors that are too tight, because you risk underfitting. (Of course, the more data you have, the less influence your priors have. But that shouldn't concern you, because overfitting is less of a concern with larger datasets.)\n\n:::\n:::::\n\n---\n\nThis is a pretty extreme example of setting a skeptical prior, but it demonstrates the point.\n\n::::: columns\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"8\"}\nm7.6a <- quap(\n  alist(\n    brain_std ~ dnorm( mu , 0.001 ),\n    mu <- a + b[1]*mass_std + b[2]*mass_std^2 +\n      b[3]*mass_std^3 + b[4]*mass_std^4 +\n      b[5]*mass_std^5 + b[6]*mass_std^6,\n    a ~ dnorm( 0.5 , 1 ),\n    b ~ dnorm( 0 , 10 )\n  ), data=d , start=list(b=rep(0,6)) )\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"8\"}\nm7.6b <- quap(\n  alist(\n    brain_std ~ dnorm( mu , 0.001 ),\n    mu <- a + b[1]*mass_std + b[2]*mass_std^2 +\n      b[3]*mass_std^3 + b[4]*mass_std^4 +\n      b[5]*mass_std^5 + b[6]*mass_std^6,\n    a ~ dnorm( 0.5 , 1 ),\n    b ~ dnorm( 0 , .05 )\n  ), data=d , start=list(b=rep(0,6)) )\n```\n:::\n\n\n\n:::\n:::::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoeftab( m7.6a, m7.6b )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     m7.6a   m7.6b  \nb[1]    0.88    1.23\nb[2]    1.70    0.57\nb[3]   -0.61   -1.55\nb[4]   -3.48   -1.78\nb[5]   -0.35    0.27\nb[6]    1.63    0.87\na       0.51    0.72\nnobs       7       7\n```\n\n\n:::\n:::\n\n\n\nOf course, we don't want our priors to be too skeptical, or we'll miss important (and regular) information. So we'll also make use of information theory to derive indicies that estimate out-of-sample prediction. Let's refer to these indicies as Model Performance Criteria.\n\n----\n\n### the path to model performance criteria\n\n1.  establish a measurement scale for the distance from perfect accuracy\n    -   need to discuss information theory\n2.  establish *deviance* as an approximation of relative distance from accuracy\n3.  establish that we only care about *out-of-sample* deviance\n\nFirst: establishing a measurement scale. The two major dimensions to consider are:\n\n-   cost-benefit analysis\n\n    -   how much does it cost when we are wrong?\n\n    -   how much do we win when we're right?\n\n-   accuracy in context\n\n    -   judging accuracy in a way that accounts for how *much* a model could possibly improve prediction\n\n------------------------------------------------------------------------\n\n### clash of the weatherpeople\n\n| Day | Current Weatherman | New Weatherman | Outcome |\n|-----|:------------------:|:--------------:|:-------:|\n| 1   |        1.0         |      0.0       |  rain   |\n| 2   |        1.0         |      0.0       |  rain   |\n| 3   |        1.0         |      0.0       |  rain   |\n| 4   |        0.6         |      0.0       |   sun   |\n| 5   |        0.6         |      0.0       |   sun   |\n| 6   |        0.6         |      0.0       |   sun   |\n| 7   |        0.6         |      0.0       |   sun   |\n| 8   |        0.6         |      0.0       |   sun   |\n| 9   |        0.6         |      0.0       |   sun   |\n| 10  |        0.6         |      0.0       |   sun   |\n\n::: fragment\nIf accuracy is the chance of a correct prediction:\n\n$\\text{Current} = [(3 \\times 1) + (.4 \\times 7)]/10 = .58$\n\n$\\text{New} = [(3 \\times 0) + (1 \\times 7) ]/10= .70$\n:::\n\n------------------------------------------------------------------------\n\n### clash of the weatherpeople\n\n| Day | Current Weatherman | New Weatherman | Outcome |\n|-----|:------------------:|:--------------:|:-------:|\n| 1   |        1.0         |      0.0       |  rain   |\n| 2   |        1.0         |      0.0       |  rain   |\n| 3   |        1.0         |      0.0       |  rain   |\n| 4   |        0.6         |      0.0       |   sun   |\n| 5   |        0.6         |      0.0       |   sun   |\n| 6   |        0.6         |      0.0       |   sun   |\n| 7   |        0.6         |      0.0       |   sun   |\n| 8   |        0.6         |      0.0       |   sun   |\n| 9   |        0.6         |      0.0       |   sun   |\n| 10  |        0.6         |      0.0       |   sun   |\n\nBut what if you hate carrying an umbrella? Let's define the *cost* of getting wet as -5 happiness points and the cost of carrying an umbrella as -1 happiness points. Which weatherperson maximizes our happiness??\n\n$\\text{Current} = (3 \\times -1) + (7 \\times -0.6) = -7.2$\n\n$\\text{New} = (3 \\times -5) + (7 \\times 0) = -150$\n\n------------------------------------------------------------------------\n\n### clash of the weatherpeople\n\n| Day | Current Weatherman | New Weatherman | Outcome |\n|-----|:------------------:|:--------------:|:-------:|\n| 1   |        1.0         |      0.0       |  rain   |\n| 2   |        1.0         |      0.0       |  rain   |\n| 3   |        1.0         |      0.0       |  rain   |\n| 4   |        0.6         |      0.0       |   sun   |\n| 5   |        0.6         |      0.0       |   sun   |\n| 6   |        0.6         |      0.0       |   sun   |\n| 7   |        0.6         |      0.0       |   sun   |\n| 8   |        0.6         |      0.0       |   sun   |\n| 9   |        0.6         |      0.0       |   sun   |\n| 10  |        0.6         |      0.0       |   sun   |\n\nFinally, our previous measure of \"accuracy\" (we used a **HIT RATE** definition) is only one way to think about accuracy. What if accuracy is knowing the true data generating model? We might consider computing the probability of predicting the exact sequence of days (joint likelihood in Bayesian terms).\n\n$\\text{Current} = (1)^3 \\times (0.4)^7 \\approx .005$\n\n$\\text{New} = (0)^3 \\times (1)^7 = 0$\n\nThis method -- **LOG SCORING RULE**, or the logarithm of the joint probability -- is the way we'll think about accuracy in terms of model prediction.\n\n------------------------------------------------------------------------\n\n### information theory\n\nHow do we measure distance from perfect prediction? One important thing to keep in mind is that some targets are easier to hit than others. Therefore, the key to measuring distance is to ask, \"How much is our uncertainty reduced by learning an outcome?\" This reduction is formally referred to as **INFORMATION**.\n\nWe need to formalize our measure of uncertainty. This measurement should:\n\n1.  be continuous;\n2.  increase as the number of possible events increases; and\n3.  be additive.\n\nThis is satisfied by the **INFORMATION ENTROPY FUNCTION**. If there are $n$ different possible events and each event $i$ has probability $p_i$, and we call the list of probabilities $p$, then the unique measure of uncertainty we seek is[^1]:\n\n$$\nH(p) = - \\text{E log}(p_i) = - \\sum^n_{i=1}p_i\\text{log}(p_i)\n$$ \n\nIn words: The uncertainty contained in a probability distribution is the average log-probability of an event.\n\n[^1]: $E$ is mathematical notation for \"expected value\" or average. \n\n---\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture04-1_files/figure-revealjs/unnamed-chunk-10-1.png){width=1440}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\nConsider the weather forecast. When the day arrives, the weather is no longer uncertain.[^lecture04-1-1] There were 3 rainy days and 3 sunny days. Therefore:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- c( .3, .7 )\n-sum( p * log(p) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6108643\n```\n\n\n:::\n:::\n\n\n\nThis is our uncertainty.\n\nIf we lived in Abu Dhabi, we might have a different probability of rain (let's say .01), and therefore a different amount of uncertainty.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- c( .01, .99 )\n-sum( p * log(p) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05600153\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### from entropy to accuracy\n\nWe define **DIVERGENCE** to be the additional uncertainty induced by using probabilities from one distribution to describe another distribution. This is known as the Kullback-Leibler divergence or simply KL divergence.\n\nThe divergence between a target $(p)$ and a model $(q)$ is defined as:\n\n$$\nD_{KL}(p,q) = \\sum_ip_i(\\text{log}(p_i) - \\text{log}(q_i)) = \\sum_ip_i\\text{log}(\\frac{p_i}{q_i})\n$$ \n\nIn plainer language, the divergence is the average difference in log probability between the target and model, or the difference in entropies.\n\nSuppose, for example, the truth is that it rains 30% of the time. If our weatherman believes that it rains 25% of the time, how much additional uncertainty is introduced as a consequence of using the weatherman's prediction to approximate the true weather?\n\n$$\nD_{KL}(p,q) = \\sum_ip_i(\\text{log}(p_i)-\\text{log}(q_i)) = \\sum_ip_i\\text{log}(\\frac{p_i}{q_i})\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\np = c(.3, .7)\nq = c(.25, .75)\nsum(p*log(p/q))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.006401457\n```\n\n\n:::\n:::\n\n\n\nWhat is the divergence when we get the model exactly correct with the target?\n\n\n::: notes\ndifference = 0\n:::\n\n---\n\nAt this point, you're probably thinking, \"How can I put this into practice? Divergence is measuring distance from truth, but I don't know the truth.\" But we won't use divergence to estimate the difference from one model to truth; we're only interested in using it to compare two models (q and r) to each other. All we need to know are each model's average log probability: $\\text{E log}(q_i)$ and $\\text{E log}(r_i)$. \n\n\\begin{align*}\nD_{KL}(p,q) - D_{KL}(p,r) &= \\sum_ip_i(\\text{log}(p_i)-\\text{log}(q_i)) - \\sum_ip_i(\\text{log}(p_i)-\\text{log}(r_i))  \\\\\n&= \\sum_ip_i\\text{log}(p_i) - \\sum_ip_i\\text{log}(q_i) - \\sum_ip_i\\text{log}(p_i) + \\sum_ip_i\\text{log}(r_i) \\\\\n&= \\sum_ip_i\\text{log}(p_i) - \\sum_ip_i\\text{log}(p_i) - \\sum_ip_i\\text{log}(q_i) + \\sum_ip_i\\text{log}(r_i) \\\\\n&= \\sum_ip_i\\text{log}(r_i) - \\sum_ip_i\\text{log}(q_i) \\\\\n&= E[\\text{log}(r_i)] - E[\\text{log}(q_i)]\n\\end{align*}\n\nTo do so, we simply sum over all the observations, $i$, yielding a total score for each model:\n\n$$\nS(q) = \\sum_i\\text{log}(q_i) \n$$\n\n\n------------------------------------------------------------------------\n\nTo compute this score for a Bayesian model, we need to find the log of the average probability for each observation i, where the average is taken over the posterior distribution. The score is known as the **LOG-POINTWISE-PREDICTIVE-DENSITY**.\n\nLet's see an example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm7.1 <- quap(\n  alist(\n    brain_std ~ dnorm( mu , exp(log_sigma) ),\n    mu <- a + b*mass_std,\n    a ~ dnorm( 0.5 , 1 ),\n    b ~ dnorm( 0 , 10 ),\n    log_sigma ~ dnorm( 0 , 1 )\n  ), data=d )\n\nlppd(m7.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.6356172  0.6733377  0.5479152  0.6492467  0.4916121  0.4559589 -0.9150873\n```\n\n\n:::\n:::\n\n\n\nEach of these values is the log-probability score for a specific observation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(lppd(m7.1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.476713\n```\n\n\n:::\n:::\n\n\n\nThis is the total log-probability score for this model.\n\nSometimes, you'll see people report deviance, which is just this value multiplied by -2 (for historical reasons).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n-2*sum(lppd(m7.1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -4.96825\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\nOne issue with the log-probability score is that it always improves as the model gets more complex. One way to address this is by calculating the log-probability out-of-sample.\n\nWhen we usually have data and use it to fit a statistical model, the data comprise a **TRAINING SAMPLE**. Parameters are estimated from it, and then we can imagine using those estimates to predict outcomes in a new sample, called the **TEST SAMPLE**.\n\nUsing out-of-sample prediction doesn't change your model; rather, it changes our estimation of the predictive accuracy of our model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nN <- 20\nkseq <- 1:5\n# this code will take a long time to run. \ndev <- sapply( kseq, function(k){\n  print(k);\n  r <- mcreplicate( 1e4 , sim_train_test( N=N, k=k ) , mc.cores=4 );\n  ## if using PC, use:\n  #  r <- replicate(1e4, sim_train_test( N=N, k=k ) );\n  c( mean(r[1,]), mean(r[2,]), sd(r[1,]), sd(r[2,] ) )\n  } \n  )\nplot( 1:5 , dev[1,] , ylim=c( min(dev[1:2,])-5 , max(dev[1:2,])+10 ) ,\n      xlim=c(1,5.1) , xlab=\"number of parameters\" , ylab=\"deviance\" ,\n      pch=16 , col=rangi2 )\nmtext( concat( \"N = \",N ) )\npoints( (1:5)+0.1 , dev[2,] )\nfor ( i in kseq ) {\n  pts_in <- dev[1,i] + c(-1,+1)*dev[3,i]\n  pts_out <- dev[2,i] + c(-1,+1)*dev[4,i]\n  lines( c(i,i) , pts_in , col=rangi2 )\n  lines( c(i,i)+0.1 , pts_out )\n}\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture04-1_files/figure-revealjs/unnamed-chunk-18-1.png){width=150%}\n:::\n:::\n\n\n\n---\n\n\n### cross-validation\n\nOne strategy for estimating predictive accuracy is to actually test the model’s predictive accuracy on another sample. This is known as **CROSS-VALIDATION**, leaving out a small chunk of observations from our sample and evaluating the model on the observations that were left out. \n\nWe're not actually going to leave out data -- imagine! -- so instead we'll divide the data into chunks, or \"folds\". The model will then predict each fold after being trained on all the other folds. This is known as k-fold validation. \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create data for k-fold cross validation visualization\nk <- 5  # Number of folds\nblocks <- 5  # Number of data blocks per fold\nfold_data <- data.frame()\n\n# Generate data for each fold\nfor (i in 1:k) {\n  # For each fold, create a row with blocks indicating training or test\n  fold_row <- data.frame(\n    fold = paste(\"Fold\", i),\n    block = 1:blocks,\n    type = rep(\"Training\", blocks)\n  )\n  \n  # Set one block as test data (different block for each fold)\n  fold_row$type[i] <- \"Test\"\n  \n  # Add to the overall data\n  fold_data <- rbind(fold_data, fold_row)\n}\n\n# Convert fold to factor to preserve order\nfold_data$fold <- factor(fold_data$fold, levels = unique(fold_data$fold))\n\n# Create the visualization\nk_fold_plot <- ggplot(fold_data, aes(x = block, y = fold, fill = type)) +\n  geom_tile(color = \"white\", size = 0.5) +\n  scale_fill_manual(values = c(\"Training\" = \"#1c5253\", \"Test\" = \"#e07a5f\")) +\n  scale_y_discrete(breaks = NULL) +\n  labs(\n    #title = \"K-Fold Cross Validation (k=5)\",\n    fill = \"Data Usage\",\n    x = \"Data Blocks\",\n    y = \"\"\n  ) +\n  facet_wrap(fold ~ ., nrow = 5, scale = \"free\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\",\n    panel.grid = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n# Display the plot\nprint(k_fold_plot)\n```\n\n::: {.cell-output-display}\n![](lecture04-1_files/figure-revealjs/unnamed-chunk-19-1.png){width=200%}\n:::\n:::\n\n\n\nThe minimum number of folds is 2, and the maximum is your sample size. The latter is referred to as **LEAVE-ONE-OUT-CROSS-VALIDATION**, and this is extremely common. \n\n---\n\nThe problem with LOOCV is that it's computationally intensive. Luckily, there are some clever maths that we can use to approximate the score we would get from running the model over and over. One approach is to use the _importance_ (or weight) of each observation -- that is, how much does the prior distribution change if we were to remove this observation from our data? (Similar to influence and leverage.) Importantly, observations that are less likely are more important. \n\nWe can use importance in a strategy called **PARETO-SMOOTHED IMPORTANCE SAMPLING CROSS-VALIDATION** (PSIS) ([see this paper](https://arxiv.org/pdf/1507.02646)). The Pareto part is a smoothing technique that improves the reliability of the importance or weights. By assuming the weights follow a known distribution, the Pareto distribution, we can estimate a reliabile cross-validation score without doing the work of actually cross-validating. We'll get a PSIS score for each observation in our dataset, as well as a standard error for the score for the entire model. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrethinking::LOO(m7.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      PSIS      lppd  penalty  std_err\n1 8.647637 -4.323818 6.689529 10.02703\n```\n\n\n:::\n\n```{.r .cell-code}\nrethinking::LOO(m7.1, pointwise = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        PSIS        lppd   penalty  std_err         k\n1 -0.8044858  0.40224292 0.1968250 17.00526 0.8902547\n2 -0.9648498  0.48242492 0.1539313 17.00526 0.8651083\n3 -0.4916328  0.24581641 0.3006009 17.00526 0.7053234\n4 -0.9804244  0.49021222 0.1265089 17.00526 0.6986363\n5  0.1427753 -0.07138764 0.5340687 17.00526 1.1270328\n6  2.1805553 -1.09027766 1.5147064 17.00526 1.4783322\n7 16.5959309 -8.29796546 7.4679227 17.00526 2.2059108\n```\n\n\n:::\n:::\n\n\n\nThe value of PSIS is reliable when $k$ is less than .5, ok when $k$ is between .5 and 7., bad when $k$ is between .7 and 1, very bad when $k$ is greater than 1. \n\n---\n\nA second approach is to use the information criteria to computed the expected score out-of-sample. If you look back at the training/testing figure, you'll find that the difference between training deviance and testing deviance is almost exactly twice the number of parameters in the model (e.g., 2 for the first model with 1 parameter and about 10 for the last with 5 parameters). This is not random, but a known finding in machine learning. We can exploit this for simple estimates of out-of-sample deviance. \n\nA well-known estimate is the **AKAIKE INFORMATION CRITERION (AIC)**:\n\n\\begin{align*}\nAIC &= D_{\\text{train}} + 2p \\\\\n&= -2\\text{lppd} + 2p\n\\end{align*}\n\nwhere $D$ is the divergence and $p$ is the number of free parameters in the posterior distribution. As the 2 is just there for scaling, what AIC tells us is that the dimensionality of the posterior distribution is a natural measure of the model’s overfitting tendency. More complex models tend to overfit more, directly in proportion to the number of parameters.\n\nAIC isn't commonly used now. Its approximation is only reliable when:\n  \n  1. Priors are flat or overwhelmed by likelihood (data).\n  2. The posterior distribution is approximately multivariate Gaussian.\n  3. The sample size $N$ is much greater than the number of parameters $k$.\n  \nSimilarly the **DEVIANCE INFORMATION CRITERION (DIC)** doesn't assume flat priors but does make the other assumptions. \n  \n---\n\n### widely applicable\n\nWatanabe's **WIDELY APPLICABLE INFORMATION CRITERION (WAIC)** makes no assumption about the shape of the posterior. Its goal is to guess the out-of-sample KL divergence. In a large sample, the approximation converges to the cross-validation approximation, but in finite samples, there may be some disagreement. \n\nIts calculation is the log-posterior-predictive-density plus a penalty proportional to the variance in the posterior predictions:\n\n$$\n\\text{WAIC}(y, \\Theta) = -2(\\text{lppd} - \\sum_i\\text{var}_{\\theta}\\text{log}p(y_i|\\Theta))\n$$\nwhere $y$ is the observations and $\\Theta$ is the posterior distribution. The penalty term means, \"compute the variance in log-probabilities for each observation $i$, and then sum up these variances to get the total penalty.\"\n\nLike PSIS, WAIC is pointwise, meaning prediction is considered on a point-by-point basis. Therefore,\n\n  1. WAIC has an approximate standard error. \n  2. Some observations have stronger influence than others, and we can see this.\n  3. it can be hard to define for a single model. Consider for example a model in which each prediction depends upon a previous observation. This happens, for example, in a time series. In a time series, a previous observation becomes a predictor variable for the next observation. So it’s not easy to think of each observation as independent or exchangeable. In such a case, you can of course compute WAIC as if each observation were independent of the others, but it’s not clear what the resulting value means.\n  \n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrethinking::WAIC(m7.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      WAIC     lppd  penalty  std_err\n1 5.671047 2.506098 5.341621 8.872958\n```\n\n\n:::\n\n```{.r .cell-code}\nrethinking::WAIC(m7.1, pointwise = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        WAIC       lppd   penalty  std_err\n1 -0.5394958  0.6316904 0.3619425 10.35069\n2 -0.7213483  0.6623767 0.3017025 10.35069\n3 -0.4058836  0.5287496 0.3258078 10.35069\n4 -0.8083658  0.6494099 0.2452270 10.35069\n5 -0.3679046  0.4937326 0.3097803 10.35069\n6  0.1621830  0.4578703 0.5389618 10.35069\n7 10.7022119 -0.9167056 4.4344004 10.35069\n```\n\n\n:::\n:::\n\n\n\n---\n\n## comparing PSIS and WAIC\n\nPSIS and WAIC perform very similarly in the context of ordinary linear models. Of course, they may not when our posterior distributions start to get away from Gaussian or when there are highly influential observations. \n\nPSIS have higher variance as estimators of the KL divergence, while WAIC has greater bias. So we should expect each to be slightly better in different contexts. However, in practice any advantage may be much smaller than the expected error. Watanabe recommends computing both WAIC and PSIS and contrasting them. If there are large differences, this implies one or both criteria are unreliable.\n\nPSIS has a distinct advantage in warning the user about when it is unreliable. The $k$ values that PSIS computes for each observation indicate when the PSIS score may be unreliable, as well as identify which observations are at fault. We’ll see later how useful this can be.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare( m7.1, m7.6a, m7.6b , func=PSIS )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            PSIS          SE      dPSIS       dSE     pPSIS        weight\nm7.6a  -70.05584   0.6006271    0.00000        NA   4.50541  1.000000e+00\nm7.1    15.63870  16.6554328   85.69454  16.92875  10.27639  2.464148e-19\nm7.6b 1150.21023 494.3177444 1220.26607 494.36495 291.43595 1.053391e-265\n```\n\n\n:::\n\n```{.r .cell-code}\ncompare( m7.1, m7.6a, m7.6b , func=WAIC )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             WAIC          SE      dWAIC        dSE      pWAIC       weight\nm7.6a  -72.058757   0.3709652    0.00000         NA   3.477105 1.000000e+00\nm7.1     4.661435   8.0846102   76.72019   8.703671   4.814155 2.189888e-17\nm7.6b 1881.891116 788.7534092 1953.94987 852.034276 663.116993 0.000000e+00\n```\n\n\n:::\n\n```{.r .cell-code}\nplot( compare( m7.1, m7.6a, m7.6b) )\n```\n\n::: {.cell-output-display}\n![](lecture04-1_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n:::{.notes}\nThe filled points are the in-sample deviance values. The open points are the WAIC values. Notice that naturally each model does better in-sample than it is expected to do out-of-sample. The line segments show the standard error of each WAIC. These are the values in the column labeled SE in the table above. \n:::\n\n---\n\n### bic?\n\nThe **Bayesian Information Criterion (BIC)**, is frequently compared with the Akaike Information Criterion (AIC). It's important to understand that choosing between these criteria isn't fundamentally about adopting a Bayesian perspective. Both criteria can be derived through either Bayesian or non-Bayesian approaches, and strictly speaking, neither is purely Bayesian.\n\nBIC is mathematically connected to the logarithm of a linear model's average likelihood. In Bayesian statistics, this average likelihood serves as the denominator in Bayes' theorem—essentially the likelihood averaged across the prior distribution. Comparing average likelihoods has long been a standard method for model comparison in Bayesian analysis. These comparisons yield what we call \"Bayes factors\" when expressed as ratios. When transformed to a logarithmic scale, these ratios become differences, making them conceptually similar to comparing information criteria differences.\n\nSince the average likelihood incorporates the prior distribution, models with more parameters naturally incur a complexity penalty. This helps prevent overfitting, although the exact penalty mechanism differs from that of information criteria.\nMany Bayesian statisticians have reservations about Bayes factors, and all acknowledge certain technical challenges. One significant obstacle is computational difficulty—calculating average likelihood is often complex. Even when posterior distributions can be successfully computed, estimating average likelihood may remain problematic. Another issue is that while weak priors might minimally impact posterior distributions within individual models, they can dramatically influence comparisons between different models.\n\nThe choice between Bayesian and non-Bayesian approaches doesn't dictate whether to use information criteria or Bayes factors. In practice, there's value in using both methods and examining where they align and diverge. However, remember that both information criteria and Bayes factors are purely predictive tools that will readily select confounded models without considering causation.\n\n---\n\n### mid-lecture review\n\nRegularizing priors—priors which are skeptical of extreme parameter values—reduce fit to sample but tend to improve predictive accuracy.\n\nHow do we choose between several plausible models when seeking to maximize accuracy?\n\n  * Calculate a measure of information divergence.\n  * Comparing accuracy within-sample is bad: we'll always favor more complex models.\n  \nWe can estimate out-of-sample accuracy with any of a number of techniques, but most popularly: \n\n  * pareto-smoothed importance sampling cross-validation (PSIS)\n  * widely applicable information criteria (WAIC)\n  \nRegularization and predictive critera are complementary. \n\nLet's actually calculate some things. \n\n---\n\n### marriage again\n\nLet's revisit the divorce data from earlier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"WaffleDivorce\", package = \"rethinking\")\n\nd <- WaffleDivorce\n\nd$A <- standardize( d$MedianAgeMarriage )\nd$D <- standardize( d$Divorce )\nd$M <- standardize( d$Marriage )\n```\n:::\n\n\n\n\n### exercise\n\nFit 3 models:\n  1. Divorce rate predicted from median age at marriage.\n  2. Divorce rate predicted from marriage rate.\n  3. Divorce rate predicted from both median age and marriage rate.\n  \n---\n\n### solution\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm5.1 <- quap(\n  alist(\n    D ~ dnorm( mu , sigma ) ,\n    mu <- a + bA * A ,\n    a ~ dnorm( 0 , 0.2 ) ,\n    bA ~ dnorm( 0 , 0.5 ) ,\n    sigma ~ dexp( 1 )\n) , data = d )\n\nm5.2 <- quap(\n  alist(\n    D ~ dnorm( mu , sigma ) ,\n    mu <- a + bM * M ,\n    a ~ dnorm( 0 , 0.2 ) ,\n    bM ~ dnorm( 0 , 0.5 ) ,\n    sigma ~ dexp( 1 )\n) , data = d )\n\n\nm5.3 <- quap(\n  alist(\n    D ~ dnorm( mu , sigma ) ,\n    mu <- a + bM*M + bA*A ,\n    a ~ dnorm( 0 , 0.2 ) ,\n    bM ~ dnorm( 0 , 0.5 ) ,\n    bA ~ dnorm( 0 , 0.5 ) ,\n    sigma ~ dexp( 1 )\n) , data = d )\n```\n:::\n\n\n\n---\n\nNext, run the following code: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(24071847)\ncompare( m5.1 , m5.2 , m5.3 , func=PSIS )\n```\n:::\n\n\n\nWhat do you notice about:\n\n  1. The order in which the models are printed?\n  2. The warning messages?\n\n\n---\n\n### solution\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(24071847)\ncompare( m5.1 , m5.2 , m5.3 , func=PSIS )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         PSIS       SE     dPSIS       dSE    pPSIS       weight\nm5.1 127.5664 14.69456  0.000000        NA 4.671350 0.8340219483\nm5.3 130.8062 16.15737  3.239747  1.808892 6.578701 0.1650727106\nm5.2 141.2178 11.56626 13.651406 10.923960 4.057238 0.0009053411\n```\n\n\n:::\n:::\n\n\n\n### exercise\n\nUsing the full model (both age and marriage rate), ceate a figure that plots the Pareto $k$ estimate on the x-axis and the WAIC penalty for each observation in the dataset. Label any outliers on this plot. The functions `PSIS()` and `WAIC()` will be helpful here, but remember to check the documentation. \n\n---\n\n### solution\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(24071847)\nobs_PSIS = PSIS(m5.3, pointwise = TRUE)\nobs_WAIC = WAIC(m5.3, pointwise = TRUE)\n\npd <- data.frame(\n  state = d$Loc,\n  PSIS = obs_PSIS$k,\n  WAIC = obs_WAIC$WAIC) \n\npd %>% \n  ggplot(aes(x = PSIS, y = WAIC)) +\n  geom_point() +\n  geom_text(\n    data = subset(pd, PSIS > .45 & WAIC > 5),\n    aes(label = state), \n    nudge_x = .08\n  )\n```\n\n::: {.cell-output-display}\n![](lecture04-1_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n\nBased on this plot, which state(s) will you look more closely at? Can you use the data to speak to what may be happening here?\n\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% \n  mutate(col = ifelse(Loc %in% c(\"ID\", \"MN\"), \"1\", \"2\")) %>% \n  ggplot(aes(x = Marriage, y = Divorce)) +\n  geom_point(aes(color = col)) +\n  geom_smooth(method = \"lm\") +\n  geom_text(\n    data = subset(d, Loc %in% c(\"ID\", \"MN\")),\n    aes(label = Loc), \n    nudge_x = 1\n  ) +\n  scale_color_manual(values = c(\"#5e8485\", \"grey\")) +\n  guides(color=F)\n```\n\n::: {.cell-output-display}\n![](lecture04-1_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n\n\nPSIS tells us when the model is \"surprised\" by an observation. Gaussian distributions are unfortunately often surprised, as their tails are quite thin. \n\n\n:::{.notes}\nWhat is unusual about Idaho and Utah? Both of these States have large proportions of members of the Church of Jesus Christ of Latter-day Saints. Members of this church have low rates of divorce, wherever they live. \n\nBut note that other outliers don't have the same influence. \n:::\n\n\n\n\n\n\n[^lecture04-1-1]: Caveats about Oregon weather aside.\n\n",
    "supporting": [
      "lecture04-1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}