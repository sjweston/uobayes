{
  "hash": "51ac89cbfebcd8d9652ba4e8a49beaaa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"week 2: linear model and causal inference\"\nsubtitle: \"geocentric models\"\nformat: \n  revealjs:\n    css: xaringan-themer2.css\n    nature:\n      highlightStyle: solarized-dark\n      highlightLines: true\n      countIncrementalSlides: false\n      mathjax: \"default\"\n    self-contained: false  # Ensures correct embedding\n    embed-resources: true  # Embeds required assets\n    slide-number: true\nexecute:\n  echo: false        \n---\n\n\n\nWorkspace setup:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(rethinking)\nlibrary(patchwork)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n# Gaussian distributions\n\nAka, normal distributions.\n\nThese distributions are unimodal and symmetric. They tend to naturally occur. And they tend to be consistent with our assumptions. (e.g., measures are continuous values on a real number line, centered around a specific value).\n\nLater in the course, we'll move away from Gaussian distributions, but they're a useful place to start.\n\n------------------------------------------------------------------------\n\n## Recipes for models\n\n1.  Recognize a set of variables to work with. (Data and parameters.)\n2.  Define each variable either in terms of the other variables OR in terms of a probability distribution.\n3.  The combination of variables and their probability distributions defines a **joint generative model** that can be used to simulate hypothetical observations and analyze real ones.\n\nHere's an example:\n\n\\begin{align*}\ny_i &\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\\n\\mu_i &= \\beta x_i \\\\\n\\beta &\\sim \\text{Normal}(0,10) \\\\\n\\sigma &\\sim \\text{Exponential}(1) \\\\\nx_i &\\sim \\text{Normal}(0,1) \\\\\n\\end{align*}\n\n------------------------------------------------------------------------\n\n### Model for globe-tossing\n\nHere's the model for last week's globe-tossing experiment:\n\n\\begin{align*}\nW &\\sim \\text{Binomial}(N,p) \\\\\np &\\sim \\text{Uniform}(0,1) \\\\\n\\end{align*}\n\n:::{.fragment}\n\n-   $W$ is the observed count of water.\n-   $N$ is the total number of tosses.\n-   $p$ is the proportion of water on the globe.\n\n:::\n\n:::{.fragment}\n\nThe whole model can be read as:\n\n> The count $W$ is distributed binomially with sample size $N$ and probability $p$. The prior for $p$ is assumed to be uniform between 0 and 1.\n\n:::\n\n------------------------------------------------------------------------\n\n### Model for globe-tossing\n\nHere's the model for last week's globe-tossing experiment:\n\n\\begin{align*}\nW &\\sim \\text{Binomial}(N,p) \\\\\np &\\sim \\text{Uniform}(0,1) \\\\\n\\end{align*}\n\n#### Estimating the posterior\n\nWe can use grid approximation to estimate the posterior distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw <- 6; n <- 9\np_grid <- seq( from=0, to=1, length.out=100 )\nposterior <- dbinom( w,n,p_grid )*dunif( p_grid,0,1 )\nposterior <- posterior / sum(posterior)\n```\n:::\n\n\n\n------------------------------------------------------------------------\n\n::::: columns\n::: {.column width=\"80%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(p_grid, posterior, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-4-1.png){width=480}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"30%\"}\nLook familiar?\n:::\n:::::\n\n------------------------------------------------------------------------\n\n### Simulating parameters from the prior\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsims = 1e4\nsim_p <- runif( nsims, 0, 1)\ndens(sim_p)\n```\n\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### Simulating observations from the prior\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_w <- rbinom( nsims, 9, sim_p)\nsimplehist(sim_w)\n```\n\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\nSimulating from your priors -- **prior predictive simulation** -- is an essential part of modeling. This allows you to see what your choices imply about the data. You'll be able to diagnose bad choices.\n\n------------------------------------------------------------------------\n\n### an aside about learning in R\n\nAt this point in the course, I'm going to start throwing a lot of code at you. Do I expect you to memorize this code? Of course not. \n\nDo you need to understand every single thing that's happening in the code? Nope.\n\nBut, you'll learn a lot by taking the time to figure out what's happening in a code chunk. Class time will frequently include exercises where I ask you to adapt code I've shared in the slides to a new dataset or to answer a new problem. When doing so, go back through the old code and figure out what's going on. Run the code one line at a time. Always observe the output and take some time to look at the object that was created or modified. Here are some functions that will be extremely useful:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr() # what kind of object is this? what is its structure?\ndim() # what are the dimensions (rows/columns) of this object\nhead() # give me the first bit of this object\n```\n:::\n\n\n\n:::{.fragment}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(sim_w)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n int [1:10000] 8 4 3 1 1 4 2 3 2 1 ...\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(sim_w)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNULL\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(sim_w)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8 4 3 1 1 4\n```\n\n\n:::\n:::\n\n\n\n:::\n\n------------------------------------------------------------------------\n\n### exercise\n\nFor the model defined below, simulate observed $y$ values from the prior:\n\n\\begin{align*}\ny_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\n\n:::::{.fragment}\n### solution\n\n:::: columns\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(128); nsims <- 1e4\nsim_mu = rnorm( nsims, 0, 10)\nsim_sig = rexp( nsims, 1)\nsim_y = rnorm(nsims,sim_mu,sim_sig)\ndens(sim_y)\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n:::\n:::::\n:::::\n\n------------------------------------------------------------------------\n\n### exercise\n\nA sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.\n\n:::{.fragment}\nRemember that every student got taller each year. Does this change your choice of priors? How?\n:::\n\n:::{.fragment}\nThe variance in heights for students of the same age is never more than 2.10 feet. Does this lead you to revise your priors?\n:::\n\n::: notes\n\\begin{align*}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i \\\\\n\\beta_0 &\\sim \\mathcal{N}(5, 1^2) \\\\\n\\beta_1 &\\sim \\mathcal{N}(.1, .1^2) \\\\\n\\sigma &\\sim \\text{Half-Normal}(0, 10)\n\\end{align*}\n:::\n\n------------------------------------------------------------------------\n\n## An example: weight and height\n\nUsing the Howell data (make sure you have the `rethinking` package loaded).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Howell1\")\nd <- Howell1\nstr(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t544 obs. of  4 variables:\n $ height: num  152 140 137 157 145 ...\n $ weight: num  47.8 36.5 31.9 53 41.3 ...\n $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(measurements)\nd$height <- conv_unit(d$height, from = \"cm\", to = \"feet\")\nd$weight <- conv_unit(d$weight, from = \"kg\", to = \"lbs\")\nprecis(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             mean         sd      5.5%    94.5%      histogram\nheight  4.5362072  0.9055921  2.661042   5.4375      ▁▁▁▁▂▂▇▇▁\nweight 78.5079631 32.4502290 20.636856 120.1583 ▁▂▃▂▂▁▁▃▅▇▇▃▂▁\nage    29.3443934 20.7468882  1.000000  66.1350      ▇▅▅▃▅▂▂▁▁\nmale    0.4724265  0.4996986  0.000000   1.0000     ▇▁▁▁▁▁▁▁▁▇\n```\n\n\n:::\n\n```{.r .cell-code}\nd2 <- d[ d$age >= 18, ]\n```\n:::\n\n\n\n------------------------------------------------------------------------\n\n### exercise\n\nWrite a mathematical model for the weights in this data set. (Don't predict from other variables yet.) \n\nSimulate both your priors and the expected observed weight values from the prior.\n\n:::{.fragment}\n### solution\n\n\\begin{align*}\nw &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &\\sim \\text{Normal}(130, 20) \\\\\n\\sigma &\\sim \\text{Uniform}(0, 25) \\\\\n\\end{align*}\n:::\n\n------------------------------------------------------------------------\n\n### solution\n\nSimulate your priors\n\n\n\n::: {.cell out.weight='300px'}\n\n```{.r .cell-code}\nnsims <- 1e4 # number of simulations\nset.seed(128) # reproducibility\nsim_mu <- rnorm( nsims, 130, 20) # simulate values of mu\nsim_sig <- runif(nsims, 0, 25) # simulate values of sigma\npar(mfrow = c(1,3)) # plot display has 1 row, 3 columns\ndens_mu <- dens(sim_mu) # density of mu\ndens_sig <- dens(sim_sig) # density of sigma\ndens_both <- plot(sim_mu, sim_sig, cex = .5, pch = 16,\n                  col=col.alpha(\"#1c5253\",0.1) ) # both together\n```\n\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndev.off() # turn off display settings\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnull device \n          1 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\nSimulate values of weight.\n\n\n\n::: {.cell fig.weight='4'}\n\n```{.r .cell-code}\nsim_h <- rnorm( nsims, sim_mu, sim_sig)\ndens(sim_h)\n```\n\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n\n```{.r .cell-code}\nPI(sim_h, .89)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       5%       94% \n 91.11472 169.08230 \n```\n\n\n:::\n:::\n\n\n\n\n\n------------------------------------------------------------------------\n\nUse grid approximation to calculate posterior distribution.\n\n(Not necessary to copy, just for teaching purposes.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# values of mu and sigma to test\nmu.list <- seq( from=75, to=165, length.out=200 )\nsigma.list <- seq( from=0, to=25 , length.out=200 )\n# fit every possible combination of m and s\npost <- expand.grid( mu=mu.list , sigma=sigma.list )\n\n# calculate log-likelihood of weights for each combination of m and s\npost$LL <- sapply( 1:nrow(post) , function(i) sum(\n  dnorm( d2$weight , post$mu[i] , post$sigma[i] , log=TRUE ) ) )\n\n# add priors\npost$prod <- post$LL + \n  dnorm( post$mu , 130 , 20 , TRUE ) +\n  dunif( post$sigma , 0 , 25 , TRUE )\n\n# convert from LL to p\npost$prob <- exp( post$prod - max(post$prod) )\n```\n:::\n\n\n\n------------------------------------------------------------------------\n\n\n\n::: {.cell fig.weight='6'}\n\n```{.r .cell-code}\npost %>% \n  ggplot(aes(x = mu, y = sigma, color = prob)) +\n  geom_point() +\n  scale_color_gradient(low = \"white\", high = \"#1c5253\") +\n  theme_cowplot()\n```\n\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-15-1.png){width=576}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\nGo back to your code and change the range of values you estimate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# values of mu and sigma to test\nmu.list <- seq( from=90, to=105 , length.out=200 )\nsigma.list <- seq( from=12, to=16 , length.out=200 )\n```\n:::\n\n\n\nRerun all the earlier code.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n------------------------------------------------------------------------\n\n\n\n::: {.cell fig.weight='6'}\n\n```{.r .cell-code}\npost %>% \n  ggplot(aes(x = mu, y = sigma, color = prob)) +\n  geom_point() +\n  scale_color_gradient(low = \"white\", high = \"#1c5253\") +\n  theme_cowplot()\n```\n\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-18-1.png){width=576}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\nCool, but we said last week that grid approximation is unwieldy and going to quickly become unmanageable. So let's repeat this process with **quadratic approximation.**\n\nWe won't be calculating the probability or likelihood of values directly (too costly), but we can make some assumptions about the shapes of distributions and get an approximation of the shape of the posterior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflist <- alist(\n  weight ~ dnorm( mu , sigma ) ,\n  mu ~ dnorm( 130 , 20 ) ,\n  sigma ~ dunif( 0 , 25 )\n)\n\nm4.1 <- quap( flist , data=d2 )\nprecis( m4.1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          mean        sd     5.5%    94.5%\nmu    99.23233 0.7571453 98.02226 100.4424\nsigma 14.21535 0.5358183 13.35901  15.0717\n```\n\n\n:::\n:::\n\n\n\nThese numbers provide Gaussian approximations for each parameter’s **marginal distribution**. This means the plausibility of each value of $\\mu$, after averaging over the plausibilities of each value of $\\sigma$, is given by a Gaussian distribution with mean 99.23 and standard deviation 0.76.\n\n:::: notes\nThe function `alist()` does not evaluate the code, whereas the code `list()` does.\n\nOur interest in quadratic approximation, recall, is as a handy way to quickly make inferences about the shape of the posterior. The posterior’s peak will lie at the MAXIMUM A POSTERIORI estimate (MAP), and we can get a useful image of the posterior’s shape by using the quadratic approximation of the posterior distribution at this peak.\n\nTo build the quadratic approximation, we’ll use quap, a command in the rethinking package. The quap function works by using the model definition you were introduced to earlier in this chapter. Each line in the definition has a corresponding definition in the form of R code. The engine inside quap then uses these definitions to define the posterior probability at each combination of parameter values. Then it can climb the posterior distribution and find the peak, its MAP. Finally, it estimates the quadratic curvature at the MAP to produce an approximation of the posterior distribution. Remember: This procedure is very similar to what many non-Bayesian procedures do, just without any priors.\n\n:::\n\n------------------------------------------------------------------------\n\n`quap()` has approximated a **multivariate** Gaussian distribution -- more than one parameter, and these parameters may be related.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvcov( m4.1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               mu       sigma\nmu    0.573269024 0.001827111\nsigma 0.001827111 0.287101275\n```\n\n\n:::\n\n```{.r .cell-code}\ndiag( vcov( m4.1 ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       mu     sigma \n0.5732690 0.2871013 \n```\n\n\n:::\n\n```{.r .cell-code}\ncov2cor( vcov( m4.1 ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               mu       sigma\nmu    1.000000000 0.004503687\nsigma 0.004503687 1.000000000\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\nYou can extract samples.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost <- extract.samples( m4.1 , n=1e4 )\nhead(post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        mu    sigma\n1 99.90849 14.56967\n2 97.96623 14.57820\n3 99.60420 14.53269\n4 98.07316 14.33421\n5 99.98630 13.63512\n6 99.85855 13.48800\n```\n\n\n:::\n\n```{.r .cell-code}\nprecis(post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          mean        sd     5.5%     94.5%     histogram\nmu    99.21876 0.7632646 97.98375 100.43142 ▁▁▁▁▃▇▇▇▃▁▁▁▁\nsigma 14.21919 0.5420450 13.34974  15.08592     ▁▁▂▅▇▅▁▁▁\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### Adding in a linear component\n\nWe might assume that height and weight are associated with each other. Indeed, within our sample:\n\n\n\n::: {.cell fig.weight='5'}\n\n```{.r .cell-code}\nplot(d2$weight ~ d2$height)\n```\n\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### exercise\n\nUpdate your mathematical model to incorporate height. Simulate from your priors to see the implied regression lines.\n\n:::{.fragment}\n\\begin{align*}\nw_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta (h_i - \\bar{h}) \\\\\n\\alpha &\\sim \\text{Normal}(130, 20) \\\\\n\\beta &\\sim \\text{Normal}(0, 25) \\\\\n\\sigma &\\sim \\text{Uniform}(0, 25) \\\\\n\\end{align*}\n:::\n\n::: notes\n$=$ is deterministic -- once we know other variables, $\\mu_i$ is known with certainty\n\nmade-up parameters are the targets of learning\n:::\n\n------------------------------------------------------------------------\n\nTo simulate from our priors:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate 100 lines\nnsims <- 100\nsim_alpha = rnorm(nsims, 130, 20)\nsim_beta = rnorm(nsims, 0, 25)\n\n# calculate height\nxbar = mean(d2$height)\n# plot with nothing in it\nplot(NULL, xlim = range(d2$height), ylim = c(50, 175),\n     xlab = \"height\", ylab = \"weight\")\nabline(h = 0, lty = 2) #line at 0\n#plot each line\nfor(i in 1:nsims){\n curve(sim_alpha[i] +sim_beta[i]*(x-xbar), \n       add = T,\n       col=col.alpha(\"#1c5253\",0.4))  \n}\n```\n:::\n\n\n\n------------------------------------------------------------------------\n\n::::: columns\n::: {.column width=\"80%\"}\n\n\n\n::: {.cell fig.weight='7'}\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"20%\"}\n\nDescribe in words what's wrong with our priors.\n\n:::{.fragment}\nSlope should not be negative. How can we fix this?\n:::\n:::{.fragment}\nCould use a uniform distribution bounded by 0.\n:::\n\n:::\n::::\n\n------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate 100 lines\nnsims <- 100\nsim_alpha = rnorm(nsims, 130, 20)\nsim_beta = runif(nsims, 0, 50)\n\n# calculate height\nxbar = mean(d2$height)\n# plot with nothing in it\nplot(NULL, xlim = range(d2$height), ylim = c(50, 175),\n     xlab = \"height\", ylab = \"weight\")\nabline(h = 0, lty = 2) #line at 0\n#plot each line\nfor(i in 1:nsims){\n curve(sim_alpha[i] +sim_beta[i]*(x-xbar), \n       add = T,\n       col=col.alpha(\"#1c5253\",0.4))  \n}\n```\n:::\n\n\n\n------------------------------------------------------------------------\n\n\n\n::: {.cell fig.weight='7'}\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### exercise\n\nFit the new weight model using the quadratic approximation.\n\n:::{.fragment}\n\n### solution\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflist <- alist(\n  weight ~ dnorm( mu , sigma ) ,\n  mu <- a + b*(height - mean(height)),\n  a ~ dnorm( 130 , 20 ) ,\n  b ~ dunif(0, 50), \n  sigma ~ dunif( 0 , 25 )\n)\n\nm2 <- quap( flist , data=d2 )\nprecis( m2 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           mean        sd      5.5%     94.5%\na     99.206172 0.4968604 98.412093 100.00025\nb     42.295334 1.9594204 39.163801  45.42687\nsigma  9.324766 0.3514366  8.763103   9.88643\n```\n\n\n:::\n:::\n\n\n:::\n------------------------------------------------------------------------\n\n### exercise\n\nDraw lines from the posterior distribution and plot with the data.\n\n:::{.fragment}\n### solution\n\n\n\n::: {.cell fig.weight='3'}\n\n```{.r .cell-code}\nsample_post = extract.samples(m2, n = 100)\nplot(d2$height, d2$weight, cex = .5, pch = 16,\n     xlim = range(d2$height), \n     xlab = \"height\", ylab = \"weight\")\n#plot each line\nfor(i in 1:nrow(sample_post)){\n curve(sample_post$a[i] +sample_post$b[i]*(x-xbar), \n       add = T,\n       col=col.alpha(\"#1c5253\",0.1))  \n}\n```\n:::\n\n\n:::\n\n------------------------------------------------------------------------\n\n\n\n::: {.cell fig.weight='8'}\n::: {.cell-output-display}\n![](lecture02-1_files/figure-revealjs/unnamed-chunk-26-1.png){width=768}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\nA side note: a major concern or critique of Bayesian analysis is that the subjectivity of the priors allow for nefarious behavior. \"Putting our thumbs on the scale,\" so to speak. But priors are quickly overwhelmed by data. Case in point:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nflist <- alist(\n  weight ~ dnorm( mu , sigma ) ,\n  mu <- a + b*(height - mean(height)),\n  a ~ dnorm( 130 , 20 ) ,\n  b ~ dnorm(-5, 20),  \n  sigma ~ dunif( 0 , 50 )\n)\n\nm2 <- quap( flist , data=d2 )\nprecis( m2 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           mean        sd      5.5%      94.5%\na     99.206057 0.4968976 98.411919 100.000196\nb     41.845822 1.9505166 38.728519  44.963124\nsigma  9.325467 0.3515152  8.763678   9.887256\n```\n\n\n:::\n:::\n\n\n\nYou'll only really get into trouble with uniform priors that have a boundary, if true population parameter is outside your boundary. A good rule of thumb is to avoid the uniform distribution. We'll cover other options for priors for $\\sigma$ in future lectures, but as a preview, the exponential function works very well for this!",
    "supporting": [
      "lecture02-1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}