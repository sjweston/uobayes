{
  "hash": "b384d2e466f5feebca45ca5196dda653",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Problem set 5\"\ndate: \"2025-05-05\"\n---\n\n\n\n\n\n## Instructions\n\nPlease use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas. \n\n## Questions\n\nThe data contained in `library(MASS)`;`data(eagles)` are records of salmon pirating attempts by Bald Eagles in Washington State. See `?eagles` for details. While one eagle feeds, sometimes another will swoop in and try to steal the salmon from it. Call the feeding eagle the \"victim\" and the thief the \"pirate.\" Use the available data to build a binomial GLM of successful pirating attempts.\n\n1. Consider the following model:\n\n\\begin{align*}\ny_i &\\sim \\text{Binomial}(n_i, p_i) \\\\\n\\text{logit}(p_i) &= \\alpha + \\beta_PP_i + \\beta_VV_i + \\beta_AA_i \\\\\n\\alpha &\\sim \\text{Normal}(0, 1.5) \\\\\n\\beta_P,\\beta_V,\\beta_A &\\sim \\text{Normal}(0, 0.5)\n\\end{align*}\n\nwhere $y$ is the number of successful attempts, $n$ is the total number of attempts, $P$ is a dummy variable indicating whether or not the pirate had large body size, $V$ is a dummy variable indicating whether or not the victim had large body size, and finally $A$ is a dummy variable indicating whether or not the pirate was an adult. Fit the model above to the `eagles` data. \n\n<details>\n<summary>Click to see the answer</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(brms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Rcpp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading 'brms' package (version 2.22.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'brms'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    ar\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidybayes)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'tidybayes'\n\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nhere() starts at /Users/sweston2/Library/CloudStorage/GoogleDrive-weston.sara@gmail.com/My Drive/Work (google drive)/teaching/uobayes\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(cowplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:patchwork':\n\n    align_plots\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n```\n\n\n:::\n\n```{.r .cell-code}\ntheme_set(theme_cowplot())\n\n# Load and examine the data\ndata(eagles, package = \"MASS\")\nd <- eagles\nrethinking::precis(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean        sd  5.5%  94.5%     histogram\ny 12.5 10.663690 0.385 25.535        ▇▁▂▇▁▂\nn 20.0  8.831761 7.080 28.615 ▃▁▁▃▁▃▁▃▁▃▁▇▃\nP  NaN        NA    NA     NA              \nA  NaN        NA    NA     NA              \nV  NaN        NA    NA     NA              \n```\n\n\n:::\n\n```{.r .cell-code}\nd %>% count(P, V, A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  P V A n\n1 L L A 1\n2 L L I 1\n3 L S A 1\n4 L S I 1\n5 S L A 1\n6 S L I 1\n7 S S A 1\n8 S S I 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit the model using brms\nm1 <- brm(\n  data = d,\n  family = binomial,\n  y | trials(n) ~ P + V + A,\n  prior = c(\n    prior(normal(0, 1.5), class = Intercept),\n    prior(normal(0, 0.5), class = b)\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  file = here(\"files/models/hw5.1\")\n)\n\n# Display model summary\nsummary(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: binomial \n  Links: mu = logit \nFormula: y | trials(n) ~ P + V + A \n   Data: eagles (Number of observations: 8) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.90      0.31     0.31     1.53 1.00     4342     3112\nPS           -1.64      0.31    -2.24    -1.04 1.00     3734     2888\nVS            1.70      0.32     1.09     2.35 1.00     4071     3253\nAI           -0.65      0.32    -1.27    -0.04 1.00     3746     3038\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n\n</details>\n\n2. Interpret the estimates and plot the posterior distributions. Compute and display both (1) the predicted probability of success and its 89% interval for each row ($i$) in the data, as well as (2) the predicted success count and its 89% interval. What different information does each type of posterior prediction provide?\n\n<details>\n<summary>Click to see the answer</summary>\n\nHere' are the summaries and plots of the coefficient estimates. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Plot posterior distributions of coefficients\nm1 %>% \n  gather_draws(`b_.*`, regex = T) %>% # this will get any variable that starts with b_\n  median_qi(.width = .89)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 7\n  .variable   .value .lower .upper .width .point .interval\n  <chr>        <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 b_AI        -0.652 -1.15  -0.152   0.89 median qi       \n2 b_Intercept  0.893  0.408  1.41    0.89 median qi       \n3 b_PS        -1.63  -2.14  -1.15    0.89 median qi       \n4 b_VS         1.70   1.19   2.23    0.89 median qi       \n```\n\n\n:::\n\n```{.r .cell-code}\nm1 %>% \n  gather_draws(`b_.*`, regex = T) %>% \n  ggplot( aes( x=.value, y=.variable ) ) +\n  stat_halfeye() +\n  geom_vline(aes(xintercept=0), linetype=\"dashed\") + \n  labs(x = \"Coefficient Est\", y=NULL)\n```\n\n::: {.cell-output-display}\n![](05-problem-set_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nBut as we know, in a GLM, these are difficult to interpret. So let's translate these into probabilites. First, let's ask how the different coefficients change our probabilities of success. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnd = distinct(eagles, V, P, A) %>% \n  mutate(n = 1e5)\n# expected probs\npost_epred = nd %>% \n  add_epred_draws(m1) %>% \n  ungroup() %>% \n  mutate(prob = .epred/n) %>% \n  dplyr::select(V, P, A, .draw, prob) \n\np1 = post_epred%>% \n  pivot_wider(names_from = P, values_from = prob) %>% \n  mutate(diff = L-S) %>% \n  ggplot(aes( x=diff )) +\n  stat_halfeye( fill = \"#1c5253\") +\n  geom_vline(aes(xintercept = 0), linetype =\"dashed\") +\n  labs(x = \"Difference in probability\", y = NULL, title = \"Pirate (Large-Small)\")\n\np2 = post_epred%>% \n  pivot_wider(names_from = V, values_from = prob) %>% \n  mutate(diff = L-S) %>% \n  ggplot(aes( x=diff )) +\n  stat_halfeye( fill = \"#e07a5f\") +\n  geom_vline(aes(xintercept = 0), linetype =\"dashed\") +\n  labs(x = \"Difference in probability\", y = NULL, title = \"Victim (Large-Small)\")\n\np3 = post_epred%>% \n  pivot_wider(names_from = A, values_from = prob) %>% \n  mutate(diff = A-I) %>% \n  ggplot(aes( x=diff )) +\n  stat_halfeye( fill = \"#3d405b\") +\n  geom_vline(aes(xintercept = 0), linetype =\"dashed\") +\n  labs(x = \"Difference in probability\", y = NULL, title = \"Pirate (Adult-Infant)\")\n\np1 + p2 + p3\n```\n\n::: {.cell-output-display}\n![](05-problem-set_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nGreat, these make more sense! Large pirates more successfully steal salmon; smaller victims are more likely to be stolen from; and adult pirates are more successful than infant ones. \n\nLet's get the probabilities and counts for each bird in our sample. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepred_draws(m1, newdata = d) %>% \n  ungroup() %>% \n  mutate(prob = .epred/n) %>% \n  ggplot(aes( y=prob, x=.row)) +\n  stat_halfeye() +\n  labs(x = \"row\", y = \"probability\")\n```\n\n::: {.cell-output-display}\n![](05-problem-set_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepred_draws(m1, newdata = d) %>% \n  ungroup() %>% \n  ggplot(aes( y=.epred, x=.row)) +\n  stat_halfeye() +\n  labs(x = \"row\", y = \"counts\")\n```\n\n::: {.cell-output-display}\n![](05-problem-set_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nThe posterior predictions provide two different types of information:\n\n1. Predicted probabilities :\n   - Shows the estimated probability of success for each combination of predictors\n   - Useful for understanding the relative impact of each predictor\n   - Ranges from 0 to 1\n\n2. Predicted counts (post_counts):\n   - Shows the expected number of successful attempts\n   - Takes into account both the probability and the number of trials\n   - More directly interpretable in terms of actual outcomes\n\nThe key difference is that probabilities show the underlying success rate, while counts show the expected number of successes given the number of attempts. The count predictions are more variable because they incorporate both the uncertainty in the probability and the randomness of the binomial process.\n\n</details>\n\n3. Now try to improve the model. Consider an interaction between the pirate's size and age (immature or adult). Compare this model to the previous one, using WAIC. Interpret.\n\n<details>\n<summary>Click to see the answer</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fit the interaction model\nm2 <- brm(\n  data = d,\n  family = binomial,\n  y | trials(n) ~ P + V + A + P:A,\n  prior = c(\n    prior(normal(0, 1.5), class = Intercept),\n    prior(normal(0, 0.5), class = b)\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  file = here(\"files/models/hw5.2\")\n)\n\n# Compare models using WAIC\nwaic(m1, m2) %>% print(simplify=F)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: \n6 (75.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: \n5 (62.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOutput of model 'm1':\n\nComputed from 4000 by 8 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic    -29.5  6.2\np_waic         8.4  3.0\nwaic          59.1 12.3\n\n6 (75.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\nOutput of model 'm2':\n\nComputed from 4000 by 8 log-likelihood matrix.\n\n          Estimate  SE\nelpd_waic    -25.5 4.1\np_waic         7.1 2.3\nwaic          51.1 8.3\n\n5 (62.5%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\nModel comparisons:\n   elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  se_waic\nm2   0.0       0.0   -25.5       4.1          7.1    2.3      51.1   8.3  \nm1  -4.0       3.7   -29.5       6.2          8.4    3.0      59.1  12.3  \n```\n\n\n:::\n\n```{.r .cell-code}\n# Display model summary\nsummary(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: binomial \n  Links: mu = logit \nFormula: y | trials(n) ~ P + V + A + P:A \n   Data: d (Number of observations: 8) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.85      0.31     0.24     1.45 1.00     5145     3487\nPS           -1.31      0.33    -1.95    -0.67 1.00     4704     3575\nVS            1.65      0.33     1.02     2.32 1.00     4301     2981\nAI           -0.40      0.33    -1.04     0.24 1.00     4663     3161\nPS:AI        -1.11      0.40    -1.89    -0.33 1.00     4404     3423\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nThe interaction model adds a term P:A to capture how the effect of pirate size (P) varies with age (A). This allows us to test whether the advantage of being a large pirate differs between adult and immature eagles. \n\nThe WAIC comparison helps us determine if the more complex model (with interaction) is justified by the data. A lower WAIC indicates better predictive performance, but we should also consider:\n1. The magnitude of the WAIC difference\n2. The standard error of the difference\n3. The complexity of the model\n\nIn this case, we improve our prediction a marginal amount -- (difference of 4.0 with a standard error of 3.7). In fact, we even have a less complex model (our `p_waic` is lower for model 2). This can happen when your added parameters capture an important pattern in the data. \n\nLet's see what we've learned from this model.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnd = distinct(eagles, V, P, A) %>% \n  mutate(n = 1e5)\n# expected probs\nnd %>% \n  add_epred_draws(m2) %>% \n  ungroup() %>% \n  mutate(prob = .epred/n) %>% \n  dplyr::select(V, P, A, .draw, prob) %>% \n  pivot_wider(names_from = \"V\", \n              names_prefix = \"V\", \n              values_from = prob) %>% \n  mutate(avg = (VL+VS)/2) %>% \n  ggplot(aes( x = avg, y = P, fill = A)) +\n  stat_halfeye(alpha=.7) +\n  scale_fill_manual(values = c(\"#1c5253\" , \"#e07a5f\")) \n```\n\n::: {.cell-output-display}\n![](05-problem-set_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nWhen pirates are large, they are successful regardless of whether they are adults or infants. However, when pirates are small, they are moderately successulf when they are adults, but much less successful when they are infants. \n\n</details>\n",
    "supporting": [
      "05-problem-set_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}