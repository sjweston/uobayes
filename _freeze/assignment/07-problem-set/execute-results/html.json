{
  "hash": "f14bae5648057a0912303462d67e52f5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Problem set 7\"\ndate: \"2025-05-19\"\n---\n\n\n\n\n\n\n## Instructions\n\nPlease use an RMarkdown file to complete this assignment. Make sure you reserve code chunks for code and write out any interpretations or explainations outside of code chunks. Submit the knitted PDF file containing your code and written answers on Canvas. \n\n## Questions\n\n1. Return to `data(Trolley)`. Define and fit a varying intercepts model for these data. Cluster intercepts on individual participants, as indicated by the unique values in the `id` variable. Include `action`, `intention`, and `contact` as ordinary terms. Compare the varying intercepts model and a model that ignores individuals, using both WAIC and posterior predictions. What is the impact of individual variation in these data?\n\n<details>\n<summary>Click to see the answer</summary>\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the data\ndata(Trolley, package=\"rethinking\")\nd <- Trolley\n```\n:::\n\n\n\n\nFirst, let's fit a model that ignores individual variation.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1 <- brm(\n  data = d,\n  family = cumulative,\n  response ~ action + intention + contact,\n  prior = c(\n    prior(normal(0, 1), class = Intercept),\n    prior(normal(0, 0.5), class = b)\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 1,\n  file=here(\"files/models/hw7.1\")\n)\n```\n:::\n\n\n\n\nNow fit the varying intercepts model.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm2 <- brm(\n  data = Trolley,\n  family = cumulative,\n  response ~ action + intention + contact + (1 | id),\n  prior = c(\n    prior(normal(0, 1), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sd)\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 2,\n  file=here(\"files/models/hw7.2\")\n)\n```\n:::\n\n\n\n\nWe'll compare these models using WAIC:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwaic(m1, m2) %>% print(simplify=F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOutput of model 'm1':\n\nComputed from 4000 by 9930 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic -18545.0 37.9\np_waic         9.0  0.0\nwaic       37089.9 75.8\n\nOutput of model 'm2':\n\nComputed from 4000 by 9930 log-likelihood matrix.\n\n          Estimate    SE\nelpd_waic -15670.4  88.6\np_waic       354.9   4.6\nwaic       31340.9 177.2\n\n1 (0.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\nModel comparisons:\n   elpd_diff se_diff  elpd_waic se_elpd_waic p_waic   se_p_waic waic    \nm2      0.0       0.0 -15670.4      88.6        354.9      4.6   31340.9\nm1  -2874.5      86.0 -18545.0      37.9          9.0      0.0   37089.9\n   se_waic \nm2    177.2\nm1     75.8\n```\n\n\n:::\n:::\n\n\n\n\nThe varying-intercepts model has better out-of-sample prediction (a very large `elpd_diff` compared to the `se_diff`), despite being a much more complex model (see `p_waic`).\n\nLet's compare the coefficients:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef_m1 = gather_draws(m1, b_action, b_intention, b_contact) %>% \n  mutate(model= \"m1\")\n\ncoef_m2 = gather_draws(m2, b_action, b_intention, b_contact) %>% \n  mutate(model= \"m2\")\n\nfull_join(coef_m1, coef_m2) %>% \n  ggplot(aes( y=.variable, x=.value, fill=model)) +\n  stat_halfeye() +\n  geom_vline(aes(xintercept = 0), linetype=\"dashed\")\n```\n\n::: {.cell-output-display}\n![](07-problem-set_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nOur coefficients are much larger in this second model. In other words, allowing for a varying-intercept has cleaned up some of the noise around these predictions, allowing us to see more clearly a structure. \n\nNow let's compare the posterior predictions.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnd = distinct(d, action, intention, contact) \n\npred_m1 = nd %>% add_predicted_draws(m1) %>% mutate(model = \"pooled\")\npred_m2 = nd %>% add_predicted_draws(m2, re_formula = NA) %>% mutate(model = \"varying intercepts\")\n\npredicted = full_join(pred_m1, pred_m2) %>% \n  count(model, action, intention, contact, .prediction) %>% \n  with_groups(model, mutate, prob=n/sum(n))\n\nobs = d %>% \n  count(action, intention, contact, response) %>% \n  mutate(prob=n/sum(n),\n         model=\"observed\",\n         response=as.factor(response)) %>% \n  rename(.prediction=response)\n\npredicted %>% \n  full_join(obs) %>% \n  mutate(\n    action=ifelse(action==0, \"no action\", \"action\"),\n    contact=ifelse(contact==0, \"no contact\", \"contact\"),\n    intention=ifelse(intention==0, \"no intention\", \"intention\"),\n  ) %>% \n  ggplot(aes(x=.prediction, y=prob, color=model)) +\n  geom_point(aes(shape=model)) +\n  geom_line(aes(group=model, linetype=model)) +\n  scale_color_manual(values = c(\"#e07a5f\", \"#5e8485\" , \"#0f393a\")) + \n  facet_wrap(intention~action+contact) +\n  theme(legend.position = \"top\") +\n  labs(x=\"response\",y=NULL)\n```\n\n::: {.cell-output-display}\n![](07-problem-set_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nIn general, the two models don't vary much in their predictions (note how small the probabilities are on the y-axis), nor is one of them consistently much closer to the observed data. It's not that the varying-intercepts model is going to better estimate the average response (which is what these plots are showing). Rather, it will better represent individual variability.\n\n</details>\n\n<details>\n<summary>Click to see the answer</summary>\n\n2. The `Trolley` data are also clustered by `story`, which indicates a unique narrative for each vignette. Define and fit a cross-classified varying intercepts model with both `id` and `story.` Use the same ordinary terms as in the previous problem. Compare this model to the previous models. What do you infer about the impact of different stories on responses?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fit cross-classified model\nm3 <- brm(\n  data = Trolley,\n  family = cumulative,\n  response ~ action + intention + contact + (1 | id) + (1 | story),\n  prior = c(\n    prior(normal(0, 1), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sd)\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 3,\n  file=here(\"files/models/hw7.3\")\n)\n```\n:::\n\n\n\n\nCompare all three models using WAIC\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwaic(m1, m2, m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOutput of model 'm1':\n\nComputed from 4000 by 9930 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic -18545.0 37.9\np_waic         9.0  0.0\nwaic       37089.9 75.8\n\nOutput of model 'm2':\n\nComputed from 4000 by 9930 log-likelihood matrix.\n\n          Estimate    SE\nelpd_waic -15670.4  88.6\np_waic       354.9   4.6\nwaic       31340.9 177.2\n\n1 (0.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\nOutput of model 'm3':\n\nComputed from 4000 by 9930 log-likelihood matrix.\n\n          Estimate    SE\nelpd_waic -15345.3  89.5\np_waic       364.2   4.7\nwaic       30690.5 178.9\n\n1 (0.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\nModel comparisons:\n   elpd_diff se_diff\nm3     0.0       0.0\nm2  -325.2      23.9\nm1 -3199.7      87.2\n```\n\n\n:::\n:::\n\n\n\n\nGet varying effects for stories\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstory_effects <- m3 %>%\n  spread_draws(r_story[story,]) %>%\n  mean_qi()\n\n# Plot story effects\nstory_effects %>%\n  ggplot(aes(x = reorder(story, r_story), y = r_story)) +\n  geom_pointrange(aes(ymin = .lower, ymax = .upper)) +\n  coord_flip() +\n  labs(x = \"Story\", y = \"Varying effect\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](07-problem-set_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nThe cross-classified model (m3) shows the best fit according to WAIC, indicating that both individual variation and story-specific effects are important. The story effects plot shows that different narratives have varying impacts on responses, with some stories eliciting more extreme responses than others. This suggests that the specific details of each trolley scenario influence how people respond, beyond just the action, intention, and contact variables.\n\nThe comparison between models shows that:\n1. The basic model (m1) performs worst, as it ignores all clustering\n2. The varying intercepts model (m2) improves fit by accounting for individual differences\n3. The cross-classified model (m3) performs best by accounting for both individual differences and story-specific effects\n\nThis indicates that both individual variation and story-specific effects are important factors in understanding responses to trolley scenarios.\n\n</details>\n",
    "supporting": [
      "07-problem-set_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}