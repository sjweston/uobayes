---
title: "week 2: linear model and causal inference"
subtitle: "geocentric models"
format: 
  revealjs:
    css: xaringan-themer2.css
    nature:
      highlightStyle: solarized-dark
      highlightLines: true
      countIncrementalSlides: false
      mathjax: "default"
    self-contained: false  # Ensures correct embedding
    embed-resources: true  # Embeds required assets
    slide-number: true
    code-annotations: hover
execute:
  echo: false        
---

Workspace setup:

```{r, results='hide', echo =T, message = F, warning = F}
library(here)
library(tidyverse)
library(cowplot)
library(brms)
library(tidybayes)
library(patchwork)
```

```{r}
# here's a bunch of stuff I include at the beginning of each qmd file. 
# it's mostly setting the color palette for figures, but it also controls the default code chunk option (showing you, i.e., echo) and setting the figure quality to ensure that it looks nice 
knitr::opts_chunk$set(fig.retina=3, echo=TRUE)
theme_set(theme_cowplot())
default_palettes <- list(
  c("#5e8485" , "#0f393a") ,
  c("#1c5253" , "#5e8485" , "#0f393a") , 
  # palette with 5 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" ) ,
  # same palette interpolated to 8 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" , "#a7a844" , "#69306d" ) 
  
)

options(ggplot2.discrete.fill = default_palettes, 
        ggplot2.discrete.colour = default_palettes)
```

------------------------------------------------------------------------

## Model "recipes"

1.  Recognize a set of variables to work with. (Data and parameters.)
2.  Define each variable either in terms of the other variables OR in terms of a probability distribution.
3.  The combination of variables and their probability distributions defines a **joint generative model** that can be used to simulate hypothetical observations and analyze real ones.

Here's an example:

\begin{align*}
y_i &\sim \text{Normal}(\mu_i,\sigma) \\
\mu_i &= \beta x_i \\
\beta &\sim \text{Normal}(0,10) \\
\sigma &\sim \text{Exponential}(1) \\
x_i &\sim \text{Normal}(0,1) \\
\end{align*}

------------------------------------------------------------------------

### Model for globe-tossing

Here's the model for last week's globe-tossing experiment:

\begin{align*}
W &\sim \text{Binomial}(N,p) \\
p &\sim \text{Uniform}(0,1) \\
\end{align*}

::: fragment
-   $W$ is the observed count of water.
-   $N$ is the total number of tosses.
-   $p$ is the proportion of water on the globe.
:::

::: fragment
The whole model can be read as:

> The count $W$ is distributed binomially with sample size $N$ and probability $p$. The prior for $p$ is assumed to be uniform between 0 and 1.
:::

------------------------------------------------------------------------

### Model for globe-tossing

Here's the model for last week's globe-tossing experiment:

\begin{align*}
W &\sim \text{Binomial}(N,p) \\
p &\sim \text{Uniform}(0,1) \\
\end{align*}

#### Estimating the posterior using `brms`

Last week, we used grid approximation to estimate the posterior distribution. In the video you watched for today, McElreath moves on to something called **QUADRATIC APPROXIMATION**. It's good to understand what that's doing, but you and I are moving right along to MCMC. We won't go into the details of what's happening for a few weeks, but let's start with the code to estimate the model.

Let's say we tossed the globe 9 times and observed 6 waters:

```{r, warning=F, message=F,results='hide'}
m1 <-
  brm(data = list(w = 6),                            # <1>
      family = binomial(link = "identity"),          # <2>
      w | trials(9) ~ 0 + Intercept,                 # <3>
      prior(beta(1, 1), class = b, lb = 0, ub = 1),  # <4>
      iter = 5000, warmup = 1000, seed = 3, chains=1,          # <5>
      file = here("files/models/m21.1"))# <6>
```

1.  Data can be a data frame or a list. Just make sure the variable names match what's in your formula.
2.  How you assume your **outcome** variable is distributed.
3.  The formula for your outcome.
4.  Priors for *every* parameter in your model. Here, we only have one parameter, so we only need 1 prior. This happens to be a flat prior between 0 and 1.
5.  Some choices about how we want our model to run. We'll go more into this later.
6.  These models can take a long time to run. You can automatically save the output to a file; when you do this, the next time you run this code, it won't actually estimate the model, but will instead pull the output from your stated file. Be WARNED: if you change the data or the model code, it will NOT restimate your model until you delete the file.

------------------------------------------------------------------------

### Sampling from the posterior

Grid approximation gave us the calculated probability of each possible value of our parameter, $p$. But our method of conducting bayes will no longer give us such a neat solution. Here's how you get the posterior distribution for $p$:

```{r}
samples_from_post = as_draws_df(m1)
samples_from_post
```

------------------------------------------------------------------------

```{r}
samples_from_post %>%  
  ggplot(aes(x=b_Intercept)) +
  geom_density(fill = "grey", color = "white") +
  labs(x="Proportion water")
```

------------------------------------------------------------------------

### Posterior predictive distribution

```{r}
ppd = posterior_predict(m1)
ppd
```

------------------------------------------------------------------------

```{r}
data.frame(obs = ppd) %>% 
  ggplot(aes(x=obs)) +
  geom_histogram() +
  labs(x="Observed water (out of 9)")
```

------------------------------------------------------------------------

### Sampling parameters from the **prior**

Oops, we've jumped ahead of ourselves! Best practice is to simulate values from your prior **first** and check to see if those priors are reasonable.

```{r, message=F,warning=F,results='hide'}
#| code-line-numbers: "7"
m1p <-
  brm(data = list(w = 6),                            
      family = binomial(link = "identity"),          
      w | trials(9) ~ 0 + Intercept,                 
      prior(beta(1, 1), class = b, lb = 0, ub = 1),  
      iter = 5000, warmup = 1000, seed = 3, chains=1,          
      sample_prior = "only")
```

::::: columns
::: {.column width="50%"}
```{r prior-samples, eval=F}
samples_from_prior = as_draws_df(m1p)
samples_from_prior %>% 
  ggplot(aes(x=b_Intercept)) +
  geom_density(fill = "grey", color = "white") +
  labs(x="Proportion water", title="Prior")
```
:::

::: {.column width="50%"}
```{r, ref.label="prior-samples", echo=F}
```
:::
:::::





------------------------------------------------------------------------

### Simulating observations from the prior

We may also want the **PRIOR PREDICTIVE DISTRIBUTION** which is the expected observiations given our prior.

```{r}
prior_pd = posterior_predict(m1p)
data.frame(obs = prior_pd) %>% 
  ggplot(aes(x=obs)) +
  geom_histogram() +
  labs(x="Observed water (out of 9)")
```

Simulating from your priors -- **prior predictive simulation** -- is an essential part of modeling. This allows you to see what your choices imply about the data. You'll be able to diagnose bad choices.

------------------------------------------------------------------------

### an aside about learning in R

At this point in the course, I'm going to start throwing a lot of code at you. Do I expect you to memorize this code? Of course not.

Do you need to understand every single thing that's happening in the code? Nope.

But, you'll learn a lot by taking the time to figure out what's happening in a code chunk. Class time will frequently include exercises where I ask you to adapt code I've shared in the slides to a new dataset or to answer a new problem. When doing so, go back through the old code and figure out what's going on. Run the code one line at a time. Always observe the output and take some time to look at the object that was created or modified. Here are some functions that will be extremely useful:

```{r, eval = F}
str() # what kind of object is this? what is its structure?
dim() # what are the dimensions (rows/columns) of this object
head() # give me the first bit of this object
```

::: fragment
```{r}
str(prior_pd)
dim(prior_pd)
head(prior_pd)
```
:::

------------------------------------------------------------------------

### Continous outcomes

The globe tossing example is cute and easy to work with, but let's move towards the kinds of variables we more frequently work with. Let's create a model for some outcome, $y$ that is continuous.

\begin{align*}
y_i &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(0, 10) \\
\sigma &\sim \text{Uniform}(0, 5)
\end{align*}

```{r, warning=F, message=F,results='hide'}
set.seed(9)
y = rnorm(n = 31, mean = 4, sd = .5)
m2 = brm(
  data = list(y=y),
  family = gaussian,
  y ~ 1,
  prior = c(prior( normal(0,10), class=Intercept),
            prior( uniform(0,5), class=sigma)),  
      iter = 5000, warmup = 1000, seed = 3, chains=1,
  file = here("files/models/m21.2")
)
```

------------------------------------------------------------------------

## An example: weight and height

Using the Howell data (don't load the `rethinking` package because it can interfere with `brms`).

```{r}
data("Howell1", package = "rethinking")
d <- Howell1
str(d)
library(measurements)
d$height <- conv_unit(d$height, from = "cm", to = "feet")
d$weight <- conv_unit(d$weight, from = "kg", to = "lbs")
rethinking::precis(d)

d2 <- d[ d$age >= 18, ]
```

------------------------------------------------------------------------

### exercise

Write a mathematical model for the weights in this data set. (Don't worry about predicting from other variables yet.)

::: fragment
### solution

\begin{align*}
w &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(130, 20) \\
\sigma &\sim \text{Uniform}(0, 25) \\
\end{align*}
:::

------------------------------------------------------------------------

\begin{align*}
w &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(130, 20) \\
\sigma &\sim \text{Uniform}(0, 25) \\
\end{align*}

### exercise

Simulate from your priors (parameters values and prior predictive values).

::: fragment
### solution

Sample from your priors:

```{r, fig.retina=3, out.weight='300px', message=F, warning=F,results='hide'}
m3p = brm(
  data = d2,
  family = gaussian,
  weight ~ 1,
  prior = c(prior( normal(130,20), class=Intercept),
            prior( uniform(0,25), class=sigma, lb=0, ub=25)),  
      iter = 5000, warmup = 1000, seed = 3, chains=1,
  sample_prior = "only")

```
:::

------------------------------------------------------------------------

Sampling parameter estimates for the Intercept.

```{r}
pairs(m3p)
```

This is a different (shorter) way to plot your posterior. Good things: it automatically includes the scatterplot so you can see the implications of how these parameters correlate. Bad things: not customizable and not useable when you have a lot of parameters.

------------------------------------------------------------------------

Simulate values of weight.

```{r, fig.retina=3, fig.weight=4}
prior_pd = posterior_predict(m3p)
dim(prior_pd)
as.data.frame(prior_pd) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x=value)) +
  geom_histogram() +
  labs(x="Expected observed weights (based on prior)")
```

------------------------------------------------------------------------

Another shorter way:

```{r}
pp_check(m3p)
```

------------------------------------------------------------------------

### Fit the model

```{r, warning=F, message=F,results='hide'}
m3 = brm(
  data = d2,
  family = gaussian,
  weight ~ 1,
  prior = c(prior( normal(130,20), class=Intercept),
            prior( uniform(0,25), class=sigma, lb=0, ub=25)),  
      iter = 5000, warmup = 1000, seed = 3, chains=1,
  file = here("files/models/m21.3"))
```

```{r}
posterior_summary(m3)
```

------------------------------------------------------------------------

```{r}
pairs(m3)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
posterior_predict(m3) %>% 
  as.data.frame() %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x=value)) +
  geom_density(fill = "grey", color = "white") +
  geom_density( aes(x = weight), data=d2, inherit.aes = F) 
```

------------------------------------------------------------------------

```{r}
pp_check(m3)
```

------------------------------------------------------------------------

## Adding in a linear component

We might assume that height and weight are associated with each other. Indeed, within our sample:

```{r, fig.weight =5}
plot(d2$weight ~ d2$height)
```

------------------------------------------------------------------------

### exercise

Update your mathematical model to incorporate height.

::: fragment
\begin{align*}
w_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta h_i \\
\alpha &\sim \text{Normal}(??, ??) \\
\beta &\sim \text{Normal}(0, 25) \\
\sigma &\sim \text{Uniform}(0, 25) \\
\end{align*}
:::

::: notes
$=$ is deterministic -- once we know other variables, $\mu_i$ is known with certainty

made-up parameters are the targets of learning
:::

------------------------------------------------------------------------

### exercise

Update your mathematical model to incorporate height.

\begin{align*}
w_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta (h_i - \bar{h}) \\
\alpha &\sim \text{Normal}(130, 20) \\
\beta &\sim \text{Normal}(0, 25) \\
\sigma &\sim \text{Uniform}(0, 25) \\
\end{align*}

::: notes
$=$ is deterministic -- once we know other variables, $\mu_i$ is known with certainty

made-up parameters are the targets of learning
:::

------------------------------------------------------------------------

\begin{align*}
w_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta (h_i - \bar{h}) \\
\alpha &\sim \text{Normal}(130, 20) \\
\beta &\sim \text{Normal}(0, 25) \\
\sigma &\sim \text{Uniform}(0, 25) \\
\end{align*}

To update our brms code:

```{r message=F, warning=F, results='hide'}
d2$height_c = d2$height - mean(d2$height)
  
m4p = brm(
  data = d2,
  family = gaussian,
  weight ~ 1 + height_c,
  prior = c(prior( normal(130,20), class=Intercept),
            prior( normal(0,25),   class=b),
            prior( uniform(0,25),  class=sigma, lb=0, ub=25)),  
      iter = 5000, warmup = 1000, seed = 3, chains=1,
  sample_prior = "only")

```

------------------------------------------------------------------------

```{r}
set.seed(9)
samples_from_prior = as_draws_df(m4p)
str(samples_from_prior)
```

------------------------------------------------------------------------


```{r}
#| code-fold: true

d2 %>% 
  ggplot(aes(x=height_c, y=weight)) +
  geom_blank() +
  geom_abline( aes(intercept=b_Intercept, slope=b_height_c), 
               data=samples_from_prior[1:50, ],
               alpha=.3) +
  scale_x_continuous(name = "height(feet)", 
                     breaks=seq(4,6,by=.5)-mean(d2$height),
                     labels=seq(4,6,by=.5))
```

Describe in words what's wrong with our priors.

::: fragment
Slope should not be negative. How can we fix this?
:::

::: fragment
Could use a uniform distribution bounded by 0.
:::

------------------------------------------------------------------------

```{r warning=FALSE, message=F, results='hide'}
m4p = brm(
  data = d2,
  family = gaussian,
  weight ~ 1 + height_c,
  prior = c(prior( normal(130,20), class=Intercept),
            prior( uniform(0,25),   class=b),
            prior( uniform(0,25),  class=sigma, lb=0, ub=25)),  
      iter = 5000, warmup = 1000, seed = 3, chains=1,
  sample_prior = "only")

```

------------------------------------------------------------------------

```{r, echo = F}

set.seed(9)
samples_from_prior = as_draws_df(m4p)

d2 %>% 
  ggplot(aes(x=height_c, y=weight)) +
  geom_blank() +
  geom_abline( aes(intercept=b_Intercept, slope=b_height_c), 
               data=samples_from_prior[1:50, ],
               alpha=.5,
               color="#1c5253") +
  scale_x_continuous(name = "height(feet)", 
                     breaks=seq(4,6,by=.5)-mean(d2$height),
                     labels=seq(4,6,by=.5))
```

------------------------------------------------------------------------

### exercise

Fit the new weight model to the data.

::: fragment
### solution

```{r, warning=F, message=F}
m4 = brm(
  data = d2,
  family = gaussian,
  weight ~ 1 + height_c,
  prior = c(prior( normal(130,20), class=Intercept),
            prior( uniform(0,25),   class=b),
            prior( uniform(0,25),  class=sigma, lb=0, ub=25)),  
      iter = 5000, warmup = 1000, seed = 3, chains=1,
  file=here("files/models/m21.4"))
posterior_summary(m4)
```
:::

------------------------------------------------------------------------

### exercise

Draw lines from the posterior distribution and plot with the data.

::: fragment

### solution

```{r sim-post, fig.weight = 3}
#| code-fold: true
#| 
set.seed(9)
samples_from_posterior = as_draws_df(m4)
d2%>% 
  ggplot(aes(x=height_c, y=weight)) +
  geom_point(size=.5) +
  geom_abline( aes(intercept=b_Intercept, slope=b_height_c), 
               data=samples_from_posterior[1:50, ],
               alpha=.3,
               color="#1c5253") +
  scale_x_continuous(name = "height(feet)", 
                     breaks=seq(4,6,by=.5)-mean(d2$height),
                     labels=seq(4,6,by=.5))
```
:::


------------------------------------------------------------------------

A side note: a major concern or critique of Bayesian analysis is that the subjectivity of the priors allow for nefarious behavior. "Putting our thumbs on the scale," so to speak. But priors are quickly overwhelmed by data. Case in point:

```{r, warning=F, message=F}
#| code-line-numbers: "6"
m4e = brm(
  data = d2,
  family = gaussian,
  weight ~ 1 + height_c,
  prior = c(prior( normal(130,20), class=Intercept),
            prior( normal(-5,5),   class=b),
            prior( uniform(0,25),  class=sigma, lb=0, ub=25)),  
      iter = 5000, warmup = 1000, seed = 3, chains=1,
  file=here("files/models/m21.4e"))
posterior_summary(m4e)
```

You'll only really get into trouble with uniform priors that have a boundary, if true population parameter is outside your boundary. A good rule of thumb is to avoid the uniform distribution. We'll cover other options for priors for $\sigma$ in future lectures, but as a preview, the exponential distribution works very well for this! 
