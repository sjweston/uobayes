---
title: "week 2: linear model and causal inference"
subtitle: "geocentric models"
format: 
  revealjs:
    css: xaringan-themer2.css
    nature:
      highlightStyle: solarized-dark
      highlightLines: true
      countIncrementalSlides: false
      mathjax: "default"
    self-contained: false  # Ensures correct embedding
    embed-resources: true  # Embeds required assets
    slide-number: true
execute:
  echo: false        
---

Workspace setup:

```{r, results='hide', echo =T, message = F, warning = F}
library(tidyverse)
library(cowplot)
library(rethinking)
library(patchwork)
```

```{r, echo = F}
knitr::opts_chunk$set(fig.retina=3, echo=TRUE)
theme_set(theme_cowplot())
default_palettes <- list(
  c("#5e8485" , "#0f393a") ,
  c("#1c5253" , "#5e8485" , "#0f393a") , 
  # palette with 5 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" ) ,
  # same palette interpolated to 8 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" , "#a7a844" , "#69306d" ) 
  
)

options(ggplot2.discrete.fill = default_palettes, 
        ggplot2.discrete.colour = default_palettes)
```

# Gaussian distributions

Aka, normal distributions.

These distributions are unimodal and symmetric. They tend to naturally occur. And they tend to be consistent with our assumptions. (e.g., measures are continuous values on a real number line, centered around a specific value).

Later in the course, we'll move away from Gaussian distributions, but they're a useful place to start.

------------------------------------------------------------------------

## Recipes for models

1.  Recognize a set of variables to work with. (Data and parameters.)
2.  Define each variable either in terms of the other variables OR in terms of a probability distribution.
3.  The combination of variables and their probability distributions defines a **joint generative model** that can be used to simulate hypothetical observations and analyze real ones.

Here's an example:

\begin{align*}
y_i &\sim \text{Normal}(\mu_i,\sigma) \\
\mu_i &= \beta x_i \\
\beta &\sim \text{Normal}(0,10) \\
\sigma &\sim \text{Exponential}(1) \\
x_i &\sim \text{Normal}(0,1) \\
\end{align*}

------------------------------------------------------------------------

### Model for globe-tossing

Here's the model for last week's globe-tossing experiment:

\begin{align*}
W &\sim \text{Binomial}(N,p) \\
p &\sim \text{Uniform}(0,1) \\
\end{align*}

:::{.fragment}

-   $W$ is the observed count of water.
-   $N$ is the total number of tosses.
-   $p$ is the proportion of water on the globe.

:::

:::{.fragment}

The whole model can be read as:

> The count $W$ is distributed binomially with sample size $N$ and probability $p$. The prior for $p$ is assumed to be uniform between 0 and 1.

:::

------------------------------------------------------------------------

### Model for globe-tossing

Here's the model for last week's globe-tossing experiment:

\begin{align*}
W &\sim \text{Binomial}(N,p) \\
p &\sim \text{Uniform}(0,1) \\
\end{align*}

#### Estimating the posterior

We can use grid approximation to estimate the posterior distribution.

```{r}
w <- 6; n <- 9
p_grid <- seq( from=0, to=1, length.out=100 )
posterior <- dbinom( w,n,p_grid )*dunif( p_grid,0,1 )
posterior <- posterior / sum(posterior)
```

------------------------------------------------------------------------

::::: columns
::: {.column width="80%"}
```{r, fig.retina = 3, fig.height = 3, fig.width = 5}
plot(p_grid, posterior, type = "l")
```
:::

::: {.column width="30%"}
Look familiar?
:::
:::::

------------------------------------------------------------------------

### Simulating parameters from the prior

```{r, fig.height = 5}
nsims = 1e4
sim_p <- runif( nsims, 0, 1)
dens(sim_p)
```

------------------------------------------------------------------------

### Simulating observations from the prior

```{r, fig.retina = 3, fig.height = 5}
sim_w <- rbinom( nsims, 9, sim_p)
simplehist(sim_w)
```

Simulating from your priors -- **prior predictive simulation** -- is an essential part of modeling. This allows you to see what your choices imply about the data. You'll be able to diagnose bad choices.

------------------------------------------------------------------------

### an aside about learning in R

At this point in the course, I'm going to start throwing a lot of code at you. Do I expect you to memorize this code? Of course not. 

Do you need to understand every single thing that's happening in the code? Nope.

But, you'll learn a lot by taking the time to figure out what's happening in a code chunk. Class time will frequently include exercises where I ask you to adapt code I've shared in the slides to a new dataset or to answer a new problem. When doing so, go back through the old code and figure out what's going on. Run the code one line at a time. Always observe the output and take some time to look at the object that was created or modified. Here are some functions that will be extremely useful:

```{r, eval = F}
str() # what kind of object is this? what is its structure?
dim() # what are the dimensions (rows/columns) of this object
head() # give me the first bit of this object
```

:::{.fragment}

```{r}
str(sim_w)
dim(sim_w)
head(sim_w)
```

:::

------------------------------------------------------------------------

### exercise

For the model defined below, simulate observed $y$ values from the prior:

\begin{align*}
y_i &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(0, 10) \\
\sigma &\sim \text{Exponential}(1)
\end{align*}

:::::{.fragment}
### solution

:::: columns

::: {.column width="50%"}
```{r ex1-hist, eval = F}
set.seed(128); nsims <- 1e4
sim_mu = rnorm( nsims, 0, 10)
sim_sig = rexp( nsims, 1)
sim_y = rnorm(nsims,sim_mu,sim_sig)
dens(sim_y)
```
:::

::: {.column width="50%"}
```{r ref.label="ex1-hist", echo = F, fig.retina =3}
dens(sim_y)
```
:::
:::::
:::::

------------------------------------------------------------------------

### exercise

A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.

:::{.fragment}
Remember that every student got taller each year. Does this change your choice of priors? How?
:::

:::{.fragment}
The variance in heights for students of the same age is never more than 2.10 feet. Does this lead you to revise your priors?
:::

::: notes
\begin{align*}
y_i &\sim \mathcal{N}(\mu_i, \sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_i \\
\beta_0 &\sim \mathcal{N}(5, 1^2) \\
\beta_1 &\sim \mathcal{N}(.1, .1^2) \\
\sigma &\sim \text{Half-Normal}(0, 10)
\end{align*}
:::

------------------------------------------------------------------------

## An example: weight and height

Using the Howell data (make sure you have the `rethinking` package loaded).

```{r}
data("Howell1")
d <- Howell1
str(d)
library(measurements)
d$height <- conv_unit(d$height, from = "cm", to = "feet")
d$weight <- conv_unit(d$weight, from = "kg", to = "lbs")
precis(d)

d2 <- d[ d$age >= 18, ]
```

------------------------------------------------------------------------

### exercise

Write a mathematical model for the weights in this data set. (Don't predict from other variables yet.) 

Simulate both your priors and the expected observed weight values from the prior.

:::{.fragment}
### solution

\begin{align*}
w &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(130, 20) \\
\sigma &\sim \text{Uniform}(0, 25) \\
\end{align*}
:::

------------------------------------------------------------------------

### solution

Simulate your priors

```{r, fig.retina=3, out.weight='300px'}
nsims <- 1e4 # number of simulations
set.seed(128) # reproducibility
sim_mu <- rnorm( nsims, 130, 20) # simulate values of mu
sim_sig <- runif(nsims, 0, 25) # simulate values of sigma
par(mfrow = c(1,3)) # plot display has 1 row, 3 columns
dens_mu <- dens(sim_mu) # density of mu
dens_sig <- dens(sim_sig) # density of sigma
dens_both <- plot(sim_mu, sim_sig, cex = .5, pch = 16,
                  col=col.alpha("#1c5253",0.1) ) # both together
```

```{r}
dev.off() # turn off display settings
```

------------------------------------------------------------------------

Simulate values of weight.

```{r, fig.retina=3, fig.weight=4}
sim_h <- rnorm( nsims, sim_mu, sim_sig)
dens(sim_h)
PI(sim_h, .89)
```



------------------------------------------------------------------------

Use grid approximation to calculate posterior distribution.

(Not necessary to copy, just for teaching purposes.)

```{r}
# values of mu and sigma to test
mu.list <- seq( from=75, to=165, length.out=200 )
sigma.list <- seq( from=0, to=25 , length.out=200 )
# fit every possible combination of m and s
post <- expand.grid( mu=mu.list , sigma=sigma.list )

# calculate log-likelihood of weights for each combination of m and s
post$LL <- sapply( 1:nrow(post) , function(i) sum(
  dnorm( d2$weight , post$mu[i] , post$sigma[i] , log=TRUE ) ) )

# add priors
post$prod <- post$LL + 
  dnorm( post$mu , 130 , 20 , TRUE ) +
  dunif( post$sigma , 0 , 25 , TRUE )

# convert from LL to p
post$prob <- exp( post$prod - max(post$prod) )
```

------------------------------------------------------------------------

```{r, fig.retina = 3, fig.weight=6, fig.width=6}
post %>% 
  ggplot(aes(x = mu, y = sigma, color = prob)) +
  geom_point() +
  scale_color_gradient(low = "white", high = "#1c5253") +
  theme_cowplot()
```

------------------------------------------------------------------------

Go back to your code and change the range of values you estimate.

```{r}
# values of mu and sigma to test
mu.list <- seq( from=90, to=105 , length.out=200 )
sigma.list <- seq( from=12, to=16 , length.out=200 )
```

Rerun all the earlier code.

```{r, echo = F}
# fit every possible combination of m and s
post <- expand.grid( mu=mu.list , sigma=sigma.list )

# calculate log-likelihood of weights for each combination of m and s
post$LL <- sapply( 1:nrow(post) , function(i) sum(
  dnorm( d2$weight , post$mu[i] , post$sigma[i] , log=TRUE ) ) )

# add priors
post$prod <- post$LL + 
  dnorm( post$mu , 130 , 20 , TRUE ) +
  dunif( post$sigma , 0 , 25 , TRUE )

# convert from LL to p
post$prob <- exp( post$prod - max(post$prod) )
```

------------------------------------------------------------------------

```{r, fig.retina = 3, fig.weight=6, fig.width=6}
post %>% 
  ggplot(aes(x = mu, y = sigma, color = prob)) +
  geom_point() +
  scale_color_gradient(low = "white", high = "#1c5253") +
  theme_cowplot()
```

------------------------------------------------------------------------

Cool, but we said last week that grid approximation is unwieldy and going to quickly become unmanageable. So let's repeat this process with **quadratic approximation.**

We won't be calculating the probability or likelihood of values directly (too costly), but we can make some assumptions about the shapes of distributions and get an approximation of the shape of the posterior.

```{r}
flist <- alist(
  weight ~ dnorm( mu , sigma ) ,
  mu ~ dnorm( 130 , 20 ) ,
  sigma ~ dunif( 0 , 25 )
)

m4.1 <- quap( flist , data=d2 )
precis( m4.1 )
```

These numbers provide Gaussian approximations for each parameter’s **marginal distribution**. This means the plausibility of each value of $\mu$, after averaging over the plausibilities of each value of $\sigma$, is given by a Gaussian distribution with mean `r round(precis( m4.1 )$mean[1], 2)` and standard deviation `r round(precis( m4.1 )$sd[1], 2)`.

:::: notes
The function `alist()` does not evaluate the code, whereas the code `list()` does.

Our interest in quadratic approximation, recall, is as a handy way to quickly make inferences about the shape of the posterior. The posterior’s peak will lie at the MAXIMUM A POSTERIORI estimate (MAP), and we can get a useful image of the posterior’s shape by using the quadratic approximation of the posterior distribution at this peak.

To build the quadratic approximation, we’ll use quap, a command in the rethinking package. The quap function works by using the model definition you were introduced to earlier in this chapter. Each line in the definition has a corresponding definition in the form of R code. The engine inside quap then uses these definitions to define the posterior probability at each combination of parameter values. Then it can climb the posterior distribution and find the peak, its MAP. Finally, it estimates the quadratic curvature at the MAP to produce an approximation of the posterior distribution. Remember: This procedure is very similar to what many non-Bayesian procedures do, just without any priors.

:::

------------------------------------------------------------------------

`quap()` has approximated a **multivariate** Gaussian distribution -- more than one parameter, and these parameters may be related.

```{r}
vcov( m4.1 )
diag( vcov( m4.1 ) )
cov2cor( vcov( m4.1 ) )
```

------------------------------------------------------------------------

You can extract samples.

```{r}
post <- extract.samples( m4.1 , n=1e4 )
head(post)
precis(post)
```

------------------------------------------------------------------------

### Adding in a linear component

We might assume that height and weight are associated with each other. Indeed, within our sample:

```{r, fig.weight =5}
plot(d2$weight ~ d2$height)
```

------------------------------------------------------------------------

### exercise

Update your mathematical model to incorporate height. Simulate from your priors to see the implied regression lines.

:::{.fragment}
\begin{align*}
w_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta (h_i - \bar{h}) \\
\alpha &\sim \text{Normal}(130, 20) \\
\beta &\sim \text{Normal}(0, 25) \\
\sigma &\sim \text{Uniform}(0, 25) \\
\end{align*}
:::

::: notes
$=$ is deterministic -- once we know other variables, $\mu_i$ is known with certainty

made-up parameters are the targets of learning
:::

------------------------------------------------------------------------

To simulate from our priors:

```{r sim-lines, eval = F}
# simulate 100 lines
nsims <- 100
sim_alpha = rnorm(nsims, 130, 20)
sim_beta = rnorm(nsims, 0, 25)

# calculate height
xbar = mean(d2$height)
# plot with nothing in it
plot(NULL, xlim = range(d2$height), ylim = c(50, 175),
     xlab = "height", ylab = "weight")
abline(h = 0, lty = 2) #line at 0
#plot each line
for(i in 1:nsims){
 curve(sim_alpha[i] +sim_beta[i]*(x-xbar), 
       add = T,
       col=col.alpha("#1c5253",0.4))  
}
```

------------------------------------------------------------------------

::::: columns
::: {.column width="80%"}

```{r ref.label="sim-lines", fig.weight=7, fig.width=10, echo = F}

```

:::

::: {.column width="20%"}

Describe in words what's wrong with our priors.

:::{.fragment}
Slope should not be negative. How can we fix this?
:::
:::{.fragment}
Could use a uniform distribution bounded by 0.
:::

:::
::::

------------------------------------------------------------------------

```{r sim-lines2, eval = F}
# simulate 100 lines
nsims <- 100
sim_alpha = rnorm(nsims, 130, 20)
sim_beta = runif(nsims, 0, 50)

# calculate height
xbar = mean(d2$height)
# plot with nothing in it
plot(NULL, xlim = range(d2$height), ylim = c(50, 175),
     xlab = "height", ylab = "weight")
abline(h = 0, lty = 2) #line at 0
#plot each line
for(i in 1:nsims){
 curve(sim_alpha[i] +sim_beta[i]*(x-xbar), 
       add = T,
       col=col.alpha("#1c5253",0.4))  
}
```

------------------------------------------------------------------------

```{r ref.label="sim-lines2", fig.weight=7, fig.width=10, echo = F}

```

------------------------------------------------------------------------

### exercise

Fit the new weight model using the quadratic approximation.

:::{.fragment}

### solution

```{r}
flist <- alist(
  weight ~ dnorm( mu , sigma ) ,
  mu <- a + b*(height - mean(height)),
  a ~ dnorm( 130 , 20 ) ,
  b ~ dunif(0, 50), 
  sigma ~ dunif( 0 , 25 )
)

m2 <- quap( flist , data=d2 )
precis( m2 )
```
:::
------------------------------------------------------------------------

### exercise

Draw lines from the posterior distribution and plot with the data.

:::{.fragment}
### solution

```{r sim-post, fig.weight = 3, eval = F}
sample_post = extract.samples(m2, n = 100)
plot(d2$height, d2$weight, cex = .5, pch = 16,
     xlim = range(d2$height), 
     xlab = "height", ylab = "weight")
#plot each line
for(i in 1:nrow(sample_post)){
 curve(sample_post$a[i] +sample_post$b[i]*(x-xbar), 
       add = T,
       col=col.alpha("#1c5253",0.1))  
}
```
:::

------------------------------------------------------------------------

```{r ref.label = "sim-post", fig.weight = 8, fig.width = 8, echo = F}

```

------------------------------------------------------------------------

A side note: a major concern or critique of Bayesian analysis is that the subjectivity of the priors allow for nefarious behavior. "Putting our thumbs on the scale," so to speak. But priors are quickly overwhelmed by data. Case in point:

```{r}
#| code-line-numbers: "5"
flist <- alist(
  weight ~ dnorm( mu , sigma ) ,
  mu <- a + b*(height - mean(height)),
  a ~ dnorm( 130 , 20 ) ,
  b ~ dnorm(-5, 20),  
  sigma ~ dunif( 0 , 50 )
)

m2 <- quap( flist , data=d2 )
precis( m2 )
```

You'll only really get into trouble with uniform priors that have a boundary, if true population parameter is outside your boundary. A good rule of thumb is to avoid the uniform distribution. We'll cover other options for priors for $\sigma$ in future lectures, but as a preview, the exponential function works very well for this!