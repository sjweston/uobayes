---
title: "week 6: integers and other monsters"
subtitle: "ordered categories"
format: 
  revealjs:
    css: xaringan-themer2.css
    nature:
      highlightStyle: solarized-dark
      highlightLines: true
      countIncrementalSlides: false
      mathjax: "default"
    self-contained: false  # Ensures correct embedding
    embed-resources: true  # Embeds required assets
    slide-number: true
    code-annotations: hover
execute:
  echo: false        
---

```{r, message = F, warning = F}
library(tidyverse)
library(psych)
library(cowplot)
library(patchwork)
library(here)
library(brms) 
library(tidybayes) 
```

```{r, echo = F}
knitr::opts_chunk$set(fig.retina=3, echo=TRUE)
theme_set(theme_cowplot())
default_palettes <- list(
  c("#5e8485" , "#0f393a") ,
  c("#1c5253" , "#5e8485" , "#0f393a") , 
  c("#1c5253" , "#e07a5f", "#5e8485", "#de9b8a"),
  # palette with 5 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" ) ,
  # same palette interpolated to 8 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" , "#a7a844" , "#69306d" ) 
  
)

options(ggplot2.discrete.fill = default_palettes, 
        ggplot2.discrete.colour = default_palettes)
```

## Ordered categorical outcomes

Ordered categories are variables that are discrete, like a count, except that the values merely indicate different ordered levels.[^1] Most Likert scale questions are ordered categories, even though we pretend they're continuous (see polychoric correlations).

If we want to model an outcome as an ordered categorical variable, what we're really asking is how does moving along an associated predictor variable move predictions in the outcome progressively through the categories _in sequence_. We must therefore model our outcomes in the right order.

To do so, we'll use the principles of the generalized linear model discussed last week. That is, we use a link function -- the **CUMULATIVE LINK FUNCTION** -- to model the probability of a value that is that value or smaller. 

[^1]: Actual counts communicate meaningful and consistent distance between possible values. 


---

### example: the trolley problem

```{r}
data(Trolley, package = "rethinking")
trolley <- Trolley
head(trolley)
```

---

```{r}
trolley %>% 
  ggplot(aes( x=response )) +
  geom_histogram()
```

---

```{r}
#| code-fold: true
trolley %>%
  count(response) %>% 
  mutate(pr_k     = n/nrow(trolley),
         cum_pr_k = cumsum(pr_k)) %>% 
  ggplot(aes(x = response, y = cum_pr_k)) +
  geom_point(size = 3, color = "#1c5253") +
  geom_line(color = "#1c5253", alpha = .3) + 
  labs(x = "response",
       y = "cumulative probability")
```

---

```{r}
#| code-fold: true
# primary data
trolley_plot <-
  trolley %>%
  count(response) %>%
  mutate(pr_k     = n / nrow(trolley),
         cum_pr_k = cumsum(n / nrow(trolley))) %>% 
  mutate(discrete_probability = ifelse(response == 1, cum_pr_k, cum_pr_k - pr_k))

# annotation
text <-
  tibble(text     = 1:7,
         response = seq(from = 1.25, to = 7.25, by = 1),
         cum_pr_k = trolley_plot$cum_pr_k - .065)

trolley_plot %>% 
  ggplot(aes(x = response, y = cum_pr_k,
             color = cum_pr_k, fill = cum_pr_k)) +
  geom_line() +
  geom_point(shape = 21, colour = "grey92", 
             size = 2.5, stroke = 1) +
  geom_linerange(aes(ymin = 0, ymax = cum_pr_k),
                 alpha = 1/2) +
  geom_linerange(aes(x = response + .025,
                     ymin = ifelse(response == 1, 0, discrete_probability), 
                     ymax = cum_pr_k),
                 color = "black") +
  # number annotation
  geom_text(data = text, 
            aes(label = text),
            size = 4) +
  scale_x_continuous(breaks = 1:7) +
  scale_y_continuous("cumulative proportion", breaks = c(0, .5, 1), 
                     limits = c(0, 1.1)) +
  theme(axis.ticks = element_blank(),
        axis.title.y = element_text(angle = 90),
        legend.position = "none")
```

---

On to the modeling. We'll start with an intercept-only model.

\begin{align*}
R_i &\sim \text{Categorical}(p) \\
\text{logit}(p_k) = \alpha_k - \phi \\
\phi &= 0 \\
\alpha_k &\sim \text{Normal}(0, 1.5)
\end{align*}

Remember, $\phi$ doesn't have an intercept because there's a $\alpha$ for each cut point.

```{r}
m1 <- brm(
  data = trolley,
  family = cumulative, 
  response ~ 1, 
  prior = c( prior(normal(0, 1.5), class = Intercept) ),
  iter=2000, warmup=1000, cores=4, chains=4,
  file=here("files/models/62.1")
)
```

:::{.notes}

$\alpha_K$ denotes the $K-1$ cut points or thresholds.

$\phi$ is a stand-in for a potential linear model.

:::

---

```{r}
summary(m1)
```


:::{.notes}
Implementation of the cumulative-logit model is 

`P(Y ≤ k) = F(disc * (thres[k] - mu))`

Where:

  - `thres[k]` - These are the threshold parameters (often called "cutpoints" or α_k in your lecture notes). They represent the boundaries between adjacent categories on the latent continuous scale. In brms, these are the "Intercept[k]" parameters you see in the model output.
  - `mu` - This is the linear predictor, which can include fixed effects, random effects, etc. It represents the location of an observation on the latent scale.

  - `disc` - This is a discrimination parameter (sometimes called a scale parameter). It controls how quickly the probabilities change as you move along the latent scale. Higher values mean sharper distinctions between categories.

In lecture, we formulate cumulative logit as

`logit(p_k) = α_k - φ_i`

Where:

  - `α_k` corresponds to `thres[k]` in `brms`
  - `φ_i` corresponds to `mu` in `brms`
  - The discrimination parameter is implicitly set to 1

The negative sign in front of φ_i in your formulation corresponds to how `brms` parameterizes the model with `(thres[k] - mu)` rather than `(mu - thres[k])`.

:::

---

The thresholds are in the logit-metric. Let's convert these back to probabilities. 

```{r}
gather_draws(m1, `b_.*`, regex=T) %>% 
  mutate(.value = inv_logit_scaled(.value)) %>% 
  mean_qi

```

---

### exercise

Create a plot showing the posterior predictive distribution for the intercept-only model (m1). Can you visualize both the posterior and the observed data?

---

### solution

```{r}
#| code-fold: true
predicted_draws(m1, newdata = data.frame(1)) %>% 
  count(.prediction) %>%
  mutate(pr_k = n / sum(n)) %>% 
  ggplot(aes(x = .prediction, y = pr_k)) +
  geom_col(fill = "#1c5253", alpha = 0.7) +
  geom_point(data = trolley_plot, aes(x = response), 
             color = "#3d405b", size = 2) +
  labs(x = "response",
       y = "probability",
       title = "Posterior Predictive Distribution") 
```

---

### adding predictors

Now let's add the features of the story -- contact, action, and intention -- as predictors in our model. 
Let's expand our mathematical model. Here's our original:

\begin{align*}
R_i &\sim \text{Categorical}(p) \\
\text{logit}(p_k) = \alpha_k - \phi_i \\
\phi_i &= 0 \\
\alpha_k &\sim \text{Normal}(0, 1.5)
\end{align*}

McElreath proposes the following causal formula:

$$
\phi_i = \beta_1\text{action}_i + \beta_2\text{contact}_i + \beta_3\text{intention}_i
$$

---


```{r}
#| code-fold: true
m2 <- brm(
  data = trolley,
  family = cumulative, 
  response ~ 1 + action + contact + intention, 
  prior = c( prior(normal(0, 1.5), class = Intercept),
             prior(normal(0, 0.5), class = b)),
  iter=2000, warmup=1000, cores=4, chains=4,
  file=here("files/models/62.2")
)

m2
```

---

```{r}
#| code-fold: true

gather_draws(m2, `b_.*`, regex=T) %>% 
  filter(!str_detect(.variable, "[0-9]")) %>% 
  mutate(.variable = str_remove(.variable, "b_")) %>% 
  ggplot( aes(x = .value, y = .variable) ) +
  geom_vline( aes(xintercept = 0), linetype = "dashed", alpha = .3 ) +
  stat_dist_halfeye(fill = "#5e8485") +
  scale_x_continuous("marginal posterior", breaks = -5:0 / 4) +
  scale_y_discrete(NULL) 
```

---

```{r}
nd <- 
  trolley %>% 
  distinct(action, contact, intention) %>% 
  mutate(combination = str_c(action, contact, intention, sep = "_"))

f <- predicted_draws( m2 , newdata = nd )

f
```

---

```{r}
#| code-fold: true

f %>% 
  as.data.frame() %>% 
  mutate(
    across( c(action, contact, intention) , 
            as.character),
    action  = str_c("action = ",  action),
    contact = str_c("contact = ", contact)) %>% 
  count(action, contact, intention, .prediction) %>% 
  ggplot( aes(x=.prediction, y=n, color=intention) )+
  geom_point() +
  geom_line(aes(group=intention)) +
  facet_grid(~action+contact) +
  labs( x="response", 
        y="count" )
```

---

### exercise

Create a new model that models the interaction between intention and the other variables. Then:

1. Compare the fits of these models using PSIS or WAIC.
2. Plot the posterior predictive distribution. How does it differ from our previous model?

---

### solution

```{r}

m2_complex <- brm(
  data = trolley,
  family = cumulative, 
  response ~ 1 + action + contact + intention + 
    action:intention + contact:intention, 
  prior = c(prior(normal(0, 1.5), class = Intercept),
            prior(normal(0, 0.5), class = b)),
  iter = 2000, warmup = 1000, cores = 4, chains = 4,
  file=here("files/models/62.2com")
)

```

---

### solution: compare

```{r}
m2  <-       add_criterion(m2,        "loo")
m2_complex <- add_criterion(m2_complex, "loo")

loo_compare(m2, m2_complex, criterion = "loo") %>% 
  print(simplify = F)

m2  <-       add_criterion(m2,        "waic")
m2_complex <- add_criterion(m2_complex, "waic")

loo_compare(m2, m2_complex, criterion = "waic") %>% 
  print(simplify = F)
```

---

### solution: ppd

```{r}
#| code-fold: true
predicted_draws(object = m2_complex, newdata = nd) %>% 
  mutate(
    intention=as.character(intention),
    action  = str_c("action = ",  action),
    contact = str_c("contact = ", contact)) %>% 
  count(action, contact, intention, .prediction) %>% 
  ggplot( aes(x=.prediction, y=n, color=intention) )+
  geom_point() +
  geom_line(aes(group=intention)) +
  facet_grid(~action+contact) +
  labs( x="response", 
        y="count" )
```

---

### solution: ppd

```{r}
#| code-fold: true

p1 = predicted_draws(object = m2, newdata = nd) %>% 
  mutate(model = "simple")
p2 = predicted_draws(object = m2_complex, newdata = nd) %>% 
  mutate(model = "complex")

p1 %>% full_join(p2) %>% 
  mutate(
    intention=as.character(intention),
    action  = str_c("action = ",  action),
    contact = str_c("contact = ", contact)) %>% 
  count(model, action, contact, intention, .prediction) %>% 
  ggplot( aes(x=.prediction, y=n, 
              color=interaction(intention,model,
                                sep="-",lex.order=TRUE)) )+
  geom_point() +
  geom_line(aes(group=interaction(intention,model,sep="-",lex.order=TRUE))) +
  facet_grid(~action+contact) +
  labs( x="response", 
        y="count",
        color = "intention+model")
```

---

## Ordered categorical predictors

```{r}
distinct(trolley, edu)
trolley <-
  trolley %>% 
  mutate(edu_new = 
           recode(edu,
                  "Elementary School" = 1,
                  "Middle School" = 2,
                  "Some High School" = 3,
                  "High School Graduate" = 4,
                  "Some College" = 5, 
                  "Bachelor's Degree" = 6,
                  "Master's Degree" = 7,
                  "Graduate Degree" = 8) %>% 
           as.integer())
```

---

To incorporate this variable as a **MONOTONIC CATEOGORICAL PREDICTOR**, we'll set up a notation such that each step up in value comes with its own incremental or marginal effect on the outcome.

$$
\phi_i = \sum_{j=1}^7 \delta_j
$$
We only need 7 because the first category is absorbed into the intercept for $\phi$. So our full formula for this parameter is:
$$
\phi_i = \beta_1\text{action}_i + \beta_2\text{contact}_i + \beta_3\text{intention}_i + \beta_E\sum_{j=0}^{E_i-1} \delta_j
$$
where $\beta_E$ is the average effect 

---

\begin{align*}
\text{response}_i &\sim \text{Categorical}(\mathbf{p}) \\
\text{logit}(p_k) &= \alpha_k - \phi_i \\
\phi_i &= \beta_1\text{action}_i + \beta_2\text{contact}_i + \beta_3\text{intention}_i + \beta_4 \text{ mo(edu_new}_i, \boldsymbol{\delta}) \\
\alpha_k &\sim \text{Normal}(0, 1.5) \\
\beta_1, \ldots, \beta_3 &\sim \text{Normal}(0, 1) \\
\beta_4 &\sim \text{Normal}(0, 0.143) \\
\boldsymbol{\delta} &\sim \text{Dirichlet}(2, 2, 2, 2, 2, 2, 2),
\end{align*}

---

```{r}
m3 <- 
  brm(data = trolley, 
      family = cumulative,
      response ~ 1 + action + contact + intention + mo(edu_new),  # note the `mo()` syntax
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = b),
                # note the new kinds of prior statements
                prior(normal(0, 0.143), class = b, coef = moedu_new),
                prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu_new1)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4, 
      seed = 12,
      file = here("files/models/62.3"))
```

_Warning: this will probably take 20 minutes or so to run on your laptop._ [You can download my model file here.](https://raw.githubusercontent.com/sjweston/uobayes/9762401fc0f67cc642610fd59d45c8bee185f176/files/share/62.3.rds)

---

```{r}
m3
```

---

Let's see the effect of education after controlling for story parameters.

```{r}
#| code-fold: true
nd <- distinct(
  trolley, 
  action, contact, intention, edu_new
)
predicted_draws(m3, nd) %>% 
  ungroup() %>% 
  count(edu_new, .prediction) %>% 
  with_groups(edu_new, mutate, total=sum(n)) %>% 
  mutate(pk = n/total,
         .prediction = as.numeric(.prediction),
         edu_new = as.factor(edu_new)) %>% 
  ggplot(aes(x = .prediction,  y = pk, color = edu_new)) +
  geom_point() +
  geom_line() +
  labs( x="response", 
        y="probability" ) +
  theme(legend.position = "top")
```

---

### exercise

Create a new model that includes education as a regular categorical predictor (not monotonic) and compare it to m3. Your tasks:

1. Fit the model using the original `edu` variable (not `edu_new`)
2. Create a plot of the posterior predictive distribution from the new model.

**OPTIONAL**
3. Use create a side-by-side visual comparison of the education effects
4. Interpret whether the monotonic constraint appears to improve the model fit

---

### solution: fit model

```{r}
#| code-fold: true
# Fit model with regular categorical education
m3_cat <- brm(
  data = trolley,
  family = cumulative,
  response ~ 1 + action + contact + intention + edu,
  prior = c(prior(normal(0, 1.5), class = Intercept),
            prior(normal(0, 1), class = b)),
  iter = 2000, warmup = 1000, cores = 4, chains = 4,
  file=here("files/models/62.3cat")
)

m3_cat

```

---

### solution: ppd

```{r}
#| code-fold: true
nd <- distinct(
  trolley, 
  action, contact, intention, edu
)
predicted_draws(m3_cat, nd) %>% 
  ungroup() %>% 
  count(edu, .prediction) %>% 
  mutate(
    edu = factor(edu,
                 levels = c(
                  "Elementary School",
                  "Middle School",
                  "Some High School",
                  "High School Graduate",
                  "Some College" ,
                  "Bachelor's Degree",
                  "Master's Degree",
                  "Graduate Degree"))) %>% 
  with_groups(edu, mutate, total=sum(n)) %>% 
  mutate(pk = n/total,
         .prediction = as.numeric(.prediction),
         edu_new = as.factor(edu)) %>% 
  ggplot(aes(x = .prediction,  y = pk, color = edu)) +
  geom_point() +
  geom_line() +
  labs( x="response", 
        y="probability" ) +
  theme(legend.position = "top")
```

---

### solution: side-by-side

```{r}
#| code-fold: true

p1 <- predicted_draws(m3, newdata = distinct(trolley, action, contact, intention, edu_new)) %>%
  mutate(model = "ordered",
         edu = factor(edu_new, 
                      levels = c(1:8),
                      labels = c(
                  "Elementary School",
                  "Middle School",
                  "Some High School",
                  "High School Graduate",
                  "Some College" ,
                  "Bachelor's Degree",
                  "Master's Degree",
                  "Graduate Degree")))
p2 <- predicted_draws(m3_cat, newdata = distinct(trolley, action, contact, intention, edu)) %>%
  mutate(model = "categorical")

full_join(p1, p2) %>% 
  ungroup() %>% 
  count(model, edu, .prediction) %>% 
  with_groups(c(model,edu), mutate, total = sum(n)) %>% 
  mutate(pk = n/total,
         .prediction = as.numeric(.prediction)) %>% 
  ggplot(aes(x = .prediction, y = pk, color = edu)) +
  geom_point(aes(shape = model)) +
  geom_line(aes(linetype = model)) +
  labs(x = "response", y = "probability") +
  guides(color = "none") + 
  facet_wrap(~edu) + 
  theme(legend.position = "top")

```


---

### solution: compare fit

```{r}
m3  <- add_criterion(m3,        "loo")
m3_cat <- add_criterion(m3_cat, "loo")

loo_compare(m3, m3_cat, criterion = "loo") %>% 
  print(simplify = F)

m3  <- add_criterion(m3,        "waic")
m3_cat <- add_criterion(m3_cat, "waic")

loo_compare(m3, m3_cat, criterion = "waic") %>% 
  print(simplify = F)
```

