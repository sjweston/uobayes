---
title: "week 8: multilevel models"
subtitle: "MELSM"
format: 
  revealjs:
    css: xaringan-themer2.css
    nature:
      highlightStyle: solarized-dark
      highlightLines: true
      countIncrementalSlides: false
      mathjax: "default"
    self-contained: false  # Ensures correct embedding
    embed-resources: true  # Embeds required assets
    slide-number: true
    code-annotations: hover
execute:
  echo: false        
---

```{r, message = F, warning = F}
library(tidyverse)
library(psych)
library(cowplot)
library(patchwork)
library(here)
library(brms) 
library(tidybayes) 
```


```{r, echo = F}
knitr::opts_chunk$set(fig.retina=3, echo=TRUE)
theme_set(theme_cowplot())
default_palettes <- list(
  c("#5e8485" , "#0f393a") ,
  c("#1c5253" , "#5e8485" , "#0f393a") , 
  # palette with 5 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" ) ,
  # same palette interpolated to 8 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" , "#a7a844" , "#69306d" ) 
  
)

options(ggplot2.discrete.fill = default_palettes, 
        ggplot2.discrete.colour = default_palettes)
```

## double-advanced ~~potions~~ multilevel models

* Multivariate MLMs (M-MLM)
    
    * Fit models with 2+ (don't get crazy) outcomes. 
    * The advantage is estimating the correlations between the outcomes, as well as the Level 2 coefficients. 

* Mixed Effects Location Scale Models (MELSM)
  * Estimate both the mean (location) and variance (scale) of the repeated measures for each of your clusters. 

---

We'll use data provided by Williams and colleagues ([2021](../files/williams_etal_2021.pdf). 

```{r}
d <- read_csv("https://raw.githubusercontent.com/sjweston/uobayes/refs/heads/main/files/data/external_data/williams.csv")
# scaled time variable
d <- d %>% mutate(day01 = (day - 2) / max((day - 2)))
distinct(d, id) %>% count()
d %>% 
  count(id) %>% 
  summarise(median = median(n),
            min = min(n),
            max = max(n))
```

---

```{r}
#| code-fold: true

d %>% 
  count(id) %>% 
  ggplot(aes(x = n)) +
  geom_histogram(fill = "#1c5253", color = "white") +
  scale_x_continuous("number of days", limits = c(0, NA)) 
```

```{r}
rethinking::precis(d) 
```

---

```{r}
#| code-fold: true
set.seed(14)

d %>% 
  nest(data=-id) %>% 
  slice_sample(n = 6) %>% 
  unnest(data) %>% 
  ggplot(aes(x = day, y = NA_lag)) +
  geom_line(color = "grey") +
  geom_point(color = "#1c5253", size = 1/2) +
  geom_line(aes(y = PA_lag), color = "darkgrey") +
  geom_point(aes(y = PA_lag), color = "#e07a5f", size = 1/2) +
  ylab("affect (standardized)") +
  facet_wrap(~ id)
```

---

Let's fit an MLM with varying intercepts and slopes.

Likelihood function and linear model

\begin{align*}
\text{NA}_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha_{\text{id}[i]} + \beta_{1\text{id}[i]}\text{NA_lag}_i + \beta_{2\text{id}[i]}\text{PA_lag}_i
\end{align*}

Varying intercepts and slopes:

\begin{align*}
\alpha_{\text{id}[i]} &= \alpha + u_{\alpha,\text{id}[i]} \\
\beta_{1\text{id}[i]} &= \beta + u_{\beta,\text{id}[i]} \\
\beta_{2\text{id}[i]} &= \beta + u_{\beta,\text{id}[i]}
\end{align*}

---

Random effects:

\begin{align*}
\begin{bmatrix} u_{\alpha,\text{id}[i]} \\ u_{\beta,1\text{id}[i]} \\ u_{\beta,2\text{id}[i]} \end{bmatrix} &\sim \text{MVNormal}\left(\begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}, \mathbf{S}\right) \\
\mathbf{S} &= \begin{pmatrix} \sigma_{\alpha} & 0 \\ 0 \\ 0 & \sigma_{\beta}\end{pmatrix}\mathbf{R}\begin{pmatrix} \sigma_{\alpha} & 0 \\ 0 \\ 0 & \sigma_{\beta}\end{pmatrix}
\end{align*} 


Priors: 
\begin{align*}
\alpha &\sim \text{Normal}(0,0.2) \\
\beta &\sim \text{Normal}(0,0.5) \\
\sigma &\sim \text{Exponential}(1) \\
\sigma_{\alpha} &\sim \text{Exponential}(1) \\
\sigma_{\beta} &\sim \text{Exponential}(1) \\
\mathbf{R} &\sim \text{LKJcorr}(2)
\end{align*}


---

```{r}
m1 <-
  brm(data = d,
      family = gaussian,
      NA.std ~ 1 + NA_lag + PA_lag + (1 + NA_lag + PA_lag | id),
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(exponential(1), class = sd),
                prior(exponential(1), class = sigma),
                prior(lkj(2), class = cor)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      seed = 14,
      file = here("files/models/m82.1"))
```

What if I wanted to model positive affect? Usually, I'd have to fit a second model. 

---

### Multivariate MLMs

But what if I told you I could estimate both positive and negative affect simultaneously? Let's start with an extremely basic, intercept-only model.


Likelihood function

\begin{align*}
\text{NA}_i &\sim \text{Normal}(\mu_{\text{NA},i}, \sigma_{\text{NA}}) \\
\mu_{\text{NA},i} &= \alpha_{\text{NA},\text{id}[i]} \\
\text{PA}_i &\sim \text{Normal}(\mu_{\text{PA},i}, \sigma_{\text{PA}}) \\
\mu_{\text{PA},i} &= \alpha_{\text{PA},\text{id}[i]} 
\end{align*}

Varying intercepts:

\begin{align*}
\alpha_{\text{NA},\text{id}[i]} &= \alpha_{\text{NA}} + u_{\alpha,1\text{NA},\text{id}[i]} \\
\alpha_{\text{PA},\text{id}[i]} &= \alpha_{\text{PA}} + u_{\alpha,\text{PA},\text{id}[i]} \\
\end{align*}

---

Random Effects: Multivariate Distribution

\begin{align*}
\mathbf{u}_{\text{id}[i]} =
\begin{bmatrix}
u_{\alpha,\text{NA},\text{id}[i]} \\
u_{\alpha,\text{PA},\text{id}[i]}
\end{bmatrix}
&\sim \text{MVNormal}(\mathbf{0}, \boldsymbol{\Sigma}) \\
\boldsymbol{\Sigma} &= \mathbf{L} \mathbf{R} \mathbf{L} \quad
\\ \text{with} \quad
\mathbf{L} &= \text{diag}(\sigma_{\alpha,\text{NA}}, \sigma_{\alpha,\text{PA}})
\end{align*}

Priors

\begin{align*}
\alpha_{\text{NA}}, \alpha_{\text{PA}} &\sim \text{Normal}(0, 0.2) \\
\sigma_{\text{NA}}, \sigma_{\text{PA}} &\sim \text{Exponential}(1) \\
\sigma_{\alpha,\text{NA}}, \sigma_{\beta,1\text{NA}}, \sigma_{\beta,2\text{NA}}, \mathbf{R} &\sim \text{LKJcorr}(2)
\end{align*}

---

```{r}
bf_na = bf(NA.std ~ 1 + (1  | c | id))
bf_pa = bf(PA.std ~ 1 + (1  | c | id))
m2 <-
  brm(data = d,
      family = gaussian,
      bf_na + bf_pa + set_rescor(TRUE),
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(lkj(2), class = cor)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      seed = 14,
      file = here("files/models/m82.2"))
```

---

This model has partitioned our data into several key pieces. First, we can get the between-person variance and between-person correlations. 

```{r}
m2
```

---

### list of terms

**between-person**

  * `sd(NAstd_Intercept)` : variability in average negative affect
  * `sd(PAstd_Intercept)` : variability in average positive affect
  * `cor(NAstd_Intercept,PAstd_Intercept)`: correlation between average NA and average PA
  
**within-person**

  * `sigma_NAstd`: variability within participants in NA
  * `sigma_PAstd`: variability within participants in PA
  * `rescor(NAstd,PAstd)`: within-person correlation of PA and NA

---

Now we'll add in some predictors. Let's predict affect from lagged affect. 


Likelihood function

\begin{align*}
\text{NA}_i &\sim \text{Normal}(\mu_{\text{NA},i}, \sigma_{\text{NA}}) \\
\mu_{\text{NA},i} &= \alpha_{\text{NA},\text{id}[i]} + \beta_{1\text{NA},\text{id}[i]}\,\text{PA_lag}_i + \beta_{2\text{NA},\text{id}[i]}\,\text{NA_lag}_i \\
\\
\text{PA}_i &\sim \text{Normal}(\mu_{\text{PA},i}, \sigma_{\text{PA}}) \\
\mu_{\text{PA},i} &= \alpha_{\text{PA},\text{id}[i]} + \beta_{1\text{PA},\text{id}[i]}\,\text{PA_lag}_i + \beta_{2\text{PA},\text{id}[i]}\,\text{NA_lag}_i 
\end{align*}

Varying intercepts and slopes:

\begin{align*}
\alpha_{\text{NA},\text{id}[i]} &= \alpha_{\text{NA}} + u_{\alpha,1\text{NA},\text{id}[i]} \\

\beta_{1\text{NA},\text{id}[i]} &= \beta_{1\text{NA}} + u_{\beta,1\text{NA},\text{id}[i]} \\

\beta_{2\text{NA},\text{id}[i]} &= \beta_{2\text{NA}} + u_{\beta,2\text{NA},\text{id}[i]} \\

\alpha_{\text{PA},\text{id}[i]} &= \alpha_{\text{PA}} + u_{\alpha,\text{PA},\text{id}[i]} \\

\beta_{1\text{PA},\text{id}[i]} &= \beta_{1\text{PA}} + u_{1\beta,\text{PA},\text{id}[i]} \\

\beta_{2\text{PA},\text{id}[i]} &= \beta_{2\text{PA}} + u_{2\beta,\text{PA},\text{id}[i]}

\end{align*}

---

Random Effects: Multivariate Distribution

\begin{align*}
\mathbf{u}_{\text{id}[i]} =
\begin{bmatrix}
u_{\alpha,\text{NA},\text{id}[i]} \\
u_{\beta,1\text{NA},\text{id}[i]} \\
u_{\beta,2\text{NA},\text{id}[i]} \\
u_{\alpha,\text{PA},\text{id}[i]} \\
u_{\beta,1\text{PA},\text{id}[i]} \\
u_{\beta,2\text{PA},\text{id}[i]}
\end{bmatrix}
&\sim \text{MVNormal}(\mathbf{0}, \boldsymbol{\Sigma}) \\
\boldsymbol{\Sigma} &= \mathbf{L} \mathbf{R} \mathbf{L} \quad
\\ \text{with} \quad
\mathbf{L} &= \text{diag}(\sigma_{\alpha,\text{NA}}, \sigma_{\beta,1\text{NA}}, \sigma_{\beta,2\text{NA}}, \sigma_{\alpha,\text{PA}}, \sigma_{\beta,1\text{PA}}, \sigma_{\beta,2\text{PA}})
\end{align*}

Priors

\begin{align*}
\alpha_{\text{NA}}, \alpha_{\text{PA}} &\sim \text{Normal}(0, 0.2) \\
\beta_{1\text{NA}}, \beta_{2\text{NA}}, \beta_{1\text{PA}}, \beta_{2\text{PA}} &\sim \text{Normal}(0, 0.5) \\
\sigma_{\text{NA}}, \sigma_{\text{PA}} &\sim \text{Exponential}(1) \\
\sigma_{\alpha,\text{NA}}, \sigma_{\beta,1\text{NA}}, \sigma_{\beta,2\text{NA}}, \sigma_{\alpha,\text{PA}}, \sigma_{\beta,1\text{PA}}, \sigma_{\beta,2\text{PA}} &\sim \text{Exponential}(1) \\
\mathbf{R} &\sim \text{LKJcorr}(2)
\end{align*}

---

```{r}
bf_na = bf(NA.std ~ 1 + NA_lag + PA_lag + (1 + NA_lag + PA_lag  | c | id))
bf_pa = bf(PA.std ~ 1 + NA_lag + PA_lag + (1 + NA_lag + PA_lag  | c | id))
m3 <-
  brm(data = d,
      family = gaussian,
      bf_na + bf_pa + set_rescor(TRUE),
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(lkj(2), class = cor)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      seed = 14,
      file = here("files/models/m82.3"))
```

---

```{r}
m3
```

---


These models can help us address many questions. First, let's examine the between- and within-person variance of these outcomes. 

```{r}
#| code-fold: true

post = as_draws_df(m3)

post %>% 
  select(between = cor_id__NAstd_Intercept__NAstd_NA_lag, 
         within = rescor__NAstd__PAstd) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mean_qi()
```

To clarify: this is the within-person correlation _after accounting_ for the predictors in the model. This can help us detect any additional patterns in the data. 

---

```{r, eval=F, echo=F}
#|code-fold: true
#|
m2_post = m3 %>% spread_draws(b_NAstd_Intercept, b_PAstd_Intercept, 
                              b_NAstd_NA_lag, b_NAstd_PA_lag, 
                              b_PAstd_NA_lag, b_PAstd_PA_lag,  
                              r_id__NAstd[id, term], 
                              r_id__PAstd[id, term]) 
```

```{r}
#| code-fold: true
m3 %>% 
  gather_draws(r_id__NAstd[id, term], r_id__PAstd[id, term]) %>% 
  mutate(.variable=str_remove(.variable, "r_id__")) %>% 
  pivot_wider(names_from = .variable, values_from = .value) %>% 
  # just for presenting to class 
  arrange(.draw, id, term) %>% select(-.chain, -.iteration) %>% 
  mutate(across(c(NAstd, PAstd), \(x) round(x, 2)))
```


---

### spaghetti plots

```{r}
#| code-fold: true
#| 
nd = data.frame(
  PA_lag = seq(from = min(d$PA_lag), to=max(d$PA_lag), length.out=50),
  NA_lag = 0,
  id = max(d$id) + 1
)

nd %>% add_epred_draws(m3, allow_new_levels=T) %>% 
  filter(.draw <= 50) %>% 
  ggplot(aes(x = PA_lag, y = .epred)) +
  geom_line(aes(group = .draw, color = .category), alpha=.2) +
  scale_color_manual(values = c("#1c5253" , "#e07a5f")) +
  facet_wrap(~.category) +
  guides(color="none") +
  labs(x = "Positive Affect (lagged)", y = "Expected score")
```

---

```{r}
#| code-fold: true
#| 
nd = data.frame(
  NA_lag = seq(from = min(d$PA_lag), to=max(d$PA_lag), length.out=50),
  PA_lag = 0,
  id = max(d$id) + 1
)

nd %>% add_epred_draws(m3, allow_new_levels=T) %>% 
  filter(.draw <= 50) %>% 
  ggplot(aes(x = NA_lag, y = .epred)) +
  geom_line(aes(group = .draw, color = .category), alpha=.2) +
  scale_color_manual(values = c("#1c5253" , "#e07a5f")) +
  facet_wrap(~.category) +
  guides(color="none") +
  labs(x = "Negative Affect (lagged)", y = "Expected score")
```

---

But of course, we've allowed these effects to vary for each individual.

```{r}
nd = expand.grid(
  PA_lag = seq(from = min(d$PA_lag), to=max(d$PA_lag), length.out=50),
  NA_lag = 0,
  id = sample(d$id, size = 4, replace=F)
)

nd %>% add_epred_draws(m3) %>% 
  filter(.draw <= 20) %>% 
  ggplot(aes(x = PA_lag, y = .epred)) +
  geom_line(aes(group = .draw, color = .category), alpha=.2) +
  scale_color_manual(values = c("#1c5253" , "#e07a5f")) +
  facet_grid(.category~id) +
  guides(color="none") +
  labs(x = "Positive Affect (lagged)", y = "Expected score")
```

---

We may even want to see whether there's a correlation between the effect of positive affect on PA and on NA.

```{r}
postm2 = m3 %>% spread_draws(r_id__PAstd[id, term],
                    r_id__NAstd[id, term]) 

postm2 %>% 
  filter(term == "PA_lag") %>% 
  mean_qi %>% 
  ggplot(aes(x = r_id__PAstd, y = r_id__NAstd)) + 
  geom_point() +
  labs(x = "Effect of PA on PA", y="Effect of PA on NA")
```

---

But of course, each of these points is a summary of a distribution! The real benefit of modeling these outcomes jointly is that we can also get a distribution of the correlations between these effects. 

```{r}
m3 %>% spread_draws(`cor.*`, regex = T) %>% 
  pivot_longer(starts_with("cor"),
               names_prefix = "cor_id__") %>% 
  group_by(name) %>% 
  mean_qi(value) %>% 
  arrange(desc(abs(value)))
```

---

## modeling variability

Traditionally, psycho-social research has focused on identifying trait-like behaviors - for example, how negative a person is on average. But more recent work has broadened to include states - that is, whether and how much individuals fluctuate in their thoughts, feelings, and behaviors over time.

On the macro time scale, we typically assume stable traits, but on the micro time scale (day to day), we often observe substantial variability in these same traits. This presents a methodological challenge: how do we simultaneously model both the stable personality traits (the "location") and the fluctuations (the "scale")?
The standard approach has been a two-stage process:

  1. compute individual means (`iM`s) in a mixed effects model
  2. obtain individual standard deviations (`iSD`s) from the residuals

But this approach has several drawbacks:

  * It can result in unreliable estimates
  * It's particularly sensitive to the number of measurement occasions
  * It assumes independence of means and variances, which seems unlikely
  * It results in biased variance estimates

---

### mixed-effects location scale models

In a standard linear mixed effects model with repeated measurements, we have:

$$
y_i = X_i\beta + Z_ib_i + \epsilon_i
$$
Where:

  * $y_i$ is the observations for the ith person
  * $X_i\beta$ represents the fixed effects
  * $Z_ib_i$ represents the random effects (person-specific deviations)
  * $\epsilon_i$ represents the error term (or states)
  
The key limitation of this standard model is that it assumes the same error variance $(\sigma^2)$ for everyone. The MELSM, instead, allows $\sigma^2$ to differ at the individual level and even at different time points. The scale component is modeled as:

$$
\varphi_i = \text{exp}(W_i\eta + V_it_i)
$$
Where:

* $\varphi_i$ contains the error variances
* $\eta$ represents fixed effects for the scale
* $t_i$ represents random effects for the scale
* The exponential function ensures positive variance estimates

---

```{r}
m4 <-
  brm(data = d,
      family = gaussian,
    bf(
    # location
    PA.std ~ 1 + (1 | c | id), 
    # scale (the c  in |c| allows for correlation between location and scale)
    sigma ~ 1 + (1 | c | id)
    ), 
    prior = c(prior(normal(0, 0.2), class = Intercept),
              prior(exponential(1), class = sd),
              prior(normal(0,1), class = Intercept, dpar=sigma),
              prior(exponential(1), class = sd, dpar=sigma),
              prior(lkj(2), class = cor)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      seed = 14,
      file = here("files/models/m82.4"))
```

---

```{r}
m4
```

---

Our posterior distribution contains both locations (means) and scales $\text{log}(\sigma)$ for each of our clusters (participants). We can plot these to see whether they're related. 

```{r}
#| code-fold: true
m4 |> 
  spread_draws( r_id[id, term], r_id__sigma[id, term] ) |> 
  summarise(  location=mean(r_id), 
              scale=mean(exp(r_id__sigma))) |> 
  ggplot( aes(x=location, y=scale) ) +
  geom_point() +
  geom_smooth(se=F, method="lm")

```

Of course, the correlation between these values can also be found in our model output. 

```{r}
posterior_summary(m4)["cor_id__Intercept__sigma_Intercept", ] |> round(2)
```

---

The real power of these models comes when we incorporate predictors for both location and scale.

```{r}
m5 <-brm(
  data = d,
  family = gaussian,
  bf(
    # location
    PA.std ~ 1 + day + PA_lag + NA_lag + steps.pm*steps.pmd + 
      (1 + day + steps.pmd | c | id), 
    sigma ~ 1 + PA_lag + NA_lag + steps.pm*steps.pmd + (1 + steps.pmd | c | id)
    ), 
    prior = c(prior(normal(0, 0.2), class = Intercept),
              prior(normal(0, 1), class = b),
              prior(exponential(1), class = sd),
              prior(normal(0,1), class = Intercept, dpar=sigma),
              prior(exponential(1), class = sd, dpar=sigma),
              prior(lkj(2), class = cor)),
    iter = 5000, warmup = 1000, chains = 4, cores = 4,
    seed = 14,
    file = here("files/models/m82.5"))
```

:::{.notes}
We should note a few things about the brm() syntax. First, because we modeled both $\mu_{ij}$ and $\sigma_i$, we nested both model formulas within the `bf()` function. Second, because the `brms` default is to use the log link when modeling $\sigma_i$, there was no need to explicitly set it that way in the family line. However, we could have if we wanted to. Third, notice our use of the `|i|` syntax within the parentheses in the formula lines. If we had used the conventional `|` syntax, that would have not allowed our $\mu_{ij}$ parameters to correlate with  
$\mu_{0i}$ and $\mu_{1i}$  from the mean structure. It would have effectively set  . Finally, notice how within the `prior()` functions, we explicitly referred to those for the new $\sigma$ structure with the `dpar = sigma` operator.
:::

---

Our model clearly has not mixed -- be sure to run this model for many iterations if you actually want to use it. As it was, this model took 80 minutes to run on my computer, so I didn't have time to go back and run for longer.

```{r}
m5
```

---


```{r}
#| code-fold: true
m5 |> 
  gather_draws(`^b_.*`, regex=T) |> 
  mutate(
    out = ifelse( str_detect(.variable, "sigma"), "var", "location"),
    .variable = str_remove(.variable, "^b_sigma_"),
    .variable = str_remove(.variable, "^b_")
  ) |> 
    filter(.variable != "Intercept") |> 
    ggplot( aes(x=.value, y=.variable) ) +
    stat_halfeye() +
    facet_wrap(~out, nrow=2, scales="free") +
    labs(x="coef estimate", y=NULL, title="posterior")
```