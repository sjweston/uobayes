---
title: "week 7: multilevel models"
subtitle: "multilevel adventures"
format: 
  revealjs:
    css: xaringan-themer2.css
    nature:
      highlightStyle: solarized-dark
      highlightLines: true
      countIncrementalSlides: false
      mathjax: "default"
    self-contained: false  # Ensures correct embedding
    embed-resources: true  # Embeds required assets
    slide-number: true
    code-annotations: hover
execute:
  echo: false        
---

```{r, message = F, warning = F}
library(tidyverse)
library(psych)
library(cowplot)
library(patchwork)
library(here)
library(brms) 
library(tidybayes) 
```


```{r, echo = F}
knitr::opts_chunk$set(fig.retina=3, echo=TRUE)
theme_set(theme_cowplot())
default_palettes <- list(
  c("#5e8485" , "#0f393a") ,
  c("#1c5253" , "#5e8485" , "#0f393a") , 
  # palette with 5 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" ) ,
  # same palette interpolated to 8 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" , "#a7a844" , "#69306d" ) 
  
)

options(ggplot2.discrete.fill = default_palettes, 
        ggplot2.discrete.colour = default_palettes)
```

---

## mlm

Often, there are opportunities to cluster your observations -- repeated measures, group membership, hierarchies, even different measures for the same particiapnt. Whenever you can cluster, you should!

* Aggregation is bad
* Regressions within regressions (ie coefficients as outcomes)
* Questions at different levels
* Variance decomposition
* Learning from other data through pooling/shrinkage
* Parameters that depend on parameters

:::{.notes}
Aggregation
* Between person H1: Do students who study more get better grades?
* Within person H2: When a student studies, do they get better grades?
* H1 and H2 are independent from one another! Aggregation collapses the two. When you have nested data with many DVs it is important to not aggregate.
:::

---

## more than one type of cluster

McElreath doesn't cover this in his video lecture, but this is from the textbook and worth discussing.

::::: columns

::: {.column width="50%"}

```{r}
data(chimpanzees, package="rethinking")
d <- chimpanzees
rethinking::precis(d)
```

:::

::: {.column width="50%"}
Data from [Silk et al. (2005)](https://www.nature.com/articles/nature04243)

![](images/7-1_chimp.png)
:::

:::::

:::{.notes}
From McElreath: 

The data for this example come from an experiment aimed at evaluating the prosocial tendencies of chimpanzees (_Pan troglodytes_). The experimental structure mimics many common experiments conducted on human students (_Homo sapiens studiensis_) by economists and psychologists. A focal chimpanzee sits at one end of a long table with two levers, one on the left and one on the right in this figure. On the table are four dishes which may contain desirable food items. The two dishes on the right side of the table are attached by a mechanism to the right-hand lever. The two dishes on the left side are similarly attached to the left-hand lever.

When either the left or right lever is pulled by the focal animal, the two dishes on the same side slide towards opposite ends of the table. This delivers whatever is in those dishes to the opposite ends. In all experimental trials, both dishes on the focal animalâ€™s side contain food items. But only one of the dishes on the other side of the table contains a food item. Therefore while both levers deliver food to the focal animal, only one of the levers delivers food to the other side of the table.

There are two experimental conditions. In the partner condition, another chimpanzee is seated at the opposite end of the table, as pictured in the figure. In the control condition, the other side of the table is empty. Finally, two counterbalancing treatments alternate which side, left or right, has a food item for the other side of the table. This helps detect any handedness preferences for individual focal animals.

When human students participate in an experiment like this, they nearly always choose the lever linked to two pieces of food, the prosocial option, but only when another student sits on the opposite side of the table. The motivating question is whether a focal chimpanzee behaves similarly, choosing the prosocial option more often when another animal is present. In terms of linear models, we want to estimate the interaction between condition (presence or absence of another animal) and option (which side is prosocial).
:::


------------------------------------------------------------------------

```{r}
unique(d$actor)
unique(d$block)
unique(d$prosoc_left)
unique(d$condition)
```


We could model the interaction between condition (presence/absence of another animal) and option (which side is prosocial), but it is more difficult to assign sensible priors to interaction effects. Another option, because we're working with categorical variables, is to turn our 2x2 into one variable with 4 levels. 

```{r}
d$treatment <- factor(1 + d$prosoc_left + 2*d$condition)
d %>% count(treatment, prosoc_left, condition)
```

---

In this experiment, each pull is within a cluster of pulls belonging to an individual chimpanzee. But each pull is also within an experimental block, which represents a collection of observations that happened on the same day. So each observed pull belongs to both an actor (1 to 7) and a block (1 to 6). There may be unique intercepts for each actor as well as for each block.

Mathematical model:

\begin{align*}
L_i &\sim \text{Binomial}(1, p_i) \\
\text{logit}(p_i) &= \bar{\alpha} + \alpha_{\text{ACTOR[i]}} + \bar{\gamma} + \gamma_{\text{BLOCK[i]}} +  \beta_{\text{TREATMENT[i]}} \\
\beta_j &\sim \text{Normal}(0, 0.5) \text{ , for }j=1..4\\
\alpha_j &\sim \text{Normal}(0, \sigma_{\alpha}) \text{ , for }j=1..7\\
\gamma_j &\sim \text{Normal}(0, \sigma_{\gamma}) \text{ , for }j=1..7\\
\bar{\alpha} &\sim \text{Normal}(0, 1.5) \\
\bar{\gamma} &\sim \text{Normal}(0, 1.5) \\
\sigma_{\alpha} &\sim \text{Exponential}(1) \\
\sigma_{\gamma} &\sim \text{Exponential}(1) \\
\end{align*}

----


```{r}
m3 <- 
  brm(
    family = bernoulli,
    data = d, 
    bf(
      pulled_left ~ a + b, 
      a ~ 1 + (1 | actor) + (1 | block), 
      b ~ 0 + treatment, 
      nl = TRUE),
    prior = c(prior(normal(0, 0.5), nlpar = b),
              prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = a),
              prior(exponential(1), class = sd, group = actor, nlpar = a),
              prior(exponential(1), class = sd, group = block, nlpar = a)),
  chains=4, cores=4, iter=2000, warmup=1000,
  seed = 1,
  file = here("files/models/71.3")
  )
```

------------------------------------------------------------------------

```{r}
m3
```

---

```{r}
posterior_summary(m3)
```

---

```{r}
m3 %>% 
  mcmc_plot(variable = c("^r_", "^b_", "^sd_"), regex = T) +
  theme(axis.text.y = element_text(hjust = 0))
```

---

Zooming in on just the actor and block effects. (Remember, these are differences from the weighted average.)

```{r}
m3 %>% 
  mcmc_plot(variable = c("^r_"), regex = T) +
  theme(axis.text.y = element_text(hjust = 0))
```

---

```{r}
#| code-fold: true
as_draws_df(m3) %>% 
  select(starts_with("sd")) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value, fill = name)) +
  geom_density(linewidth = 0, alpha = 3/4, adjust = 2/3, show.legend = F) +
  annotate(geom = "text", x = 0.67, y = 2, label = "block", color = "#5e8485") +
  annotate(geom = "text", x = 2.725, y = 0.5, label = "actor", color = "#0f393a") +
  scale_fill_manual(values = c("#0f393a", "#5e8485")) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle(expression(sigma["group"])) +
  coord_cartesian(xlim = c(0, 4))
```

---

### exercise

Return to the `data(Trolley)` from an earlier lecture. Define and fit a varying intercepts model for these data, with responses clustered within participants. Include `action`, `intention`, and `contact.` Compare the varying-intercepts model and the model that ignores individuals using both some method of cross-validation. 

---

### solution

```{r}
data(Trolley, package="rethinking")

# fit model without varying intercepts
m_simple <- brm(
  data = Trolley,
  family = cumulative, 
  response ~ 1 + action + intention + contact, 
  prior = c( prior(normal(0, 1.5), class = Intercept) ),
  iter=2000, warmup=1000, cores=4, chains=4,
  file=here("files/data/generated_data/m71.e1")
)

# fit model with varying intercepts
m_varying <- brm(
  data = Trolley,
  family = cumulative, 
  response ~ 1 + action + intention + contact + (1|id), 
  prior = c( prior(normal(0, 1.5), class = Intercept),
             prior(normal(0, 0.5), class = b),
             prior(exponential(1), class = sd)),
  iter=2000, warmup=1000, cores=4, chains=4,
  file=here("files/data/generated_data/m71.e2")
)
```

---

### solution

```{r}
# compare models using WAIC cross-validation
m_simple  <- add_criterion(m_simple , "loo")
m_varying <- add_criterion(m_varying, "loo")

loo_compare(m_simple, m_varying, criterion = "loo") %>% 
  print(simplify=F)
```

---

```{r}
pp_check(m_simple, ndraws = 5, type="hist") +
  ggtitle("Simple Model")
```

---

```{r}
pp_check(m_varying, ndraws = 5, type="hist") +
  ggtitle("Varying Intercepts Model")
```


---

### predictions

Posterior predictions in multilevel models are a bit more complicated than single-level, because the question arises: predictions for the same clusters or predictions for new clusters?

In other words, do you want to know more about the chimps you collected data on, or new chimps? Let's talk about both. 

**predictions for chimps in our sample**

Recall that the function `fitted()` give predictions. Using the argument `re_formula = NULL` specifies that we want to include our group-level estimates in our estimations. 

```{r}
labels <- c("R/N", "L/N", "R/P", "L/P")

nd <- distinct(d, treatment, actor) %>% 
  mutate(block=1)

f <- add_epred_draws(m3,newdata = nd) %>% 
  mutate(treatment = factor(treatment, 
                            levels = c("1", "2", "3", "4"), 
                            labels = labels)) %>% 
  group_by(actor, treatment) %>% 
  mean_qi(.epred)
```

---

```{r}
#| code-fold: true

f %>% 
  ggplot( aes(x=treatment, y=.epred, group=1) ) +
  geom_ribbon(aes( ymin=.lower, ymax=.upper ), 
              fill = "#0f393a",
              alpha=.3) +
  geom_line(color="#0f393a") +
  scale_y_continuous(limits=c(0,1)) +
  facet_wrap(~actor)
```

---

```{r}
#| code-fold: true

# observed p
obs = d %>% 
  filter(block==1) %>% 
  group_by(actor, treatment) %>% 
  summarise(p = mean(pulled_left), .groups = "drop") %>% 
  mutate(treatment = factor(treatment, labels = labels))


f %>% 
  ggplot( aes(x=treatment, y=.epred, group=1) ) +
  geom_ribbon(aes( ymin=.lower, ymax=.upper ), 
              fill = "#0f393a",
              alpha=.3) +
  geom_point( aes(y=p), 
              data=obs, 
              shape=1) +
  geom_line(color="#0f393a") +
  facet_wrap(~actor)
```

We can add in the observed probabilities.

---

**predictions for new chimps**

Even here, we have some choice. Let's start by predicting scores for the average chimp. We can use the same code as before, but set `re_formula` to `NA`.

```{r}
labels <- c("R/N", "L/N", "R/P", "L/P")

nd <- distinct(d, treatment) %>% 
  mutate(block=1)

f_avg <- add_epred_draws(m3,newdata = nd, re_formula = NA) %>% 
  mutate(treatment = factor(treatment, 
                            levels=as.character(1:4),
                            labels = labels)) %>% 
  group_by(treatment) %>% 
  mean_qi(.epred)
```

---

We'll add the average chimp to the plot. 

```{r}
#| code-fold: true

f %>% 
  ggplot( aes(x=treatment, y=.epred, group=1) ) +
  geom_ribbon(aes( ymin=.lower, ymax=.upper ), 
              fill = "#0f393a",
              alpha=.3) +
  geom_line(color="#0f393a") +
  geom_ribbon(aes( ymin=.lower, ymax=.upper ), 
              data=f_avg,
              fill = "#e07a5f",
              alpha=.3) +
  geom_line(color="#e07a5f", data=f_avg) +
  scale_y_continuous(limits=c(0,1)) +
  facet_wrap(~actor)
```

---

But the average chimp is only one possible chimp we could encounter. Let's simulate 100 possible chimps.

```{r}
#| code-fold: true

nd = expand_grid( actor= max(d$actor) + 1,
                  treatment = as.factor(1:4),
                  block=1)

add_epred_draws(nd, m3, allow_new_levels=T) %>% 
  filter(.draw <= 100) %>% 
  mutate(treatment = factor(treatment,
                            levels=as.character(1:4),
                            labels = labels)) %>%
  ggplot(aes(x = treatment, y = .epred, group = .draw)) +
  geom_line(alpha = 1/2, color = "#e07a5f") +
  coord_cartesian(ylim = 0:1)
```

---

### exercise

Returning to the Trolley data and the varying intercept model, get predictions for...

1. a subset of 3 participants in the dataset. 
2. the average participant.
3. 2 new participants.

Hint: don't forget that the model uses a link function. You may need to play with arguments or fiddle around with the outputs of your functions. 

---

### solution

3 participants

```{r}
#| code-fold: true

part3 = sample( unique(Trolley$id) , size=3, replace=F )
nd <- distinct(Trolley, action, intention, contact, id) %>% 
  filter(id %in% part3)

f <- add_epred_draws(newdata = nd, 
                     m_varying, 
                     scale = "response") 

f %>% 
  ggplot(aes(x=.category, y=.epred, color=as.factor(intention))) +
  stat_halfeye() +
  labs(y="p", color = "intention") +
  facet_grid(action+contact~id) +
  theme(legend.position = "top")
```

---

### solution

The average participant 

```{r}
#| code-fold: true

nd <- distinct(Trolley, action, intention, contact) 

f <- add_epred_draws(newdata = nd, m_varying, scale = "response", 
            re_formula = NA) 

f %>% 
  ggplot(aes(x=.category, y=.epred, color=as.factor(intention))) +
  stat_halfeye() +
  labs(y="p", color = "intention") +
  facet_grid(action~contact) +
  theme(legend.position = "top")
```

---

Two new participants

```{r}
#| code-fold: true
# create data for 2 new participants
nd <- distinct(Trolley, action, intention, contact) %>%
  slice(rep(1:n(), times = 2)) %>%
  mutate(id = rep(c("New1", "New2"), each = n()/2))

# get predictions including random effects
f <- add_epred_draws(m_varying, newdata = nd, 
            scale = "response", allow_new_levels=T) %>% 
  filter(.draw==1)

# plot
f %>% 
  ggplot(aes(x=.category, y=.epred, fill=as.factor(intention))) +
  geom_bar(stat="identity", position="dodge") +
  labs(y="p", fill="intention") +
  facet_grid(action+contact~id) +
  theme(legend.position = "bottom")
```

