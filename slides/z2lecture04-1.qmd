---
title: "Week 4: Overfitting/MCMC"
subtitle: "Overfitting"
format: 
  revealjs:
    css: xaringan-themer2.css
    nature:
      highlightStyle: solarized-dark
      highlightLines: true
      countIncrementalSlides: false
      mathjax: "default"
    self-contained: false  # Ensures correct embedding
    embed-resources: true  # Embeds required assets
    slide-number: true
execute:
  echo: false  
---

```{r, message = F, warning = F}
library(tidyverse)
library(cowplot)
library(brms)
library(tidybayes)
library(patchwork)
library(here)
```

```{r, echo = F}
knitr::opts_chunk$set(fig.retina=3, echo=TRUE)
theme_set(theme_cowplot())
default_palettes <- list(
  c("#5e8485" , "#0f393a") ,
  c("#1c5253" , "#5e8485" , "#0f393a") , 
  # palette with 5 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" ) ,
  # same palette interpolated to 8 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" , "#a7a844" , "#69306d" ) 
  
)

options(ggplot2.discrete.fill = default_palettes, 
        ggplot2.discrete.colour = default_palettes)
```

------------------------------------------------------------------------

A central tension in our modeling is the one between explanation -- good causal models -- and prediction. In McElreath's lecture, he leads us to the intuition that predictive models are generally those that do a terrible job of representing the causal model. So the tools covered in this lecture should be considered tools for prediction, but *not* for identifying causal models.

When trying to maximize prediction, we need to be wary of **OVERFITTING** -- when the model learns too much from the sample. Methods for avoiding overfitting favor simpler models. However, we must also be wary of **UNDERFITTING** or learning too little. 

There are two common famililes of approches:

  1. Use of a **REGULARIZING** prior, which helps stop the model from becoming too excited about any one data point. 
  
  2. Use of a scoring device, like **INFORMATION CRITERIA** and **CROSS-VALIDATION**, to estimate predictive accuracy. 

------------------------------------------------------------------------


### the problem with parameters

```{r, out.width='100%'}
#| code-fold: true

sppnames <- c( "afarensis","africanus","habilis","boisei","rudolfensis","ergaster","sapiens")
brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )
d <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )
base = d %>% 
  ggplot(aes(x=masskg, y=brainvolcc)) +
  geom_point() +
  geom_text(aes(label=sppnames), hjust=0, nudge_x = 1) +
  labs(x="body mass (kg)", y="brain volume (cc)")
p1 = base + geom_smooth(method='lm', se =F) +ggtitle("Simple linear model")
p2 = base + geom_smooth(method='lm', se =F, formula=y~poly(x, 6)) +ggtitle("6th degree polynomial")
(p1 | p2)
```

If your goal is simply prediction, not (causal) explanation, why not simply add as many variables into the model as possible? 

  * Adding parameters nearly always increases measures of model fit, like $R^2$.
  * More complex models fit the data in-hand better, but often predict new data worse. 

::: notes
models = data compression when there are as many parameters as data points, we haven't compressed the data. we've just encoded the raw data into a different form.
:::

------------------------------------------------------------------------

```{r, fig.width=10, fig.height=8, warning=FALSE, message=FALSE}
#| code-fold: true
#| 
d$mass_std <- rethinking::standardize(d$mass)
d$brain_std <- d$brain / max(d$brain)

m1 <- brm(
  data=d,
  family=gaussian,
  brain_std ~ mass_std,
  prior = c( prior(normal(.5, 1), class=Intercept),
             prior(normal(0, 10), class=b),
             prior(exponential(1), class=sigma)),
  iter=2000, warmup=1000, seed=5, chains=1,
  file=here("files/models/41.1"))

m2 <- 
  update(m1,
         newdata = d, 
         brain_std ~ 1 + mass_std + I(mass_std^2) + 
           I(mass_std^3) + I(mass_std^4) + I(mass_std^5),
         iter=2000, warmup=1000, chains=1, seed = 5,
         control = list(adapt_delta = .99999),
         file = here("files/models/41.2"))

# define the new data 
nd <- tibble(mass_std = seq(from = -2, to = 2, length.out = 200))


brain_loo_lines <- function(brms_fit, row, ...) {
  
  # refit the model
  new_fit <- 
    update(brms_fit,
           newdata = filter(d, row_number() != row), 
           iter = 2000, warmup = 1000, chains = 4, cores = 4,
           seed = 7,
           refresh = 0,
           ...)
  
  # pull the lines values
  fitted(new_fit, 
         newdata = nd) %>% 
    data.frame() %>% 
    select(Estimate) %>% 
    bind_cols(nd)
  
}

m1_fits <-
  tibble(row = 1:7) %>% 
  mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = m1, row = .))) %>% 
  unnest(post)

m2_fits <-
  tibble(row = 1:7) %>% 
  mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = m2, 
                                                 row = ., 
                                                 control = list(adapt_delta = .999)))) %>% 
  unnest(post)

# left
p1 <-
  m1_fits %>%  
  
  ggplot(aes(x = mass_std)) +
  geom_line(aes(y = Estimate, group = row),
            color = "#1c5253", linewidth = 1/2, alpha = 1/2) +
  geom_point(data = d,
             aes(y = brain_std),
             color = "#1c5253") +
  labs(subtitle = "b7.1",
       x = "body mass (std)",
       y = "brain volume (std)") +
  coord_cartesian(xlim = range(d$mass_std),
                  ylim = range(d$brain_std))

# right
p2 <-
  m2_fits %>%  
  
  ggplot(aes(x = mass_std, y = Estimate)) +
  geom_line(aes(group = row),
            color = "#1c5253", linewidth = 1/2, alpha = 1/2) +
  geom_point(data = d,
             aes(y = brain_std),
             color = "#1c5253") +
  labs(subtitle = "b7.4",
       x = "body mass (std)",
       y = "brain volume (std)") +
  coord_cartesian(xlim = range(d$mass_std),
                  ylim = c(-0.1, 1.4))
p1 + p2
```


------------------------------------------------------------------------

::::: columns
::: {.column width="50%"}

```{r, fig.height = 9}
#| code-fold: true

# Set up plotting area
plot(0, 0, type = "n", xlim = c(-3, 3), ylim = c(0, 2), 
     xlab = "parameter value", ylab = "Density",
     main = "")

# Create x-values for plotting
x <- seq(-3, 3, length.out = 1000)

# Generate three density curves with different spreads
# Thick curve - very peaked (high kurtosis)
y1 <- dnorm(x, mean = 0, sd = 0.3)
# Scale to match the peak height in the original
y1 <- y1 * (2/max(y1))

# Medium curve - moderate spread
y2 <- dnorm(x, mean = 0, sd = 0.6)
# Scale to match the peak height in the original
y2 <- y2 * (0.8/max(y2))

# Dashed curve - most spread (normal distribution)
y3 <- dnorm(x, mean = 0, sd = 1)
# Scale to match the peak height in the original
y3 <- y3 * (0.4/max(y3))

# Add the curves to the plot
lines(x, y1, lwd = 3)
lines(x, y2, lwd = 1)
lines(x, y3, lwd = 1, lty = 2)
```
:::

::: {.column width="50%"}

One tool in our toolbelt is regularizing priors. **REGULARIZATION** is a means by which you prevent the model from being "too excited" by the training sample, or to fit too closely to the specific patterns in that sample. There are many tools for regularization (ridge regression, lasso regression), but they all have the effect of downweighting regression parameters towards 0. 

As a Bayesian, you also have the tool of regularizing priors. As your priors become more "skeptical" (usually, closer to 0), your model will adapt less to the data. Be wary of having priors that are too tight, because you risk underfitting. (Of course, the more data you have, the less influence your priors have. But that shouldn't concern you, because overfitting is less of a concern with larger datasets.)

:::
:::::

---

This is a pretty extreme example of setting a skeptical prior, but it demonstrates the point. Of course, we don't want our priors to be too skeptical, or we'll miss important (and regular) information. So we'll also make use of information theory to derive indicies that estimate out-of-sample prediction. Let's refer to these indicies as Model Performance Criteria.

::::: columns
::: {.column width="50%"}

```{r}
#| code-line-numbers: "7"
m3a <- brm(
  data=d,
  family=gaussian,
  brain_std ~ 1 + mass_std + I(mass_std^2) + 
           I(mass_std^3) + I(mass_std^4) + I(mass_std^5) + I(mass_std^6), 
  prior = c( prior(normal(.5, 1), class=Intercept),
             prior(normal(0, 10), class=b),
             prior(exponential(1), class=sigma)),
  iter=2000, warmup=1000, seed=5, chains=1,
  file=here("files/models/41.3a"))

```

:::

::: {.column width="50%"}

```{r}
#| code-line-numbers: "7"
m3b <- brm(
  data=d,
  family=gaussian,
  brain_std ~ 1 + mass_std + I(mass_std^2) + 
           I(mass_std^3) + I(mass_std^4) + I(mass_std^5) + I(mass_std^6), 
  prior = c( prior(normal(.5, 1), class=Intercept),
             prior(normal(0, .05), class=b),
             prior(exponential(1), class=sigma)),
  iter=2000, warmup=1000, seed=5, chains=1,
  file=here("files/models/41.3b"))
```

:::
:::::

---

```{r}
#| code-fold: true
nd = data.frame(mass_std = seq(min(d$mass_std), max(d$mass_std), length.out=100))
pred_3a = add_epred_draws(nd, m3a) %>% filter(.draw <= 20)
pred_3b = add_epred_draws(nd, m3b) %>% filter(.draw <= 20)

p1 = pred_3a %>% ggplot(aes(x = mass_std, y=.epred)) +
  geom_line(aes(group=.draw), alpha=.3) +
  geom_point(aes(y=brain_std), data=d, color = "#1c5253") +
  lims(y=c(-2,4)) +
  labs(y="Brain size (standardized)")
p2 = pred_3b %>% ggplot(aes(x = mass_std, y=.epred)) +
  geom_line(aes(group=.draw), alpha=.3) +
  geom_point(aes(y=brain_std), data=d, color = "#1c5253") +
  lims(y=c(-2,4)) +
  labs(y="Brain size (standardized)")

p1 + p2
```


----

### the path to model performance criteria

1.  establish a measurement scale for the distance from perfect accuracy
    -   need to discuss information theory
2.  establish *deviance* as an approximation of relative distance from accuracy
3.  establish that we only care about *out-of-sample* deviance

First: establishing a measurement scale. The two major dimensions to consider are:

-   cost-benefit analysis

    -   how much does it cost when we are wrong?

    -   how much do we win when we're right?

-   accuracy in context

    -   judging accuracy in a way that accounts for how *much* a model could possibly improve prediction

------------------------------------------------------------------------

### clash of the weatherpeople

| Day | Current Weatherman | New Weatherman | Outcome |
|-----|:------------------:|:--------------:|:-------:|
| 1   |        1.0         |      0.0       |  rain   |
| 2   |        1.0         |      0.0       |  rain   |
| 3   |        1.0         |      0.0       |  rain   |
| 4   |        0.6         |      0.0       |   sun   |
| 5   |        0.6         |      0.0       |   sun   |
| 6   |        0.6         |      0.0       |   sun   |
| 7   |        0.6         |      0.0       |   sun   |
| 8   |        0.6         |      0.0       |   sun   |
| 9   |        0.6         |      0.0       |   sun   |
| 10  |        0.6         |      0.0       |   sun   |

::: fragment
If accuracy is the chance of a correct prediction:

$\text{Current} = [(3 \times 1) + (.4 \times 7)]/10 = .58$

$\text{New} = [(3 \times 0) + (1 \times 7) ]/10= .70$
:::

------------------------------------------------------------------------

### clash of the weatherpeople

| Day | Current Weatherman | New Weatherman | Outcome |
|-----|:------------------:|:--------------:|:-------:|
| 1   |        1.0         |      0.0       |  rain   |
| 2   |        1.0         |      0.0       |  rain   |
| 3   |        1.0         |      0.0       |  rain   |
| 4   |        0.6         |      0.0       |   sun   |
| 5   |        0.6         |      0.0       |   sun   |
| 6   |        0.6         |      0.0       |   sun   |
| 7   |        0.6         |      0.0       |   sun   |
| 8   |        0.6         |      0.0       |   sun   |
| 9   |        0.6         |      0.0       |   sun   |
| 10  |        0.6         |      0.0       |   sun   |

But what if you hate carrying an umbrella? Let's define the *cost* of getting wet as -5 happiness points and the cost of carrying an umbrella as -1 happiness points. Which weatherperson maximizes our happiness??

$\text{Current} = (3 \times -1) + (7 \times -0.6) = -7.2$

$\text{New} = (3 \times -5) + (7 \times 0) = -150$

------------------------------------------------------------------------

### clash of the weatherpeople

| Day | Current Weatherman | New Weatherman | Outcome |
|-----|:------------------:|:--------------:|:-------:|
| 1   |        1.0         |      0.0       |  rain   |
| 2   |        1.0         |      0.0       |  rain   |
| 3   |        1.0         |      0.0       |  rain   |
| 4   |        0.6         |      0.0       |   sun   |
| 5   |        0.6         |      0.0       |   sun   |
| 6   |        0.6         |      0.0       |   sun   |
| 7   |        0.6         |      0.0       |   sun   |
| 8   |        0.6         |      0.0       |   sun   |
| 9   |        0.6         |      0.0       |   sun   |
| 10  |        0.6         |      0.0       |   sun   |

Finally, our previous measure of "accuracy" (we used a **HIT RATE** definition) is only one way to think about accuracy. What if accuracy is knowing the true data generating model? We might consider computing the probability of predicting the exact sequence of days (joint likelihood in Bayesian terms).

$\text{Current} = (1)^3 \times (0.4)^7 \approx .005$

$\text{New} = (0)^3 \times (1)^7 = 0$

This method -- **LOG SCORING RULE**, or the logarithm of the joint probability -- is the way we'll think about accuracy in terms of model prediction.

------------------------------------------------------------------------

### information theory

How do we measure distance from perfect prediction? One important thing to keep in mind is that some targets are easier to hit than others. Therefore, the key to measuring distance is to ask, "How much is our uncertainty reduced by learning an outcome?" This reduction is formally referred to as **INFORMATION**.

We need to formalize our measure of uncertainty. This measurement should:

1.  be continuous;
2.  increase as the number of possible events increases; and
3.  be additive.

This is satisfied by the **INFORMATION ENTROPY FUNCTION**. If there are $n$ different possible events and each event $i$ has probability $p_i$, and we call the list of probabilities $p$, then the unique measure of uncertainty we seek is[^1]:

$$
H(p) = - \text{E log}(p_i) = - \sum^n_{i=1}p_i\text{log}(p_i)
$$ 

In words: The uncertainty contained in a probability distribution is the average log-probability of an event.

[^1]: $E$ is mathematical notation for "expected value" or average. 

---

```{r, echo = F, fig.height = 10, fig.width =15}
# Function to calculate entropy: H(p) = -sum(p_i * log2(p_i))
calculate_entropy <- function(probabilities) {
  # Handle zero probabilities (log(0) is undefined)
  probabilities <- probabilities[probabilities > 0]
  -sum(probabilities * log2(probabilities))
}

# Create two distributions
# 1. High entropy: Uniform distribution (maximum entropy)
high_entropy_dist <- rep(1/10, 10)  # 10 equally likely outcomes

# 2. Low entropy: Highly skewed distribution
low_entropy_dist <- c(0.91, rep(0.01, 9))  # One highly likely outcome, others unlikely

# Calculate entropy values
high_entropy_value <- calculate_entropy(high_entropy_dist)
low_entropy_value <- calculate_entropy(low_entropy_dist)

# Prepare data for plotting
high_entropy_df <- data.frame(
  Outcome = 1:10,
  Probability = high_entropy_dist,
  Distribution = "High Entropy"
)

low_entropy_df <- data.frame(
  Outcome = 1:10,
  Probability = low_entropy_dist,
  Distribution = "Low Entropy"
)

# Combine data frames
plot_data <- rbind(high_entropy_df, low_entropy_df)

# Create labels with entropy values
plot_labels <- c(
  "High Entropy" = paste("High Entropy: H =", round(high_entropy_value, 3)),
  "Low Entropy" = paste("Low Entropy: H =", round(low_entropy_value, 3))
)

# Plot both distributions
ggplot(plot_data, aes(x = factor(Outcome), y = Probability, fill = Distribution)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ Distribution, scales = "free_y", labeller = labeller(Distribution = plot_labels)) +
  labs(
    #title = "Comparison of High and Low Entropy Distributions",
    x = "Outcome",
    y = "Probability",
    fill = "Distribution Type"
  ) +
  guides(fill = FALSE) +
  theme_cowplot(font_size = 20) +
  theme(
    legend.position = "bottom",
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.x = element_text(angle = 0)
  )
```


------------------------------------------------------------------------

Consider the weather forecast. When the day arrives, the weather is no longer uncertain.[^lecture04-1-1] There were 3 rainy days and 3 sunny days. Therefore:

```{r}
p <- c( .3, .7 )
-sum( p * log(p) )
```

This is our uncertainty.

If we lived in Abu Dhabi, we might have a different probability of rain (let's say .01), and therefore a different amount of uncertainty.

```{r}
p <- c( .01, .99 )
-sum( p * log(p) )
```

------------------------------------------------------------------------

### from entropy to accuracy

We define **DIVERGENCE** to be the additional uncertainty induced by using probabilities from one distribution to describe another distribution. This is known as the Kullback-Leibler divergence or simply KL divergence.

The divergence between a target $(p)$ and a model $(q)$ is defined as:

$$
D_{KL}(p,q) = \sum_ip_i(\text{log}(p_i) - \text{log}(q_i)) = \sum_ip_i\text{log}(\frac{p_i}{q_i})
$$ 

In plainer language, the divergence is the average difference in log probability between the target and model, or the difference in entropies.

Suppose, for example, the truth is that it rains 30% of the time. If our weatherman believes that it rains 25% of the time, how much additional uncertainty is introduced as a consequence of using the weatherman's prediction to approximate the true weather?

$$
D_{KL}(p,q) = \sum_ip_i(\text{log}(p_i)-\text{log}(q_i)) = \sum_ip_i\text{log}(\frac{p_i}{q_i})
$$
```{r}
p = c(.3, .7)
q = c(.25, .75)
sum(p*log(p/q))
```

What is the divergence when we get the model exactly correct with the target?


::: notes
difference = 0
:::

---

At this point, you're probably thinking, "How can I put this into practice? Divergence is measuring distance from truth, but I don't know the truth." But we won't use divergence to estimate the difference from one model to truth; we're only interested in using it to compare two models (q and r) to each other. All we need to know are each model's average log probability: $\text{E log}(q_i)$ and $\text{E log}(r_i)$. 

\begin{align*}
D_{KL}(p,q) - D_{KL}(p,r) &= \sum_ip_i(\text{log}(p_i)-\text{log}(q_i)) - \sum_ip_i(\text{log}(p_i)-\text{log}(r_i))  \\
&= \sum_ip_i\text{log}(p_i) - \sum_ip_i\text{log}(q_i) - \sum_ip_i\text{log}(p_i) + \sum_ip_i\text{log}(r_i) \\
&= \sum_ip_i\text{log}(p_i) - \sum_ip_i\text{log}(p_i) - \sum_ip_i\text{log}(q_i) + \sum_ip_i\text{log}(r_i) \\
&= \sum_ip_i\text{log}(r_i) - \sum_ip_i\text{log}(q_i) \\
&= E[\text{log}(r_i)] - E[\text{log}(q_i)]
\end{align*}

To do so, we simply sum over all the observations, $i$, yielding a total score for each model:

$$
S(q) = \sum_i\text{log}(q_i) 
$$


------------------------------------------------------------------------

To compute this score for a Bayesian model, we need to find the log of the average probability for each observation i, where the average is taken over the posterior distribution. The score is known as the **LOG-POINTWISE-PREDICTIVE-DENSITY**.

Let's see an example:

```{r}
m1 <- brm(
  data=d,
  family=gaussian,
  brain_std ~ mass_std,
  prior = c( prior(normal(.5, 1), class=Intercept),
             prior(normal(0, 10), class=b),
             prior(exponential(1), class=sigma)),
  iter=2000, warmup=1000, seed=5, chains=1,
  file=here("files/models/41.1"))
```

---

First, we get the log-probabilities:

```{r}
log_prob = log_lik(m1)
glimpse(log_prob)
```

```{r}
prob <- as.data.frame(log_prob) %>% 
  set_names(pull(d, species)) %>%  # add case names, for convenience
  mutate(s = 1:n()) %>% # add an s iteration index, for convenience
  # make it long
  pivot_longer(-s,
               names_to = "species",
               values_to = "logprob") %>% 
  # compute the probability scores
  mutate(prob = exp(logprob))

prob
```

---

```{r}
prob <- prob %>% 
  group_by(species) %>% 
  summarise(log_probability_score = mean(prob) %>% log())

prob
```

log-pointwise-predictive-density for the entire model is just the sum of these:

```{r}
prob %>% 
  summarise(total_log_probability_score = sum(log_probability_score))
```

Sometimes, you'll see people report deviance, which is just this value multiplied by -2 (for historical reasons).

```{r}
-2*sum(prob$log_probability_score)
```

---

Here's the work in a custom function for your convenience.

```{r}
lppd <- function(brms_fit) {
  
  log_lik(brms_fit) %>% 
    data.frame() %>% 
    pivot_longer(everything(),
                 values_to = "logprob") %>% 
    mutate(prob = exp(logprob)) %>% 
    group_by(name) %>% 
    summarise(log_probability_score = mean(prob) %>% log()) %>% 
    summarise(total_log_probability_score = sum(log_probability_score))
  
}
```

------------------------------------------------------------------------

One issue with the log-probability score is that it always improves as the model gets more complex. One way to address this is by calculating the log-probability out-of-sample.

When we usually have data and use it to fit a statistical model, the data comprise a **TRAINING SAMPLE**. Parameters are estimated from it, and then we can imagine using those estimates to predict outcomes in a new sample, called the **TEST SAMPLE**.

Using out-of-sample prediction doesn't change your model; rather, it changes our estimation of the predictive accuracy of our model.


### cross-validation

One strategy for estimating predictive accuracy is to actually test the model’s predictive accuracy on another sample. This is known as **CROSS-VALIDATION**, leaving out a small chunk of observations from our sample and evaluating the model on the observations that were left out. 

We're not actually going to leave out data -- imagine! -- so instead we'll divide the data into chunks, or "folds". The model will then predict each fold after being trained on all the other folds. This is known as k-fold validation. 

```{r, out.width="200%", fig.width = 8, fig.height=4}
#| code-fold: true
# Create data for k-fold cross validation visualization
k <- 5  # Number of folds
blocks <- 5  # Number of data blocks per fold
fold_data <- data.frame()

# Generate data for each fold
for (i in 1:k) {
  # For each fold, create a row with blocks indicating training or test
  fold_row <- data.frame(
    fold = paste("Fold", i),
    block = 1:blocks,
    type = rep("Training", blocks)
  )
  
  # Set one block as test data (different block for each fold)
  fold_row$type[i] <- "Test"
  
  # Add to the overall data
  fold_data <- rbind(fold_data, fold_row)
}

# Convert fold to factor to preserve order
fold_data$fold <- factor(fold_data$fold, levels = unique(fold_data$fold))

# Create the visualization
k_fold_plot <- ggplot(fold_data, aes(x = block, y = fold, fill = type)) +
  geom_tile(color = "white", linewidth = 0.5) +
  scale_fill_manual(values = c("Training" = "#1c5253", "Test" = "#e07a5f")) +
  scale_y_discrete(breaks = NULL) +
  labs(
    #title = "K-Fold Cross Validation (k=5)",
    fill = "Data Usage",
    x = "Data Blocks",
    y = ""
  ) +
  facet_wrap(fold ~ ., nrow = 5, scale = "free") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks = element_blank()
  )

# Display the plot
print(k_fold_plot)
```

The minimum number of folds is 2, and the maximum is your sample size. The latter is referred to as **LEAVE-ONE-OUT-CROSS-VALIDATION**, and this is extremely common. 

---

The problem with LOOCV is that it's computationally intensive. Luckily, there are some clever maths that we can use to approximate the score we would get from running the model over and over. One approach is to use the _importance_ (or weight) of each observation -- that is, how much does the prior distribution change if we were to remove this observation from our data? (Similar to influence and leverage.) Importantly, observations that are less likely are more important. 

We can use importance in a strategy called **PARETO-SMOOTHED IMPORTANCE SAMPLING CROSS-VALIDATION** (PSIS) ([see this paper](https://arxiv.org/pdf/1507.02646)). The Pareto part is a smoothing technique that improves the reliability of the importance or weights. By assuming the weights follow a known distribution, the Pareto distribution, we can estimate a reliabile cross-validation score without doing the work of actually cross-validating. We'll get a PSIS score for each observation in our dataset, as well as a standard error for the score for the entire model. 

```{r}
library(loo)
loo(m1)
```

The value of PSIS is reliable when $k$ is less than .5, ok when $k$ is between .5 and 7., bad when $k$ is between .7 and 1, very bad when $k$ is greater than 1. 

---

A second approach is to use the information criteria to computed the expected score out-of-sample. If you look back at the training/testing figure, you'll find that the difference between training deviance and testing deviance is almost exactly twice the number of parameters in the model (e.g., 2 for the first model with 1 parameter and about 10 for the last with 5 parameters). This is not random, but a known finding in machine learning. We can exploit this for simple estimates of out-of-sample deviance. 

A well-known estimate is the **AKAIKE INFORMATION CRITERION (AIC)**:

\begin{align*}
AIC &= D_{\text{train}} + 2p \\
&= -2\text{lppd} + 2p
\end{align*}

where $D$ is the divergence and $p$ is the number of free parameters in the posterior distribution. As the 2 is just there for scaling, what AIC tells us is that the dimensionality of the posterior distribution is a natural measure of the model’s overfitting tendency. More complex models tend to overfit more, directly in proportion to the number of parameters.

AIC isn't commonly used now. Its approximation is only reliable when:
  
  1. Priors are flat or overwhelmed by likelihood (data).
  2. The posterior distribution is approximately multivariate Gaussian.
  3. The sample size $N$ is much greater than the number of parameters $k$.
  
Similarly the **DEVIANCE INFORMATION CRITERION (DIC)** doesn't assume flat priors but does make the other assumptions. 
  
---

### widely applicable

Watanabe's **WIDELY APPLICABLE INFORMATION CRITERION (WAIC)** makes no assumption about the shape of the posterior. Its goal is to guess the out-of-sample KL divergence. In a large sample, the approximation converges to the cross-validation approximation, but in finite samples, there may be some disagreement. 

Its calculation is the log-posterior-predictive-density plus a penalty proportional to the variance in the posterior predictions:

$$
\text{WAIC}(y, \Theta) = -2(\text{lppd} - \sum_i\text{var}_{\theta}\text{log}p(y_i|\Theta))
$$
where $y$ is the observations and $\Theta$ is the posterior distribution. The penalty term means, "compute the variance in log-probabilities for each observation $i$, and then sum up these variances to get the total penalty."

Like PSIS, WAIC is pointwise, meaning prediction is considered on a point-by-point basis. Therefore,

  1. WAIC has an approximate standard error. 
  2. Some observations have stronger influence than others, and we can see this.
  3. it can be hard to define for a single model. Consider for example a model in which each prediction depends upon a previous observation. This happens, for example, in a time series. In a time series, a previous observation becomes a predictor variable for the next observation. So it’s not easy to think of each observation as independent or exchangeable. In such a case, you can of course compute WAIC as if each observation were independent of the others, but it’s not clear what the resulting value means.
  
---

```{r, warning=F}
m1_waic = waic(m1)
m1_waic
m1_waic$pointwise
```

---

## comparing PSIS and WAIC

PSIS and WAIC perform very similarly in the context of ordinary linear models. Of course, they may not when our posterior distributions start to get away from Gaussian or when there are highly influential observations. 

PSIS have higher variance as estimators of the KL divergence, while WAIC has greater bias. So we should expect each to be slightly better in different contexts. However, in practice any advantage may be much smaller than the expected error. Watanabe recommends computing both WAIC and PSIS and contrasting them. If there are large differences, this implies one or both criteria are unreliable.

PSIS has a distinct advantage in warning the user about when it is unreliable. The $k$ values that PSIS computes for each observation indicate when the PSIS score may be unreliable, as well as identify which observations are at fault. We’ll see later how useful this can be.

```{r}

m1 <- add_criterion(m1, criterion = c("loo", "waic"))
m2 <- add_criterion(m2, criterion = c("loo", "waic"))

loo_compare( m1, m2, criterion="loo")
loo_compare( m1, m2, criterion="waic")
```

:::{.notes}
The filled points are the in-sample deviance values. The open points are the WAIC values. Notice that naturally each model does better in-sample than it is expected to do out-of-sample. The line segments show the standard error of each WAIC. These are the values in the column labeled SE in the table above. 
:::

---

### bic?

The **Bayesian Information Criterion (BIC)**, is frequently compared with the Akaike Information Criterion (AIC). It's important to understand that choosing between these criteria isn't fundamentally about adopting a Bayesian perspective. Both criteria can be derived through either Bayesian or non-Bayesian approaches, and strictly speaking, neither is purely Bayesian.

BIC is mathematically connected to the logarithm of a linear model's average likelihood. In Bayesian statistics, this average likelihood serves as the denominator in Bayes' theorem—essentially the likelihood averaged across the prior distribution. Comparing average likelihoods has long been a standard method for model comparison in Bayesian analysis. These comparisons yield what we call "Bayes factors" when expressed as ratios. When transformed to a logarithmic scale, these ratios become differences, making them conceptually similar to comparing information criteria differences.

Since the average likelihood incorporates the prior distribution, models with more parameters naturally incur a complexity penalty. This helps prevent overfitting, although the exact penalty mechanism differs from that of information criteria.
Many Bayesian statisticians have reservations about Bayes factors, and all acknowledge certain technical challenges. One significant obstacle is computational difficulty—calculating average likelihood is often complex. Even when posterior distributions can be successfully computed, estimating average likelihood may remain problematic. Another issue is that while weak priors might minimally impact posterior distributions within individual models, they can dramatically influence comparisons between different models.

The choice between Bayesian and non-Bayesian approaches doesn't dictate whether to use information criteria or Bayes factors. In practice, there's value in using both methods and examining where they align and diverge. However, remember that both information criteria and Bayes factors are purely predictive tools that will readily select confounded models without considering causation.

---

### mid-lecture review

Regularizing priors—priors which are skeptical of extreme parameter values—reduce fit to sample but tend to improve predictive accuracy.

How do we choose between several plausible models when seeking to maximize accuracy?

  * Calculate a measure of information divergence.
  * Comparing accuracy within-sample is bad: we'll always favor more complex models.
  
We can estimate out-of-sample accuracy with any of a number of techniques, but most popularly: 

  * pareto-smoothed importance sampling cross-validation (PSIS)
  * widely applicable information criteria (WAIC)
  
Regularization and predictive criteria are complementary. 

Let's actually calculate some things. 


