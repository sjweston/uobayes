---
title: "week 7: multilevel models"
subtitle: "multilevel adventures"
format: 
  revealjs:
    css: xaringan-themer2.css
    nature:
      highlightStyle: solarized-dark
      highlightLines: true
      countIncrementalSlides: false
      mathjax: "default"
    self-contained: false  # Ensures correct embedding
    embed-resources: true  # Embeds required assets
    slide-number: true
    code-annotations: hover
execute:
  echo: false        
---

```{r, message = F, warning = F}
library(tidyverse)
library(psych)
library(cowplot)
library(patchwork)
library(here)
library(brms) 
library(tidybayes) 
```

```{r, echo = F}
knitr::opts_chunk$set(fig.retina=3, echo=TRUE)
theme_set(theme_cowplot())
default_palettes <- list(
  c("#5e8485" , "#0f393a") ,
  c("#1c5253" , "#5e8485" , "#0f393a") , 
  # palette with 5 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" ) ,
  # same palette interpolated to 8 colours
 c( "#1c5253" , "#e07a5f", "#f2cc8f" , "#81b29a" , "#3d405b" , "#a7a844" , "#69306d" ) 
  
)

options(ggplot2.discrete.fill = default_palettes, 
        ggplot2.discrete.colour = default_palettes)
```

---

## multilevel tadpoles

```{r}
data(reedfrogs, package = "rethinking")
d <- reedfrogs
dim(d)
d %>% sample_n(10)
```

---

Let's start with the unpooled model. Up to this point in the course, this would be a good model to use to estimate survival in each of the tanks. 

\begin{align*}
\text{surv}_i &\sim \text{Binomial}(n_i,p_i) \\
\text{logit}(p_i) &= \alpha_{\text{tank}[i]} \\
\alpha_j &\sim \text{Normal}(0, 1.5) \text{ for }j=1,...,48
\end{align*}

```{r}
d$tank = factor(1:nrow(d))

m1 <- 
  brm(data = d, 
      family = binomial,
      bf(surv | trials(density) ~ 0 + alpha, 
         alpha ~ 0 + tank, 
         nl = TRUE),
      prior(normal(0, 1.5), class = b, nlpar=alpha),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 13,
      file = here("files/models/71.1"))
```

---

```{r}
print(m1)
```

---

```{r}
#| code-fold: true
# get posterior
post <- as_draws_df(m1)
# transform logit to probability
p1 <- post %>% 
  pivot_longer(starts_with("b_alpha"),
               names_prefix = "b_alpha_tank",
               values_to = "logit") %>% 
  mutate(prob = logistic(logit),
         tank = as.numeric(name)) %>% 
  ggplot( aes (x = tank, y = prob)) +
  stat_gradientinterval(alpha = .3, color="#5e8485") +
  geom_point( aes(x=as.numeric(tank), y=propsurv),
              data=d) +
  labs(title = "Unpooled model")
p1
```

---

Now let's build up the pooled (multilevel) model and see how it compares. 

\begin{align*}
\text{surv}_i &\sim \text{Binomial}(n_i,p_i) \\
\text{logit}(p_i) &= \alpha_{\text{tank}[i]} \\
\alpha_j &\sim \text{Normal}(\bar{\alpha}, \sigma) \\
\bar{\alpha} &\sim \text{Normal}(0, 1.5) \\
\sigma &\sim \text{Exponential}(1)
\end{align*}

```{r}
m2 <- 
  brm(data = d, 
      family = binomial,
      surv | trials(density) ~ 1 + (1 | tank),
      prior = c(prior(normal(0, 1.5), class = Intercept),  # alpha bar
                prior(exponential(1), class = sd)),        # sigma
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      sample_prior = "yes",
      seed = 13,
      file = here("files/models/71.2"))
```

---

```{r}
print(m2)
```

---

```{r}
posterior_summary(m2) %>% round(2)
```

---

The "intercepts" for each tank are actually the distance of that tank's intercept from the grand mean. 

```{r}
gather_draws(m2, r_tank[tank, ]) %>% 
  mean_qi() %>% 
  ggplot(aes( y=tank, x=.value )) +
  geom_point() +
  geom_errorbar( aes(xmin=.lower, xmax=.upper ), alpha=.5) +
  geom_vline(xintercept = 0) +
  labs(x="Varying intercepts")
```


---

```{r}
#| code-fold: true
m1 <- add_criterion(m1, "waic")
m2 <- add_criterion(m2, "waic")

w <- loo_compare(m1, m2, criterion = "waic")

print(w, simplify = F)
```

---

```{r}
#| code-fold: true
#average survival
post_sum = posterior_summary(m2)
average_surv = post_sum["b_Intercept", "Estimate"] 
p2 <- gather_draws(m2, r_tank[tank, Intercept]) %>% 
  mutate(prob = logistic(.value+average_surv)) %>% 
  ggplot( aes (x = tank, y = prob)) +
  stat_gradientinterval(alpha = .3, color="#5e8485") +
  geom_point( aes(x=as.numeric(tank), y=propsurv),
              data=d ) +
  geom_hline( aes(yintercept = logistic(average_surv)),
              linetype = "dashed")+
  labs(title = "Partial pooling model")

p2
```

---

```{r, echo = F, out.height="100%"}
p1 / p2
```

---

## divergent transitions

From McElreath:

> Recall that HMC simulates the frictionless flow of a particle on a surface. In any given transition, which is just a single flick of the particle, the total energy at the start should be equal to the total energy at the end. That’s how energy in a closed system works. And in a purely mathematical system, the energy is always conserved correctly. It’s just a fact about the physics.

> But in a numerical system, it might not be. Sometimes the total energy is not the same at the end as it was at the start. In these cases, the energy is divergent. How can this happen? It tends to happen when the posterior distribution is very steep in some region of parameter space. Steep changes in probability are hard for a discrete physics simulation to follow. When that happens, the algorithm notices by comparing the energy at the start to the energy at the end. When they don’t match, it indicates numerical problems exploring that part of the posterior distribution.

---

### centered parameterization

In his lecture, McElreath uses **CENTERED PARAMETERIZATION** to demonstrate divergent transitions. A very simple example:

\begin{align*}
x &\sim \text{Normal}(0, exp(\nu)) \\
\nu &\sim \text{Normal}(0, 3) \\
\end{align*}


This expression is centered because one set of priors (the priors for $x$) are centered around another prior (the prior for $\nu$). It's intuitive, but this can cause a lot of problems with Stan, which is probably why McElreath used this for his example. In short, when there is limited data within our groups or the population variance is small, the parameters $x$ and $\nu$ become highly correlated. This geometry is challenging for MCMC to sample. (Think of a long and narrow groove, not a bowl, for your Hamiltonian skateboard.)

---

```{r}
#| code-fold: true
set.seed(1)
# plot the likelihoods
ps <- seq( from=-4, to=4, length.out=200) # possible parameter values for both x and nu

crossing(nu = ps, x=ps) %>%  #every possible combination of nu and x
  mutate(
    likelihood_nu = dnorm(nu, 0, 3),
    likelihood_x  = dnorm(x, 0, exp(nu)),
    joint_likelihood = likelihood_nu*likelihood_x
  ) %>% 
  ggplot( aes(x=x, y=nu, fill=joint_likelihood) ) +
  geom_raster() + 
  scale_fill_viridis_c() +
  guides(fill = F)
  
```


---


The way to fix this is by using an uncentered parameterization:

\begin{align*}
x &= z\times \text{exp}(\nu) \\
z &\sim \text{Normal}(0, 1) \\
\nu &\sim \text{Normal}(0, 3) \\
\end{align*}

```{r}
#| code-fold: true
set.seed(1)
# plot the likelihoods
ps <- seq( from=-4, to=4, length.out=200) # possible parameter values for both x and nu

crossing(nu = ps, z=ps) %>%  #every possible combination of nu and x
  mutate(
    likelihood_nu = dnorm(nu, 0, 3),
    likelihood_z  = dnorm(z, 0, 1),
    joint_likelihood = likelihood_nu*likelihood_z
  ) %>% 
  ggplot( aes(x=z, y=nu, fill=joint_likelihood) ) +
  geom_raster() +
  scale_fill_viridis_c() +
  guides(fill = F)
  
```

---

It's an important point, except the issues of [centered parameterization are so prevalent](https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html)[^1], that `brms` generally doesn't allow centered parameterization (with some exceptions). So we can't recreate the divergent transition situation that McElreath demonstrates in his lecture. 

[^1]: [this video](https://www.youtube.com/watch?v=gSd1msFFZTw) is a great explanation

McElreath describes the problem of fertility in Bangladesh as such: 

\begin{align*}
C &\sim \text{Bernoulli}(p_i) \\
\text{logit}(p_i) &= \alpha_{D_{[i]}} \\
\alpha_j &\sim \text{Normal}(\bar{\alpha}, \sigma) \\
\bar{\alpha} &\sim \text{Normal}(0, 1) \\
\sigma &\sim \text{Exponential}(1) \\
\end{align*}

But to fit this using `brms`, we'll rewrite as:

\begin{align*}
C &\sim \text{Bernoulli}(p_i) \\
\text{logit}(p_i) &= \alpha + \alpha_{D[i]} \\
\alpha &\sim \text{Normal}(0, 1) \\
\alpha_{D[j]} &\sim \text{Normal}(0, \sigma_{D}) \\
\sigma_{D} &\sim \text{Exponential}(1)
\end{align*}

---


\begin{align*}
C &\sim \text{Bernoulli}(p_i) \\
\text{logit}(p_i) &= \alpha + \alpha_{D[i]} \\
\alpha &\sim \text{Normal}(0, 1) \\
\alpha_{D[j]} &\sim \text{Normal}(0, \sigma_{D}) \\
\sigma_{D} &\sim \text{Exponential}(1)
\end{align*}


```{r}
data(bangladesh, package="rethinking")
d <- bangladesh

m1 <- brm(
  data=d,
  family=bernoulli,
  use.contraception ~ 1 + (1 | district),
  prior = c( prior(normal(0, 1), class = Intercept), # alpha bar
             prior(exponential(1), class = sd)),       # sigma

  chains=4, cores=4, iter=2000, warmup=1000,
  seed = 1,
  file = here("files/data/generated_data/m71.1"))
```

---

```{r}
m1
```

---

```{r}
gather_draws(m1, b_Intercept, r_district[district, ]) %>% 
  with_groups(c(.variable, district), median_qi, .value)
```

---


```{r}
#| code-fold: true

gather_draws(m1, b_Intercept, r_district[district, ]) %>% 
  with_groups(c(.variable, district), median_qi, .value) %>% 
  ggplot(aes( x=district, y=.value)) +
  geom_pointinterval( aes(ymin = .lower, ymax = .upper), 
                      alpha=.5) +
  labs(y="District distance from mean") +
  coord_flip()
```

---

\begin{align*}
C &\sim \text{Bernoulli}(p_i) \\
\text{logit}(p_i) &= \alpha + \alpha_{D[i]} + \beta U_i + \beta_{D[i]}U_i \\
\alpha, \beta &\sim \text{Normal}(0, 1) \\
\alpha_{D[j]} &\sim \text{Normal}(0, \sigma_{D}) \\
\beta_{D[j]} &\sim \text{Normal}(0, \tau_{D}) \\
\sigma, \tau &\sim \text{Exponential}(1) \\
\end{align*}


```{r, message=F}
m2 <- brm(
  data=d,
  family=bernoulli,
  use.contraception ~ 1 + urban + (1 + urban || district),
  prior = c( prior(normal(0, 1), class = Intercept), 
             prior(normal(0, 1), class = b),
             prior(exponential(1), class = sd)),     

  chains=4, cores=4, iter=2000, warmup=1000,
  seed = 1,
  file = here("files/data/generated_data/m71.2"))
```

------------------------------------------------------------------------

Oops, no divergent transitions.

```{r}
m2
```

---

### more about divergent transitions

* [Divergent transitions -- a primer](https://discourse.mc-stan.org/t/divergent-transitions-a-primer/17099)

* [Towards a principled Bayesian workflow](https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html)

* [Bayesian workflow](https://arxiv.org/abs/2011.01808)


---

![](images/7-1_workflow.png)

From Gelman et al [(2020)](https://arxiv.org/abs/2011.01808)
